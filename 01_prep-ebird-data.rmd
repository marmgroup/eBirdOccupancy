---
editor_options: 
  chunk_output_type: console
---

# Preparing eBird data

## Prepare libraries and data sources

```{r load_libs, eval=FALSE, message=FALSE, warning=FALSE}

# load libs
library(tidyverse) 
library(readr) 
library(sf)
library(auk)
library(readxl)

# custom sum function
sum.no.na <- function(x){sum(x, na.rm = T)}

# set file paths for auk functions
f_in_ebd <- file.path("data/ebd_IN_relApr-2020.txt")
f_in_sampling <- file.path("data/ebd_sampling_relApr-2020.txt")
```

## Filter data

```{r soi, eval=FALSE, message=FALSE, warning=FALSE}
# add species of interest
specieslist <- read.csv("data/4_List_of_species_final.csv")
speciesOfInterest <- as.character(specieslist$scientific_name)
```

```{r prep_ebd_filters, eval=FALSE}

# run filters using auk packages
ebd_filters <- auk_ebd(f_in_ebd, f_in_sampling) %>%
  auk_species(speciesOfInterest) %>%
  auk_country(country = "IN") %>%
  auk_state(c("IN-KL","IN-TN", "IN-KA")) %>% # Restricting geography to TamilNadu, Kerala & Karnataka
  auk_date(c("2013-01-01", "2019-12-31")) %>%
  auk_complete()

# check filters
ebd_filters
```

Below code need not be run if it has been filtered once already and the above path leads to the right dataset. NB: This is a computation heavy process, run with caution.

```{r output_loc, eval=FALSE}

# specify output location and perform filter
f_out_ebd <- "data/eBirdDataWG_filtered.txt"
f_out_sampling <- "data/eBirdSamplingDataWG_filtered.txt"
```

```{r filter_data, eval=FALSE}
ebd_filtered <- auk_filter(ebd_filters, file = f_out_ebd,
                           file_sampling = f_out_sampling, overwrite = TRUE)
```

## Process filtered data

```{r read_data, eval=FALSE}
# read in the data
ebd <- read_ebd(f_out_ebd)
```

```{r fill_zeroes, eval=FALSE}
# fill zeroes
zf <- auk_zerofill(f_out_ebd, f_out_sampling)
new_zf <- collapse_zerofill(zf) # Creates a new zero-filled dataframe with a 0 marked for each checklist when the bird was not observed
```

```{r choose_cols, eval=FALSE}
# choose columns of interest
columnsOfInterest <- c("checklist_id", "scientific_name", "common_name", 
                       "observation_count", "locality", "locality_id",
                       "locality_type", "latitude", "longitude",
                       "observation_date", "time_observations_started",
                       "observer_id", "sampling_event_identifier",
                       "protocol_type", "duration_minutes",
                       "effort_distance_km", "effort_area_ha",
                       "number_observers", "species_observed",
                       "reviewed")

# make list of presence and absence data and choose cols of interest
data <- list(ebd, new_zf) %>%
  map(function(x){
    x %>% select(one_of(columnsOfInterest))
  })

# remove zerofills to save working memory
rm(zf, new_zf); gc()

# check presence and absence in absences df, remove essentially the presences df
data[[2]] <- data[[2]] %>% filter(species_observed == F)
```

## Spatial filter

```{r spatial_filters, eval=FALSE}

# load shapefiles of hill ranges
library(sf)
hills <- st_read("data/spatial/hillsShapefile/Nil_Ana_Pal.shp")

# write a prelim filter by bounding box
box <- st_bbox(hills)

# get data spatial coordinates
dataLocs <- data %>%
  map(function(x){
    select(x, longitude, latitude) %>% 
      filter(between(longitude, box["xmin"], box["xmax"]) & 
               between(latitude, box["ymin"], box["ymax"]))}) %>%
  bind_rows() %>%
  distinct() %>%
  st_as_sf(coords = c("longitude", "latitude")) %>%
  st_set_crs(4326) %>%
  st_intersection(hills)

# get simplified data and drop geometry
dataLocs <- mutate(dataLocs, spatialKeep = T) %>%
  bind_cols(., as_tibble(st_coordinates(dataLocs))) %>% 
  st_drop_geometry()

# bind to data and then filter
data <- data %>%
  map(function(x){
    left_join(x, dataLocs, by = c("longitude" = "X", "latitude" = "Y")) %>%
      filter(spatialKeep == T) %>%
      select(-Id, -spatialKeep)
  })

```

```{r save_temp_data, eval=FALSE}
# save a temp data file
save(data, file = "data/data_temp.rdata")
```

## Handle presence data

```{r proc_presence_data, eval=FALSE}

# in the first set, replace X, for presences, with 1
data[[1]] <- data[[1]] %>% 
  mutate(observation_count = ifelse(observation_count == "X", 
                                    "1", observation_count))

# remove records where duration is 0
data <- map(data, function(x) filter(x, duration_minutes > 0))

# group data by site and sampling event identifier
# then, summarise relevant variables as the sum
dataGrouped <- map(data, function(x){
  x %>% 
    group_by(sampling_event_identifier) %>%
    summarise_at(vars(duration_minutes, effort_distance_km, 
                      effort_area_ha), 
                 list(sum.no.na))
})

# bind rows combining data frames, and filter
dataGrouped <- bind_rows(dataGrouped) %>%
  filter(duration_minutes <= 300, 
         effort_distance_km <= 5, 
         effort_area_ha <= 500)

# get data identifiers, such as sampling identifier etc
dataConstants <- data %>% 
  bind_rows() %>% 
  select(sampling_event_identifier, time_observations_started, 
         locality, locality_type, locality_id, 
         observer_id, observation_date, scientific_name, 
         observation_count, protocol_type, number_observers, 
         longitude, latitude)

# join the summarised data with the identifiers, 
# using sampling_event_identifier as the key
dataGrouped <- left_join(dataGrouped, dataConstants, 
                         by = "sampling_event_identifier")

# remove checklists or seis with more than 10 obervers
count(dataGrouped, number_observers > 10) # count how many have 10+ obs
dataGrouped <- filter(dataGrouped, number_observers <= 10)

```

## Add decimal time

```{r decimal_time, eval=FALSE}

# assign present or not, and get time in decimal hours since midnight
library(lubridate)
time_to_decimal <- function(x) {
  x <- hms(x, quiet = TRUE)
  hour(x) + minute(x) / 60 + second(x) / 3600
}

# will cause issues if using time obs started as a linear effect and not quadratic
dataGrouped = mutate(dataGrouped, 
                     pres_abs = observation_count >= 1,
                     decimalTime = time_to_decimal(time_observations_started))

# check class of dataGrouped, make sure not sf
assertthat::assert_that(!"sf" %in% class(dataGrouped))
```

## Write data

```{r write_clean_data, eval=FALSE}

# save a temp data file
save(dataGrouped, file = "data/data_prelim_processing.rdata")

```

