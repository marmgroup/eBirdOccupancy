---
editor_options: 
  chunk_output_type: console
---

# Prepare observer expertise

## Prepare libraries

```{r load_libs3, eval=FALSE, message=FALSE, warning=FALSE}

# load libs
library(data.table)
library(readxl)
library(magrittr)
library(stringr)
library(dplyr)
library(tidyr)

# get decimal time function
library(lubridate)
time_to_decimal <- function(x) {
  x <- hms(x, quiet = TRUE)
  hour(x) + minute(x) / 60 + second(x) / 3600
}
```

## Prepare data

Here, we go through the data preparation process again because we might want to assess observer expertise over a larger area than the study site.

```{r load_raw_data, eval=FALSE}
# read in shapefile of nilgiris to subset by bounding box
library(sf)
wg <- st_read("data/spatial/hillsShapefile/Nil_Ana_Pal.shp"); box <- st_bbox(wg)

# read in data and subset
ebd <- fread("ebd_Filtered_Jun2019.txt")[between(LONGITUDE, box["xmin"], box["xmax"]) & between(LATITUDE, box["ymin"], box["ymax"]),][year(`OBSERVATION DATE`) >= 2013,]

# make new column names
newNames <- str_replace_all(colnames(ebd), " ", "_") %>%
  str_to_lower()
setnames(ebd, newNames)

# keep useful columns
columnsOfInterest <- c("checklist_id","scientific_name","observation_count","locality","locality_id","locality_type","latitude","longitude","observation_date","time_observations_started","observer_id","sampling_event_identifier","protocol_type","duration_minutes","effort_distance_km","effort_area_ha","number_observers","species_observed","reviewed","state_code", "group_identifier")

ebd <- select(ebd, one_of(columnsOfInterest))

# get the checklist id as SEI or group id
ebd[,checklist_id := ifelse(group_identifier == "", 
                            sampling_event_identifier, group_identifier),]
```

## Explicit spatial subset

```{r hills_subset_exp, eval=FALSE}
# get checklist locations
ebd_locs <- ebd[,.(longitude, latitude)]
ebd_locs <- setDF(ebd_locs) %>% distinct(ebd_locs)
ebd_locs <- st_as_sf(ebd_locs, 
                     coords = c("longitude","latitude")) %>% 
  `st_crs<-`(4326) %>% 
  mutate(id = 1:nrow(.))

# check whether to include
to_keep <- unlist(st_contains(wg, ebd_locs))

# filter locs
ebd_locs <- filter(ebd_locs, id %in% to_keep) %>% 
  bind_cols(as_tibble(st_coordinates(.))) %>% 
  st_drop_geometry()
```

```{r hills_subset_exp2, eval=FALSE}
ebd <- ebd[longitude %in% ebd_locs$X & latitude %in% ebd_locs$Y,]
```


## Prepare species of interest

```{r soi_score, eval=FALSE}
# read in species list
specieslist = read_excel(path = "data/species_list_13_11_2019.xlsx")

# set species of interest
soi = specieslist$scientific_name

ebdSpSum <- ebd[,.(nSp = .N,
                   totSoiSeen = length(intersect(scientific_name, soi))), 
                by = list(sampling_event_identifier)]

# write to file and link with checklsit id later
fwrite(ebdSpSum, file = "data/dataChecklistSpecies.csv")
```

## Prepare checklists for observer score

```{r prepare_chk_covars, eval=FALSE}
# 1. add new columns of decimal time and julian date
ebd[,`:=`(decimalTime = time_to_decimal(time_observations_started),
          julianDate = yday(as.POSIXct(observation_date)))]

ebdEffChk <- setDF(ebd) %>% 
  mutate(year = year(observation_date)) %>% 
  distinct(sampling_event_identifier, observer_id,
           year,
           duration_minutes, effort_distance_km, longitude, latitude,
           decimalTime, julianDate, number_observers) %>% 
  # drop rows with NAs in cols used in the model
  tidyr::drop_na(sampling_event_identifier, observer_id,
                 duration_minutes, decimalTime, julianDate) %>% 
  
  # drop years below 2013
  filter(year >= 2013)

# 3. join to covariates and remove large groups (> 10)
ebdChkSummary <- inner_join(ebdEffChk, ebdSpSum)

# remove ebird data
rm(ebd); gc()
```

## Get landcover

Using the landcover 1km resolution here.

```{r chk_covar_landuse, eval=FALSE}
# read in 1km landcover and set 0 to NA
library(raster)
landcover <- raster::raster("data/spatial/landcover1km.tif")
landcover[landcover==0] <- NA

# get locs in utm coords
locs <- distinct(ebdChkSummary, sampling_event_identifier, longitude, latitude)
locs <- st_as_sf(locs, coords = c("longitude", "latitude")) %>% 
  `st_crs<-`(4326) %>% 
  st_transform(32643) %>% 
  st_coordinates()

# get for unique points
landcoverVec <- raster::extract(x = landcover, 
                                y = locs)

# assign to df and overwrite
setDT(ebdChkSummary)[,landcover:= landcoverVec]

# save to file for later reuse
fwrite(ebdChkSummary, file = "data/eBirdChecklistVars.csv")
```

## Filter data for stats

```{r filter_data2, eval=FALSE}
# change names for easy handling
setnames(ebdChkSummary, c("sei", "observer","year", "duration", "distance",
                          "longitude", "latitude", "decimalTime",
                          "julianDate", "nObs", "nSp", "nSoi", "landcover"))

# count data points per observer 
obscount <- count(ebdChkSummary, observer) %>% 
  filter(n >= 10)

# make factor variables and remove obs not in obscount
# also remove 0 durations
ebdChkSummary <- ebdChkSummary %>% 
  filter(observer %in% obscount$observer, 
         duration > 0,
         duration <= 300,
         nSoi > 0,
         !is.na(nSoi)) %>% 
  mutate(landcover = as.factor(landcover),
         observer = as.factor(observer)) %>% 
  drop_na(landcover)
```

## Run observer expertise model

Our observer expertise model aims to include the random intercpet effect of observer identity, with a random slope effect of duration. This models the different rate of species accumulation by different observers, as well as their different starting points.

```{r run_model, eval=FALSE}
# uses either a subset or all data
library(lmerTest)

# here we specify a glmm with random effects for observer
# time is considered a fixed log predictor and a random slope
modObsExp <- glmer(nSoi ~ sqrt(duration) + 
                     landcover+
                     sqrt(decimalTime) + 
                     I((sqrt(decimalTime))^2) + 
                     log(julianDate) + 
                     I((log(julianDate)^2)) + 
                     (1|observer) + (0+duration|observer),
                   data = ebdChkSummary, family = "poisson")
``` 

## Write model to file

```{r write_obsexp_model, eval=FALSE}
# make dir if absent
if(!dir.exists("data/modOutput")){
  dir.create("data/modOutput")
}

# write model output to text file
{
  writeLines(R.utils::captureOutput(list(Sys.time(), summary(modObsExp))), 
             con = "data/modOutput/modOutExpertise.txt")
}
```

## Get observer expertise as species in 60 mins

```{r obsexp_ranef, eval=FALSE}
# make df with means
observer <- unique(ebdChkSummary$observer)

# predict at 60 mins on the most common landcover
dfPredict <- ebdChkSummary %>% 
  summarise_at(vars(duration, decimalTime, julianDate), list(~mean(.))) %>% 
  mutate(duration = 60, landcover = as.factor(6)) %>% 
  tidyr::crossing(observer)

# run predict from model on it
dfPredict <- mutate(dfPredict, 
                    score = predict(modObsExp, newdata = dfPredict, type = "response",
                                    allow.new.levels = TRUE)) %>% 
  mutate(score = scales::rescale(score))
```

## Write observer expertise to file

```{r write_obsexp, eval=FALSE}
fwrite(dfPredict %>% dplyr::select(observer, score), file = "data/dataObsExpScore.csv")
```

