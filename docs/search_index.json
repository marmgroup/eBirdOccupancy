[
["index.html", "Influence of land cover and climate on the occupancy of avian distributions along a tropical montane gradient Section 1 Introduction 1.1 Attribution 1.2 Data access 1.3 Data processing", " Influence of land cover and climate on the occupancy of avian distributions along a tropical montane gradient Vijay Ramesh, Pratik R Gupte, and Morgan Tingley 2019-12-25 Section 1 Introduction This is the bookdown version of a project in preparation that models occupancy for birds in the Nilgiri hills. Methods and format are derived from Strimas-Mackey et al., the supplement to Jonhnston et al.Â (2019). 1.1 Attribution Please contact the following in case of interest in the project. Vijay Ramesh (lead author) PhD student, Columbia University Pratik Gupte (repo maintainer) PhD student, University of Groningen Morgan Tingley (PI) 1.2 Data access The data used in this work are available from eBird. 1.3 Data processing The data processing for this project is described in the following sections. Navigate through them using the links in the sidebar. "],
["preparing-ebird-data.html", "Section 2 Preparing eBird data 2.1 Prepare libraries and data sources 2.2 Filter data 2.3 Process filtered data 2.4 Spatial filter 2.5 Handle presence data 2.6 Add decimal time 2.7 Write data", " Section 2 Preparing eBird data 2.1 Prepare libraries and data sources # load libs library(tidyverse) library(readr) library(sf) library(auk) library(readxl) # custom sum function sum.no.na &lt;- function(x){sum(x, na.rm = T)} #set file paths for auk functions f_in_ebd &lt;- file.path(&quot;ebd_Filtered_Jun2019.txt&quot;) f_in_sampling &lt;- file.path(&quot;ebd_sampling_Filtered_Jun2019.txt&quot;) 2.2 Filter data # add species of interest specieslist &lt;- read_excel(&quot;data/species_list_13_11_2019.xlsx&quot;) speciesOfInterest &lt;- specieslist$scientific_name # run filters using auk packages ebd_filters &lt;- auk_ebd(f_in_ebd, f_in_sampling) %&gt;% auk_species(speciesOfInterest) %&gt;% auk_country(country = &quot;IN&quot;) %&gt;% auk_state(c(&quot;IN-KL&quot;,&quot;IN-TN&quot;, &quot;IN-KA&quot;)) %&gt;% # Restricting geography to TamilNadu, Kerala &amp; Karnataka auk_date(c(&quot;2013-01-01&quot;, &quot;2018-12-31&quot;)) %&gt;% auk_complete() # check filters ebd_filters Below code need not be run if it has been filtered once already and the above path leads to the right dataset. NB: This is a computation heavy process, run with caution. # specify output location and perform filter f_out_ebd &lt;- &quot;data/eBirdDataWG_filtered.txt&quot; f_out_sampling &lt;- &quot;data/eBirdSamplingDataWG_filtered.txt&quot; ebd_filtered &lt;- auk_filter(ebd_filters, file = f_out_ebd, file_sampling = f_out_sampling, overwrite = TRUE) 2.3 Process filtered data # read in the data ebd &lt;- read_ebd(f_out_ebd) # fill zeroes zf &lt;- auk_zerofill(f_out_ebd, f_out_sampling) new_zf &lt;- collapse_zerofill(zf) # Creates a new zero-filled dataframe with a 0 marked for each checklist when the bird was not observed # choose columns of interest columnsOfInterest &lt;- c(&quot;checklist_id&quot;,&quot;scientific_name&quot;,&quot;observation_count&quot;,&quot;locality&quot;,&quot;locality_id&quot;,&quot;locality_type&quot;,&quot;latitude&quot;,&quot;longitude&quot;,&quot;observation_date&quot;,&quot;time_observations_started&quot;,&quot;observer_id&quot;,&quot;sampling_event_identifier&quot;,&quot;protocol_type&quot;,&quot;duration_minutes&quot;,&quot;effort_distance_km&quot;,&quot;effort_area_ha&quot;,&quot;number_observers&quot;,&quot;species_observed&quot;,&quot;reviewed&quot;) # make list of presence and absence data and choose cols of interest data &lt;- list(ebd, new_zf) %&gt;% map(function(x){ x %&gt;% select(one_of(columnsOfInterest)) }) # remove zerofills to save working memory rm(zf, new_zf); gc() # check presence and absence in absences df, remove essentially the presences df data[[2]] &lt;- data[[2]] %&gt;% filter(species_observed == F) 2.4 Spatial filter # load shapefiles of hill ranges library(sf) hills &lt;- st_read(&quot;data/spatial/hillsShapefile/Nil_Ana_Pal.shp&quot;) # write a prelim filter by bounding box box &lt;- st_bbox(hills) # get data spatial coordinates dataLocs &lt;- data %&gt;% map(function(x){ select(x, longitude, latitude) %&gt;% filter(between(longitude, box[&quot;xmin&quot;], box[&quot;xmax&quot;]) &amp; between(latitude, box[&quot;ymin&quot;], box[&quot;ymax&quot;]))}) %&gt;% bind_rows() %&gt;% distinct() %&gt;% st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;)) %&gt;% st_set_crs(4326) %&gt;% st_intersection(hills) # get simplified data and drop geometry dataLocs &lt;- mutate(dataLocs, spatialKeep = T) %&gt;% bind_cols(., as_tibble(st_coordinates(dataLocs))) %&gt;% st_drop_geometry() # bind to data and then filter data &lt;- data %&gt;% map(function(x){ left_join(x, dataLocs, by = c(&quot;longitude&quot; = &quot;X&quot;, &quot;latitude&quot; = &quot;Y&quot;)) %&gt;% filter(spatialKeep == T) %&gt;% select(-Id, -spatialKeep) }) # save a temp data file save(data, file = &quot;data.temp.rdata&quot;) 2.5 Handle presence data # in the first set, replace X, for presences, with 1 data[[1]] &lt;- data[[1]] %&gt;% mutate(observation_count = ifelse(observation_count == &quot;X&quot;, &quot;1&quot;, observation_count)) # remove records where duration is 0 data &lt;- map(data, function(x) filter(x, duration_minutes &gt; 0)) # group data by site and sampling event identifier # then, summarise relevant variables as the sum dataGrouped &lt;- map(data, function(x){ x %&gt;% group_by(sampling_event_identifier) %&gt;% summarise_at(vars(duration_minutes, effort_distance_km, effort_area_ha), list(sum.no.na)) }) # bind rows combining data frames, and filter dataGrouped &lt;- bind_rows(dataGrouped) %&gt;% filter(duration_minutes &lt;= 300, effort_distance_km &lt;= 5, effort_area_ha &lt;= 500) # get data identifiers, such as sampling identifier etc dataConstants &lt;- data %&gt;% bind_rows() %&gt;% select(sampling_event_identifier, time_observations_started, locality, locality_type, locality_id, observer_id, observation_date, scientific_name, observation_count, protocol_type, number_observers, longitude, latitude) # join the summarised data with the identifiers, using sampling_event_identifier as the key dataGrouped &lt;- left_join(dataGrouped, dataConstants, by = &quot;sampling_event_identifier&quot;) # remove checklists or seis with more than 10 obervers count(dataGrouped, number_observers &gt; 10) # count how many have 10+ obs dataGrouped &lt;- filter(dataGrouped, number_observers &lt;= 10) 2.6 Add decimal time # assign present or not, and get time in decimal hours since midnight library(lubridate) time_to_decimal &lt;- function(x) { x &lt;- hms(x, quiet = TRUE) hour(x) + minute(x) / 60 + second(x) / 3600 } # will cause issues if using time obs started as a linear effect and not quadratic dataGrouped = mutate(dataGrouped, pres_abs = observation_count &gt;= 1, decimalTime = time_to_decimal(time_observations_started)) # check class of dataGrouped, make sure not sf assertthat::assert_that(!&quot;sf&quot; %in% class(dataGrouped)) 2.7 Write data # save a temp data file save(dataGrouped, file = &quot;data_prelim_processing.rdata&quot;) "],
["prepare-landscape-data.html", "Section 3 Prepare landscape data 3.1 Prepare libraries 3.2 Prepare initial data 3.3 Resample rasters", " Section 3 Prepare landscape data 3.1 Prepare libraries # load libs library(raster) library(stringi) library(glue) library(gdalUtils) library(purrr) # prep mode function to aggregate funcMode &lt;- function(x, na.rm = T) { ux &lt;- unique(x) ux[which.max(tabulate(match(x, ux)))] } # a basic test assertthat::assert_that(funcMode(c(2,2,2,2,3,3,3,4)) == as.character(2), msg = &quot;problem in the mode function&quot;) # works 3.2 Prepare initial data 3.2.1 Prepare spatial extent # load hills library(sf) hills &lt;- st_read(&quot;data/spatial/hillsShapefile/Nil_Ana_Pal.shp&quot;) 3.2.2 Prepare terrain rasters # load elevation and crop to hills size, then mask by hills alt &lt;- raster(&quot;data/spatial/Elevation/alt&quot;) alt.hills &lt;- crop(alt, as(hills, &quot;Spatial&quot;)) rm(alt); gc() # get slope and aspect slopeData &lt;- terrain(x = alt.hills, opt = c(&quot;slope&quot;, &quot;aspect&quot;)) elevData &lt;- raster::stack(alt.hills, slopeData) rm(alt.hills); gc() 3.2.3 Prepare EVI rasters # load evi layers evi &lt;- raster::stack(&quot;data/spatial/EVI/MOD13Q1_EVI_AllYears.tif&quot;)[[c(1,7,10)]] # reproject to elevdata evi &lt;- projectRaster(evi, elevData, res(elevData)) 3.2.4 Prepare CHELSA rasters # list chelsa files chelsaFiles &lt;- list.files(&quot;data/chelsa/&quot;, full.names = TRUE, pattern = &quot;crop.tif&quot;) # gather chelsa rasters chelsaData &lt;- purrr::map(chelsaFiles, function(chr){ a &lt;- raster(chr) crs(a) &lt;- crs(evi) return(a) }) # stack chelsa data chelsaData &lt;- raster::stack(chelsaData) # project to elevation chelsaData &lt;- projectRaster(chelsaData, elevData, res(elevData)) 3.2.5 Stack prepared rasters # stack rasters for efficient reprojection later env_data &lt;- stack(elevData, evi, chelsaData) 3.2.6 Prepare landcover # read in landcover raster location landcover &lt;- &quot;data/landUseClassification/ClassifiedImage_31stAug_UTM34N_Ghats.tif&quot; # get extent e = bbox(raster(landcover)) # use gdalutils gdalwarp for resampling transform # to 1km from 10m gdalwarp(srcfile = landcover, dstfile = &quot;data/landUseClassification/lc_01km.tif&quot;, tr=c(1000,1000), r=&#39;mode&#39;, te=c(e)) # to 10km from 1km gdalwarp(srcfile = &quot;data/landUseClassification/lc_01km.tif&quot;, dstfile = &quot;data/landUseClassification/lc_10km.tif&quot;, tr=c(10000,10000), r=&#39;mode&#39;, te=c(e)) # to 25km from 1km gdalwarp(srcfile = &quot;data/landUseClassification/lc_01km.tif&quot;, dstfile = &quot;data/landUseClassification/lc_25km.tif&quot;, tr=c(25000,25000), r=&#39;mode&#39;, te=c(e)) 3.3 Resample rasters 3.3.1 Read landcover as list Here, we read in the 1km, 10km, and 25km resampled landcover rasters as a list. # list the files and map a raster read over them lc_files &lt;- list.files(&quot;data/landUseClassification/&quot;, pattern = &quot;km.tif&quot;, full.names = TRUE) lc_data &lt;- map(lc_files, raster) 3.3.2 Reproject environmental data to landcover # resample to the corresponding landcover data env_data_resamp &lt;- map(lc_data, function(rs){ projectRaster(from = env_data, to = rs, crs = crs(rs), res = res(rs)) }) # export as raster stack land_stacks &lt;- map2(env_data_resamp, lc_data, function(a, b){ land_stack = stack(a, b)}) # get names land_names &lt;- glue(&#39;data/spatial/landscape_resamp{c(&quot;01&quot;,&quot;10&quot;,&quot;25&quot;)}km.tif&#39;) # write to file map2(land_stacks, land_names, function(x,y){ writeRaster(x, filename = as.character(y), overwrite=TRUE) }) "],
["prepare-observer-expertise.html", "Section 4 Prepare observer expertise 4.1 Prepare libraries 4.2 Prepare data 4.3 Prepare species of interest 4.4 Prepare checklists for observer score 4.5 Get landcover 4.6 Filter data for stats 4.7 Run observer expertise model 4.8 Write model to file 4.9 Get observer expertise as random effect 4.10 Write observer expertise to file", " Section 4 Prepare observer expertise 4.1 Prepare libraries # load libs library(data.table) library(readxl) library(magrittr) library(stringr) library(dplyr) # get decimal time function library(lubridate) time_to_decimal &lt;- function(x) { x &lt;- hms(x, quiet = TRUE) hour(x) + minute(x) / 60 + second(x) / 3600 } 4.2 Prepare data Here, we go through the data preparation process again because we might want to assess observer expertise over a larger area than the study site. # read in shapefile of nilgiris to subset by bounding box library(sf) wg &lt;- st_read(&quot;data/spatial/hillsShapefile/Nil_Ana_Pal.shp&quot;); box &lt;- st_bbox(wg) # read in data and subset ebd &lt;- fread(&quot;ebd_Filtered_Jun2019.txt&quot;)[between(LONGITUDE, box[&quot;xmin&quot;], box[&quot;xmax&quot;]) &amp; between(LATITUDE, box[&quot;ymin&quot;], box[&quot;ymax&quot;]),][year(`OBSERVATION DATE`) &gt;= 2013,] # make new column names newNames &lt;- str_replace_all(colnames(ebd), &quot; &quot;, &quot;_&quot;) %&gt;% str_to_lower() setnames(ebd, newNames) # keep useful columns columnsOfInterest &lt;- c(&quot;checklist_id&quot;,&quot;scientific_name&quot;,&quot;observation_count&quot;,&quot;locality&quot;,&quot;locality_id&quot;,&quot;locality_type&quot;,&quot;latitude&quot;,&quot;longitude&quot;,&quot;observation_date&quot;,&quot;time_observations_started&quot;,&quot;observer_id&quot;,&quot;sampling_event_identifier&quot;,&quot;protocol_type&quot;,&quot;duration_minutes&quot;,&quot;effort_distance_km&quot;,&quot;effort_area_ha&quot;,&quot;number_observers&quot;,&quot;species_observed&quot;,&quot;reviewed&quot;,&quot;state_code&quot;, &quot;group_identifier&quot;) ebd &lt;- select(ebd, one_of(columnsOfInterest)) # get the checklist id as SEI or group id ebd[,checklist_id := ifelse(group_identifier == &quot;&quot;, sampling_event_identifier, group_identifier),] # n checklists per observer ebdNchk &lt;- ebd[,year:=year(observation_date) ][,.(nChk = length(unique(checklist_id)), nSei = length(unique(sampling_event_identifier))), by= list(observer_id, year)] 4.3 Prepare species of interest # read in species list specieslist = read_excel(path = &quot;data/species_list_13_11_2019.xlsx&quot;) # set species of interest soi = specieslist$scientific_name ebdSpSum &lt;- ebd[,.(nSp = .N, totSoiSeen = length(intersect(scientific_name, soi))), by = list(sampling_event_identifier, observer_id, year)] # write to file and link with checklsit id later fwrite(ebdSpSum, file = &quot;data/dataChecklistSpecies.csv&quot;) 4.4 Prepare checklists for observer score # 1. add new columns of decimal time and julian date ebd[,`:=`(decimalTime = time_to_decimal(time_observations_started), julianDate = yday(as.POSIXct(observation_date)))] ebdEffChk &lt;- setDF(ebd) %&gt;% mutate(year = year(observation_date)) %&gt;% distinct(sampling_event_identifier, observer_id, year, duration_minutes, effort_distance_km, longitude, latitude, decimalTime, julianDate, number_observers) %&gt;% # drop rows with NAs in cols used in the model tidyr::drop_na(sampling_event_identifier, observer_id, duration_minutes, decimalTime, julianDate) %&gt;% # drop years below 2013 filter(year &gt;= 2013) # 3. join to covariates and remove large groups (&gt; 10) ebdChkSummary &lt;- inner_join(ebdEffChk, ebdSpSum) # remove ebird data rm(ebd); gc() 4.5 Get landcover Using the landcover 100m resolution here. # read in 100m landcover landcover &lt;- raster::raster(&quot;data/landUseClassification/lc_100m.tif&quot;) # get for unique points landcoverVec &lt;- raster::extract(x = landcover, y = as.matrix(ebdChkSummary[,c(&quot;longitude&quot;,&quot;latitude&quot;)])) # assign to df and overwrite setDT(ebdChkSummary)[,landcover:= landcoverVec] # save to file for later reuse fwrite(ebdChkSummary, file = &quot;data/eBirdChecklistVars.csv&quot;) 4.6 Filter data for stats # change names for easy handling setnames(ebdChkSummary, c(&quot;sei&quot;, &quot;observer&quot;,&quot;year&quot;, &quot;duration&quot;, &quot;distance&quot;, &quot;longitude&quot;, &quot;latitude&quot;, &quot;decimalTime&quot;, &quot;julianDate&quot;, &quot;nObs&quot;, &quot;nSp&quot;, &quot;nSoi&quot;, &quot;landcover&quot;)) # count data points per observer obscount &lt;- count(ebdChkSummary, observer) %&gt;% filter(n &gt;= 10) # make factor variables and remove obs not in obscount # also remove 0 durations ebdChkSummary &lt;- ebdChkSummary %&gt;% filter(observer %in% obscount$observer, duration &gt; 0, duration &lt;= 300, nSoi &gt; 0, !is.na(nSoi)) %&gt;% mutate(landcover = as.factor(landcover), observer = as.factor(observer)) %&gt;% tidyr::drop_na() # remove NAs, avoids errors later 4.7 Run observer expertise model Our observer expertise model aims to include the random intercpet effect of observer identity, with a random slope effect of duration. This models the different rate of species accumulation by different observers, as well as their different starting points. # uses either a subset or all data library(rptR) library(lmerTest) # here we specify a glmm with random effects for observer # time is considered a fixed log predictor and a random slope modObsRep &lt;- glmer(nSoi ~ log(duration) + sqrt(decimalTime) + I((sqrt(decimalTime))^2) + log(julianDate) + I((log(julianDate)^2)) + (1|observer) + (0+duration|observer), data = ebdChkSummary, family = &quot;poisson&quot;) 4.8 Write model to file # make dir if absent if(!dir.exists(&quot;data/modOutput&quot;)){ dir.create(&quot;data/modOutput&quot;) } # write model output to text file {writeLines(R.utils::captureOutput(list(Sys.time(), summary(modObsRep))), con = &quot;data/modOutput/modOutExpertise.txt&quot;)} 4.9 Get observer expertise as random effect # get the ranef coefficients as a measure of observer score obsRanef &lt;- lme4::ranef(modObsRep$mod)[[1]] # make datatable setDT(obsRanef, keep.rownames = T)[] # set names setnames(obsRanef, c(&quot;observer&quot;, &quot;rptrScore&quot;)) # scale ranefscore between 0 and 1 obsRanef[,rptrScore:=scales::rescale(rptrScore)] 4.10 Write observer expertise to file # make dataframe obsRanef &lt;- merge(ebdChkSummary[,c(&quot;observer&quot;, &quot;landcover&quot;,&quot;year&quot;)], obsRanef, by = c(&quot;observer&quot;)) # write to file fwrite(obsRanef[,.(observer, rptrScore)], file = &quot;data/dataObsRptrScore.csv&quot;) "],
["add-covariates-to-subsampled-data.html", "Section 5 Add covariates to subsampled data 5.1 Prepare libraries and data 5.2 Subsample data 5.3 Add expertise score 5.4 Add landscape covariates 5.5 Construct 2.5km buffer around subsampled points 5.6 Getting area means 5.7 Add landscape covariates 5.8 Write random 10 with covariates", " Section 5 Add covariates to subsampled data 5.1 Prepare libraries and data # load libs library(dplyr); library(readr) library(stringr) library(purrr) library(raster) library(glue) library(velox) library(tidyr) library(sf) # load saved data object load(&quot;data_prelim_processing.rdata&quot;) 5.2 Subsample data Get 10 random (if available) observations of each species at each locality. Only 1000 samples used as an example. # subsample data for random 10 observations dataSubsample &lt;- dataGrouped %&gt;% # sample_n(1e3) %&gt;% # SAMPLING 1000 AS AN EXAMPLE plyr::dlply(c(&quot;scientific_name&quot;, &quot;locality_id&quot;)) %&gt;% map_if(function(x) nrow(x) &gt; 10, function(x) sample_n(x, 10, replace = FALSE)) %&gt;% bind_rows() # remove full data rm(dataGrouped) 5.3 Add expertise score # read in obs score and extract numbers expertiseScore &lt;- read_csv(&quot;data/dataObsRptrScore.csv&quot;) %&gt;% mutate(numObserver = str_extract(observer, &quot;\\\\d+&quot;)) %&gt;% dplyr::select(-observer) # group seis consist of multiple observers # in this case, seis need to have the highest expertise observer score # as the associated covariate # get unique observers per sei dataSeiScore &lt;- distinct(dataSubsample, sampling_event_identifier, observer_id) %&gt;% # make list column of observers mutate(observers = str_split(observer_id, &quot;,&quot;)) %&gt;% unnest(cols = c(observers)) %&gt;% # add numeric observer id mutate(numObserver = str_extract(observers, &quot;\\\\d+&quot;)) %&gt;% # now get distinct sei and observer id numeric distinct(sampling_event_identifier, numObserver) # now add expertise score to sei dataSeiScore &lt;- left_join(dataSeiScore, expertiseScore, by=&quot;numObserver&quot;) %&gt;% # get max expertise score per sei group_by(sampling_event_identifier) %&gt;% summarise(expertise = max(rptrScore)) # add to dataCovar dataSubsample &lt;- left_join(dataSubsample, dataSeiScore, by = &quot;sampling_event_identifier&quot;) # remove data without expertise score dataSubsample &lt;- filter(dataSubsample, !is.na(expertise)) 5.4 Add landscape covariates # list landscape covariate stacks landscape_files &lt;- list.files(&quot;data/spatial/&quot;, pattern = &quot;landscape&quot;, full.names = T) # read in as stacks landscape_data &lt;- map(landscape_files, stack) # get proper names elev_names &lt;- c(&quot;elev&quot;, &quot;slope&quot;, &quot;aspect&quot;) evi_names &lt;- c(&quot;evi_01&quot;, &quot;evi_07&quot;, &quot;evi_10&quot;) chelsa_names &lt;- c(&quot;chelsa_bio10_04&quot;, &quot;chelsa_bio10_17&quot;, &quot;chelsa_bio10_18&quot;, &quot;chelsa_prec&quot;, &quot;chelsa_temp&quot;) landscape_names &lt;- map(landscape_data, names) %&gt;% map(function(chr){ as.character(glue(&#39;{str_sub(chr,1,20)}_{c(elev_names, evi_names, chelsa_names, &quot;landcover&quot;)}&#39;))}) # assign names for(i in 1:length(landscape_data)){ names(landscape_data[[i]]) &lt;- landscape_names[[i]] } 5.5 Construct 2.5km buffer around subsampled points # assign neighbourhood radius in m neighbourhood_radius &lt;- 2500 # get distinct points and make buffer ebird_buff &lt;- dataSubsample %&gt;% distinct(locality_id, latitude, longitude) %&gt;% # convert to spatial features st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4326) %&gt;% # transform to modis projection st_transform(crs = 32643) %&gt;% # buffer to create neighborhood around each point st_buffer(dist = neighbourhood_radius) 5.6 Getting area means 5.6.1 Mean environmental covariates All covariates are 2.5km mean values and prefixed âam_â. # get area mean for all preds except landcover, which is the last one env_area_mean &lt;- purrr::map(landscape_data, function(stk){ stk &lt;- stk[[-dim(stk)[3]]] # removing landcover here velstk &lt;- velox(stk) dextr &lt;- velstk$extract(sp = ebird_buff, df = TRUE, fun = function(x)mean(x, na.rm=T)) # assign names for joining names(dextr) &lt;- c(&quot;id&quot;, names(stk)) return(as_tibble(dextr)) }) # run a reduce leftjoin on the data env_area_mean &lt;- purrr::reduce(env_area_mean, left_join, by=&quot;id&quot;) # rename to be clear which are mean-area vars temp_names &lt;- str_split(names(env_area_mean), &quot;resamp&quot;) %&gt;% map(function(x)x[2]) names(env_area_mean) &lt;- c(&quot;id&quot;, glue::glue(&#39;am_{temp_names[-1]}&#39;)) 5.6.2 Proportional landcover # get the last element of each stack from the list # this is the landcover at that resolution lc_area_prop &lt;- purrr::map(landscape_data, function(stk){ lc &lt;- stk[[dim(stk)[3]]] # accessing landcover here lc_velox &lt;- velox(lc) lc_vals &lt;- lc_velox$extract(sp = ebird_buff, df = TRUE) names(lc_vals) &lt;- c(&quot;id&quot;, &quot;lc&quot;) # temporary names here as well temp_name &lt;- str_split(names(lc), &quot;resamp&quot;) %&gt;% map_chr(function(x) x[2]) # get landcover proportions lc_prop &lt;- count(lc_vals, id, lc) %&gt;% group_by(id) %&gt;% mutate(lc = glue(&#39;am_{temp_name}_{str_pad(lc, 2, pad = &quot;0&quot;)}&#39;), prop = n/sum(n)) %&gt;% dplyr::select(-n) %&gt;% tidyr::pivot_wider(names_from = lc, values_from = prop, values_fill = list(prop = 0)) return(lc_prop) }) lc_area_prop &lt;- reduce(lc_area_prop, left_join, by=&quot;id&quot;) 5.7 Add landscape covariates # link back to rand10 via ebird buff using coordinate id # drop geometry and assign id column ebird_buff &lt;- ebird_buff %&gt;% st_drop_geometry() %&gt;% mutate(id = 1:nrow(.)) %&gt;% # merge on id left_join(lc_prop, by = &quot;id&quot;) %&gt;% left_join(env_area_mean, by = &quot;id&quot;) # join to random 10 dataset on locality id dataSubsample &lt;- left_join(dataSubsample, ebird_buff, by = &quot;locality_id&quot;) 5.8 Write random 10 with covariates # write using write_csv write_csv(dataSubsample, &quot;data/dataRand10_withLC.csv&quot;) "],
["occupancy-modelling-here.html", "Section 6 Occupancy modelling here", " Section 6 Occupancy modelling here "],
["supplementary-material.html", "Section 7 Supplementary material 7.1 Distance to roads 7.2 WIP", " Section 7 Supplementary material 7.1 Distance to roads 7.1.1 Prepare libraries # load libraries library(reticulate) library(ggplot2) library(ggthemes) # set python path use_python(&quot;/usr/bin/python3&quot;) Importing python libraries. # import classic python libs import itertools from operator import itemgetter import numpy as np import matplotlib.pyplot as plt import math # libs for dataframes import pandas as pd # import libs for geodata from shapely.ops import nearest_points import geopandas as gpd import rasterio # import ckdtree from scipy.spatial import cKDTree from shapely.geometry import Point, MultiPoint, LineString, MultiLineString 7.1.2 Prepare data for processing # read in roads shapefile roads = gpd.read_file(&quot;data/spatial/roads_studysite_2019/roads_studysite_2019.shp&quot;) roads.head() # read in checklist covariates for conversion to gpd # get unique coordinates, assign them to the df # convert df to geo-df chkCovars = pd.read_csv(&quot;data/eBirdChecklistVars.csv&quot;) unique_locs = chkCovars.drop_duplicates(subset=[&#39;longitude&#39;,&#39;latitude&#39;])[[&#39;longitude&#39;, &#39;latitude&#39;, &#39;nSp&#39;]] unique_locs[&#39;coordId&#39;] = np.arange(1, unique_locs.shape[0]+1) chkCovars = chkCovars.merge(unique_locs, on=[&#39;longitude&#39;, &#39;latitude&#39;]) unique_locs = gpd.GeoDataFrame( unique_locs, geometry=gpd.points_from_xy(unique_locs.longitude, unique_locs.latitude)) unique_locs.crs = {&#39;init&#39; :&#39;epsg:4326&#39;} # reproject spatials to 43n epsg 32642 roads = roads.to_crs({&#39;init&#39;: &#39;epsg:32642&#39;}) unique_locs = unique_locs.to_crs({&#39;init&#39;: &#39;epsg:32642&#39;}) # function to simplify multilinestrings def simplify_roads(complex_roads): simpleRoads = [] for i in range(len(complex_roads.geometry)): feature = complex_roads.geometry.iloc[i] if feature.geom_type == &quot;LineString&quot;: simpleRoads.append(feature) elif feature.geom_type == &quot;MultiLineString&quot;: for road_level2 in feature: simpleRoads.append(road_level2) return simpleRoads # function to use ckdtrees for nearest point finding def ckdnearest(gdfA, gdfB): A = np.concatenate( [np.array(geom.coords) for geom in gdfA.geometry.to_list()]) simplified_features = simplify_roads(gdfB) B = [np.array(geom.coords) for geom in simplified_features] B = np.concatenate(B) ckd_tree = cKDTree(B) dist, idx = ckd_tree.query(A, k=1) return dist # get distance to nearest road unique_locs[&#39;dist_road&#39;] = ckdnearest(unique_locs, roads) # write to file unique_locs = pd.DataFrame(unique_locs.drop(columns=&#39;geometry&#39;)) unique_locs[&#39;dist_road&#39;] = unique_locs[&#39;dist_road&#39;] unique_locs.to_csv(path_or_buf=&quot;data/locs_dist_to_road.csv&quot;, index=False) # merge unique locs with chkCovars chkCovars = chkCovars.merge(unique_locs, on=[&#39;latitude&#39;, &#39;longitude&#39;, &#39;coordId&#39;]) 7.1.3 Plot histogram: distance to roads # extract data from python chkCovars &lt;- py$chkCovars # make histogram hist_roads &lt;- ggplot(chkCovars)+ geom_histogram(aes(dist_road / 1e3), bins = 20, size=0.2, fill=&quot;steelblue&quot;)+ labs(x = &quot;distance to roads (km)&quot;, y = &quot;# checklists&quot;)+ scale_x_log10(label=label_number(accuracy = 0.1), breaks = c(0.1, 1, 10))+ scale_y_continuous(label=label_number(scale=0.001, suffix = &quot;K&quot;))+ theme_few()+ theme(plot.background = element_rect(fill=&quot;white&quot;, colour = 1), panel.background = element_blank(), panel.border = element_blank(), axis.line = element_blank()) 7.1.4 Plot map: points on roads # load data into R library(sf) library(ggspatial) hills &lt;- st_read(&quot;data/spatial/hillsShapefile/Nil_Ana_Pal.shp&quot;) roads &lt;- st_read(&quot;data/spatial/roads_studysite_2019/roads_studysite_2019.shp&quot;) points &lt;- read_csv(&quot;data/locs_dist_to_road.csv&quot;) %&gt;% st_as_sf(coords = c(&quot;longitude&quot;,&quot;latitude&quot;)) %&gt;% `st_crs&lt;-`(4326) # plot on maps ggplot(hills)+ geom_sf(fill=&quot;transparent&quot;, col = 2)+ geom_sf(data=roads, col=&quot;steelblue&quot;, size=0.2)+ geom_sf(data=points, aes(col=dist_road), size = 0.1, col = &quot;grey40&quot;)+ annotation_custom(grob = hist_roads %&gt;% ggplotGrob(), xmin = 77.1, xmax = 77.8, ymin = 10.9, ymax = 11.5)+ annotation_scale(location = &quot;br&quot;, width_hint = 0.4, text_cex = 1) + annotation_north_arrow(location = &quot;br&quot;, which_north = &quot;true&quot;, pad_x = unit(0.1, &quot;in&quot;), pad_y = unit(0.5, &quot;in&quot;), style = north_arrow_fancy_orienteering) + theme_few()+ theme(legend.position = &quot;none&quot;)+ coord_sf(expand = FALSE) # save figure ggsave(filename = &quot;figs/fig_distRoads.png&quot;, device = png()) dev.off() (#fig:plot_figure1)Checklist locations in the Nilgiris. Inset histogram shows checklistsâ distance to the nearest road, with the X-axis on a log-scale. 7.2 WIP "]
]
