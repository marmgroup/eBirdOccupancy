[
["index.html", "Influence of land cover and climate on the occupancy of avian distributions along a tropical montane gradient Section 1 Introduction 1.1 Attribution 1.2 Data access 1.3 Data processing", " Influence of land cover and climate on the occupancy of avian distributions along a tropical montane gradient Vijay Ramesh, Pratik R Gupte, and Morgan Tingley 2020-07-25 Section 1 Introduction This is the bookdown version of a project in preparation that models occupancy for birds in the Nilgiri hills. Methods and format are derived from Strimas-Mackey et al., the supplement to Johnston et al.Â (2019). 1.1 Attribution Please contact the following in case of interest in the project. Vijay Ramesh (lead author) PhD student, Columbia University Pratik Gupte (repo maintainer) PhD student, University of Groningen Morgan Tingley (PI) 1.2 Data access The data used in this work are available from eBird. 1.3 Data processing The data processing for this project is described in the following sections. Navigate through them using the links in the sidebar. "],
["preparing-ebird-data.html", "Section 2 Preparing eBird data 2.1 Prepare libraries and data sources 2.2 Filter data 2.3 Process filtered data 2.4 Spatial filter 2.5 Handle presence data 2.6 Add decimal time 2.7 Write data", " Section 2 Preparing eBird data 2.1 Prepare libraries and data sources # load libs library(tidyverse) library(readr) library(sf) library(auk) library(readxl) # custom sum function sum.no.na &lt;- function(x){sum(x, na.rm = T)} # set file paths for auk functions f_in_ebd &lt;- file.path(&quot;data/ebd_IN_relApr-2020.txt&quot;) f_in_sampling &lt;- file.path(&quot;data/ebd_sampling_relApr-2020.txt&quot;) 2.2 Filter data # add species of interest specieslist &lt;- read.csv(&quot;data/4_List_of_species_final.csv&quot;) speciesOfInterest &lt;- as.character(specieslist$scientific_name) # run filters using auk packages ebd_filters &lt;- auk_ebd(f_in_ebd, f_in_sampling) %&gt;% auk_species(speciesOfInterest) %&gt;% auk_country(country = &quot;IN&quot;) %&gt;% auk_state(c(&quot;IN-KL&quot;,&quot;IN-TN&quot;, &quot;IN-KA&quot;)) %&gt;% # Restricting geography to TamilNadu, Kerala &amp; Karnataka auk_date(c(&quot;2013-01-01&quot;, &quot;2019-12-31&quot;)) %&gt;% auk_complete() # check filters ebd_filters Below code need not be run if it has been filtered once already and the above path leads to the right dataset. NB: This is a computation heavy process, run with caution. # specify output location and perform filter f_out_ebd &lt;- &quot;data/eBirdDataWG_filtered.txt&quot; f_out_sampling &lt;- &quot;data/eBirdSamplingDataWG_filtered.txt&quot; ebd_filtered &lt;- auk_filter(ebd_filters, file = f_out_ebd, file_sampling = f_out_sampling, overwrite = TRUE) 2.3 Process filtered data # read in the data ebd &lt;- read_ebd(f_out_ebd) # fill zeroes zf &lt;- auk_zerofill(f_out_ebd, f_out_sampling) new_zf &lt;- collapse_zerofill(zf) # Creates a new zero-filled dataframe with a 0 marked for each checklist when the bird was not observed # choose columns of interest columnsOfInterest &lt;- c(&quot;checklist_id&quot;, &quot;scientific_name&quot;, &quot;common_name&quot;, &quot;observation_count&quot;, &quot;locality&quot;, &quot;locality_id&quot;, &quot;locality_type&quot;, &quot;latitude&quot;, &quot;longitude&quot;, &quot;observation_date&quot;, &quot;time_observations_started&quot;, &quot;observer_id&quot;, &quot;sampling_event_identifier&quot;, &quot;protocol_type&quot;, &quot;duration_minutes&quot;, &quot;effort_distance_km&quot;, &quot;effort_area_ha&quot;, &quot;number_observers&quot;, &quot;species_observed&quot;, &quot;reviewed&quot;) # make list of presence and absence data and choose cols of interest data &lt;- list(ebd, new_zf) %&gt;% map(function(x){ x %&gt;% select(one_of(columnsOfInterest)) }) # remove zerofills to save working memory rm(zf, new_zf); gc() # check presence and absence in absences df, remove essentially the presences df data[[2]] &lt;- data[[2]] %&gt;% filter(species_observed == F) 2.4 Spatial filter # load shapefiles of hill ranges library(sf) hills &lt;- st_read(&quot;data/spatial/hillsShapefile/Nil_Ana_Pal.shp&quot;) # write a prelim filter by bounding box box &lt;- st_bbox(hills) # get data spatial coordinates dataLocs &lt;- data %&gt;% map(function(x){ select(x, longitude, latitude) %&gt;% filter(between(longitude, box[&quot;xmin&quot;], box[&quot;xmax&quot;]) &amp; between(latitude, box[&quot;ymin&quot;], box[&quot;ymax&quot;]))}) %&gt;% bind_rows() %&gt;% distinct() %&gt;% st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;)) %&gt;% st_set_crs(4326) %&gt;% st_intersection(hills) # get simplified data and drop geometry dataLocs &lt;- mutate(dataLocs, spatialKeep = T) %&gt;% bind_cols(., as_tibble(st_coordinates(dataLocs))) %&gt;% st_drop_geometry() # bind to data and then filter data &lt;- data %&gt;% map(function(x){ left_join(x, dataLocs, by = c(&quot;longitude&quot; = &quot;X&quot;, &quot;latitude&quot; = &quot;Y&quot;)) %&gt;% filter(spatialKeep == T) %&gt;% select(-Id, -spatialKeep) }) # save a temp data file save(data, file = &quot;data/data_temp.rdata&quot;) 2.5 Handle presence data # in the first set, replace X, for presences, with 1 data[[1]] &lt;- data[[1]] %&gt;% mutate(observation_count = ifelse(observation_count == &quot;X&quot;, &quot;1&quot;, observation_count)) # remove records where duration is 0 data &lt;- map(data, function(x) filter(x, duration_minutes &gt; 0)) # group data by site and sampling event identifier # then, summarise relevant variables as the sum dataGrouped &lt;- map(data, function(x){ x %&gt;% group_by(sampling_event_identifier) %&gt;% summarise_at(vars(duration_minutes, effort_distance_km, effort_area_ha), list(sum.no.na)) }) # bind rows combining data frames, and filter dataGrouped &lt;- bind_rows(dataGrouped) %&gt;% filter(duration_minutes &lt;= 300, effort_distance_km &lt;= 5, effort_area_ha &lt;= 500) # get data identifiers, such as sampling identifier etc dataConstants &lt;- data %&gt;% bind_rows() %&gt;% select(sampling_event_identifier, time_observations_started, locality, locality_type, locality_id, observer_id, observation_date, scientific_name, observation_count, protocol_type, number_observers, longitude, latitude) # join the summarised data with the identifiers, # using sampling_event_identifier as the key dataGrouped &lt;- left_join(dataGrouped, dataConstants, by = &quot;sampling_event_identifier&quot;) # remove checklists or seis with more than 10 obervers count(dataGrouped, number_observers &gt; 10) # count how many have 10+ obs dataGrouped &lt;- filter(dataGrouped, number_observers &lt;= 10) 2.6 Add decimal time # assign present or not, and get time in decimal hours since midnight library(lubridate) time_to_decimal &lt;- function(x) { x &lt;- hms(x, quiet = TRUE) hour(x) + minute(x) / 60 + second(x) / 3600 } # will cause issues if using time obs started as a linear effect and not quadratic dataGrouped = mutate(dataGrouped, pres_abs = observation_count &gt;= 1, decimalTime = time_to_decimal(time_observations_started)) # check class of dataGrouped, make sure not sf assertthat::assert_that(!&quot;sf&quot; %in% class(dataGrouped)) 2.7 Write data # save a temp data file save(dataGrouped, file = &quot;data/data_prelim_processing.rdata&quot;) "],
["prepare-landscape-data.html", "Section 3 Prepare landscape data 3.1 Prepare libraries 3.2 Prepare initial data 3.3 Resample rasters", " Section 3 Prepare landscape data 3.1 Prepare libraries # load libs library(raster) library(stringi) library(glue) library(gdalUtils) library(purrr) # prep mode function to aggregate funcMode &lt;- function(x, na.rm = T) { ux &lt;- unique(x) ux[which.max(tabulate(match(x, ux)))] } # a basic test assertthat::assert_that(funcMode(c(2,2,2,2,3,3,3,4)) == as.character(2), msg = &quot;problem in the mode function&quot;) # works 3.2 Prepare initial data 3.2.1 Prepare spatial extent # load hills library(sf) hills &lt;- st_read(&quot;data/spatial/hillsShapefile/Nil_Ana_Pal.shp&quot;) hills &lt;- st_transform(hills, 32643) buffer &lt;- st_buffer(hills, 3e4) %&gt;% st_transform(4326) 3.2.2 Prepare terrain rasters # load elevation and crop to hills size, then mask by hills alt &lt;- raster(&quot;data/spatial/Elevation/alt&quot;) alt.hills &lt;- crop(alt, as(buffer, &quot;Spatial&quot;)) rm(alt); gc() # get slope and aspect slopeData &lt;- terrain(x = alt.hills, opt = c(&quot;slope&quot;, &quot;aspect&quot;)) elevData &lt;- raster::stack(alt.hills, slopeData) rm(alt.hills); gc() 3.2.3 Prepare CHELSA rasters # list chelsa files chelsaFiles &lt;- list.files(&quot;data/chelsa/&quot;, full.names = TRUE, pattern = &quot;*.tif&quot;) # gather chelsa rasters chelsaData &lt;- purrr::map(chelsaFiles, function(chr){ a &lt;- raster(chr) crs(a) &lt;- crs(elevData) a &lt;- crop(a, as(buffer, &quot;Spatial&quot;)) return(a) }) # stack chelsa data chelsaData &lt;- raster::stack(chelsaData) 3.2.4 Stack prepared rasters # stack rasters for efficient reprojection later env_data &lt;- stack(elevData, chelsaData) 3.2.5 Prepare landcover # read in landcover raster location landcover &lt;- &quot;data/landUseClassification/Reprojected Image_26thJan2020_UTM_Ghats.tif&quot; # get extent e = bbox(raster(landcover)) # init resolution res_init &lt;- res(raster(landcover)) # res to transform to 1000m res_final &lt;- res_init*100 # use gdalutils gdalwarp for resampling transform # to 1km from 10m gdalUtils::gdalwarp(srcfile = landcover, dstfile = &quot;data/landUseClassification/lc_01000m.tif&quot;, tr = c(res_final), r= &#39;mode&#39;, te = c(e)) 3.2.6 Show resampled landcover # mask by study area { landcover &lt;- raster(landcover) # landcover &lt;- mask(landcover, mask = as(hills, &quot;Spatial&quot;)) lc_data &lt;- raster(&quot;data/landUseClassification/lc_01000m.tif&quot;) # lc_data &lt;- mask(lc_data, mask = as(hills, &quot;Spatial&quot;)) lc_data[lc_data == 0] &lt;- NA } # make raster barplot data data1km &lt;- raster::getValues(lc_data) data1km &lt;- data1km[data1km &gt; 0] data1km &lt;- table(data1km) data1km &lt;- data1km/sum(data1km) { data10m &lt;- raster::getValues(landcover); data10m = data10m[data10m &gt; 0] data10m &lt;- tibble(value = data10m) data10m &lt;- dplyr::count(data10m, value) %&gt;% dplyr::mutate(n=n/sum(n)) data10m &lt;- xtabs(n~value, data10m) } # map rasters { png(filename = &quot;figs/figLandcoverResample.png&quot;, width = 1200, height = 1200, res = 150) par(mfrow=c(2,2)) # rasterplots raster::plot(landcover, col = c(&quot;white&quot;, scico::scico(palette = &quot;batlow&quot;, 7)), main = &quot;10m sentinel data&quot;, xlab = &quot;longitude&quot;, y = &quot;latitude&quot;) plot(hills, add=T, border = &quot;red&quot;, col = &quot;transparent&quot;) raster::plot(rasterAgg1km, col = c(&quot;white&quot;, scico::scico(palette = &quot;batlow&quot;, 7)), main = &quot;1km resampled data&quot;, xlab = &quot;longitude&quot;, y = &quot;latitude&quot;) plot(hills, add=T, border = &quot;red&quot;, col = &quot;transparent&quot;) # barplots barplot(data10m, xlab = c(&quot;landcover class&quot;), ylab = &quot;prop.&quot;, col = scico::scico(palette = &quot;batlow&quot;, 7)) barplot(data1km, xlab = c(&quot;landcover class&quot;), ylab = &quot;prop.&quot;, col = scico::scico(palette = &quot;batlow&quot;, 7), alpha =0.8, add = F) barplot(data10m, xlab = c(&quot;landcover class&quot;), ylab = &quot;prop.&quot;, col = &quot;grey20&quot;, border = NA, density = 30, add = T) dev.off() } knitr::include_graphics(&quot;figs/figLandcoverResample.png&quot;) 3.3 Resample rasters 3.3.1 Read landcover as list Here, we read in the 1km landcover raster and set 0 to NA. lc_data &lt;- raster(&quot;data/landUseClassification/lc_01000m.tif&quot;) lc_data[lc_data == 0] &lt;- NA 3.3.2 Reproject environmental data to landcover # resample to the corresponding landcover data env_data_resamp &lt;- projectRaster(from = env_data, to = lc_data, crs = crs(lc_data), res = res(lc_data)) # export as raster stack land_stack &lt;- stack(env_data_resamp, lc_data) # get names land_names &lt;- glue(&#39;data_spatial/landscape_resamp{c(&quot;01&quot;)}km.tif&#39;) # write to file writeRaster(land_stack, filename = as.character(land_names), overwrite=TRUE) "],
["prepare-observer-expertise.html", "Section 4 Prepare observer expertise 4.1 Prepare libraries 4.2 Prepare data 4.3 Explicit spatial subset 4.4 Prepare species of interest 4.5 Prepare checklists for observer score 4.6 Get landcover 4.7 Filter data for stats 4.8 Run observer expertise model 4.9 Write model to file 4.10 Get observer expertise as species in 60 mins 4.11 Write observer expertise to file", " Section 4 Prepare observer expertise 4.1 Prepare libraries # load libs library(data.table) library(readxl) library(magrittr) library(stringr) library(dplyr) library(tidyr) library(auk) # get decimal time function library(lubridate) time_to_decimal &lt;- function(x) { x &lt;- lubridate::hms(x, quiet = TRUE) lubridate::hour(x) + lubridate::minute(x) / 60 + lubridate::second(x) / 3600 } 4.2 Prepare data Here, we go through the data preparation process again because we might want to assess observer expertise over a larger area than the study site. # Read in shapefile of study area to subset by bounding box library(sf) wg &lt;- st_read(&quot;data/spatial/hillsShapefile/Nil_Ana_Pal.shp&quot;) %&gt;% st_transform(32643) # set file paths for auk functions f_in_ebd &lt;- file.path(&quot;data/eBirdDataWG_filtered.txt&quot;) f_in_sampling &lt;- file.path(&quot;data/eBirdSamplingDataWG_filtered.txt&quot;) # run filters using auk packages ebd_filters &lt;- auk_ebd(f_in_ebd, f_in_sampling) %&gt;% auk_country(country = &quot;IN&quot;) %&gt;% auk_state(c(&quot;IN-KL&quot;,&quot;IN-TN&quot;, &quot;IN-KA&quot;)) %&gt;% # Restricting geography to TamilNadu, Kerala &amp; Karnataka auk_date(c(&quot;2013-01-01&quot;, &quot;2018-12-31&quot;)) %&gt;% auk_complete() # check filters ebd_filters # specify output location and perform filter f_out_ebd &lt;- &quot;data/ebird_for_expertise.txt&quot; f_out_sampling &lt;- &quot;data/ebird_sampling_for_expertise.txt&quot; ebd_filtered &lt;- auk_filter(ebd_filters, file = f_out_ebd, file_sampling = f_out_sampling, overwrite = TRUE) ## Process filtered data # read in the data ebd &lt;- fread(f_out_ebd) names &lt;- names(ebd) %&gt;% stringr::str_to_lower() %&gt;% stringr::str_replace_all(&quot; &quot;, &quot;_&quot;) setnames(ebd, names) # choose columns of interest columnsOfInterest &lt;- c(&quot;checklist_id&quot;,&quot;scientific_name&quot;,&quot;observation_count&quot;, &quot;locality&quot;,&quot;locality_id&quot;,&quot;locality_type&quot;,&quot;latitude&quot;, &quot;longitude&quot;,&quot;observation_date&quot;, &quot;time_observations_started&quot;,&quot;observer_id&quot;, &quot;sampling_event_identifier&quot;,&quot;protocol_type&quot;, &quot;duration_minutes&quot;,&quot;effort_distance_km&quot;,&quot;effort_area_ha&quot;, &quot;number_observers&quot;,&quot;species_observed&quot;,&quot;reviewed&quot;) ebd &lt;- setDF(ebd) %&gt;% as_tibble() %&gt;% select(one_of(columnsOfInterest)) setDT(ebd) 4.3 Explicit spatial subset # get checklist locations ebd_locs &lt;- ebd[,.(longitude, latitude)] ebd_locs &lt;- setDF(ebd_locs) %&gt;% distinct() ebd_locs &lt;- st_as_sf(ebd_locs, coords = c(&quot;longitude&quot;,&quot;latitude&quot;)) %&gt;% `st_crs&lt;-`(4326) %&gt;% bind_cols(as_tibble(st_coordinates(.))) %&gt;% st_transform(32643) %&gt;% mutate(id = 1:nrow(.)) # check whether to include to_keep &lt;- unlist(st_contains(wg, ebd_locs)) # filter locs ebd_locs &lt;- filter(ebd_locs, id %in% to_keep) %&gt;% bind_cols(as_tibble(st_coordinates(.))) %&gt;% st_drop_geometry() ebd &lt;- ebd[longitude %in% ebd_locs$X &amp; latitude %in% ebd_locs$Y,] 4.4 Prepare species of interest # read in species list specieslist = read.csv(&quot;data/species_list.csv&quot;) # set species of interest soi = specieslist$scientific_name ebdSpSum &lt;- ebd[,.(nSp = .N, totSoiSeen = length(intersect(scientific_name, soi))), by = list(sampling_event_identifier)] # write to file and link with checklsit id later fwrite(ebdSpSum, file = &quot;data/dataChecklistSpecies.csv&quot;) 4.5 Prepare checklists for observer score # 1. add new columns of decimal time and julian date ebd[,`:=`(decimalTime = time_to_decimal(time_observations_started), julianDate = yday(as.POSIXct(observation_date)))] ebdEffChk &lt;- setDF(ebd) %&gt;% mutate(year = year(observation_date)) %&gt;% distinct(sampling_event_identifier, observer_id, year, duration_minutes, effort_distance_km, effort_area_ha, longitude, latitude, locality, locality_id, decimalTime, julianDate, number_observers) %&gt;% # drop rows with NAs in cols used in the model tidyr::drop_na(sampling_event_identifier, observer_id, duration_minutes, decimalTime, julianDate) %&gt;% # drop years below 2013 filter(year &gt;= 2013) # 3. join to covariates and remove large groups (&gt; 10) ebdChkSummary &lt;- inner_join(ebdEffChk, ebdSpSum) # remove ebird data rm(ebd); gc() 4.6 Get landcover Using the landcover 1km resolution here. # read in 1km landcover and set 0 to NA library(raster) landcover &lt;- raster::raster(&quot;data/landUseClassification/lc_01000m.tif&quot;) landcover[landcover==0] &lt;- NA # get locs in utm coords locs &lt;- distinct(ebdChkSummary, sampling_event_identifier, longitude, latitude, locality, locality_id) locs &lt;- st_as_sf(locs, coords = c(&quot;longitude&quot;, &quot;latitude&quot;)) %&gt;% `st_crs&lt;-`(4326) %&gt;% st_transform(32643) %&gt;% st_coordinates() # get for unique points landcoverVec &lt;- raster::extract(x = landcover, y = locs) # assign to df and overwrite setDT(ebdChkSummary)[,landcover:= landcoverVec] 4.7 Filter data for stats # change names for easy handling setnames(ebdChkSummary, c(&quot;sei&quot;, &quot;observer&quot;,&quot;year&quot;, &quot;duration&quot;, &quot;distance&quot;, &quot;area&quot;, &quot;longitude&quot;, &quot;latitude&quot;, &quot;locality&quot;, &quot;locality_id&quot;, &quot;decimalTime&quot;, &quot;julianDate&quot;, &quot;nObs&quot;, &quot;nSp&quot;, &quot;nSoi&quot;, &quot;landcover&quot;)) # count data points per observer obscount &lt;- count(ebdChkSummary, observer) %&gt;% filter(n &gt;= 10) # make factor variables and remove obs not in obscount # also remove 0 durations ebdChkSummary &lt;- ebdChkSummary %&gt;% mutate(distance = ifelse(is.na(distance), 0, distance), duration = if_else(is.na(duration), 0.0, as.double(duration))) %&gt;% filter(observer %in% obscount$observer, duration &gt; 0, duration &lt;= 300, nSoi &gt;= 0, distance &lt;= 5, !is.na(nSoi)) %&gt;% mutate(landcover = as.factor(landcover), observer = as.factor(observer)) %&gt;% drop_na(landcover) # save to file for later reuse fwrite(ebdChkSummary, file = &quot;data/eBirdChecklistVars.csv&quot;) 4.8 Run observer expertise model Our observer expertise model aims to include the random intercpet effect of observer identity, with a random slope effect of duration. This models the different rate of species accumulation by different observers, as well as their different starting points. # uses either a subset or all data library(lmerTest) # here we specify a glmm with random effects for observer # time is considered a fixed log predictor and a random slope modObsExp &lt;- glmer(nSoi ~ sqrt(duration) + landcover+ sqrt(decimalTime) + I((sqrt(decimalTime))^2) + log(julianDate) + I((log(julianDate)^2)) + (1|observer) + (0+duration|observer), data = ebdChkSummary, family = &quot;poisson&quot;) 4.9 Write model to file # make dir if absent if(!dir.exists(&quot;data/modOutput&quot;)){ dir.create(&quot;data/modOutput&quot;) } # write model output to text file { writeLines(R.utils::captureOutput(list(Sys.time(), summary(modObsExp))), con = &quot;data/modOutput/modOutExpertise.txt&quot;) } 4.10 Get observer expertise as species in 60 mins # make df with means observer &lt;- unique(ebdChkSummary$observer) # predict at 60 mins on the most common landcover dfPredict &lt;- ebdChkSummary %&gt;% summarise_at(vars(duration, decimalTime, julianDate), list(~mean(.))) %&gt;% mutate(duration = 60, landcover = as.factor(6)) %&gt;% tidyr::crossing(observer) # run predict from model on it dfPredict &lt;- mutate(dfPredict, score = predict(modObsExp, newdata = dfPredict, type = &quot;response&quot;, allow.new.levels = TRUE)) %&gt;% mutate(score = scales::rescale(score)) 4.11 Write observer expertise to file fwrite(dfPredict %&gt;% dplyr::select(observer, score), file = &quot;data/dataObsExpScore.csv&quot;) "],
["add-covariates-to-subsampled-data.html", "Section 5 Add covariates to subsampled data 5.1 Prepare libraries and data 5.2 Spatial subsampling 5.3 Temporal subsampling 5.4 Add expertise score 5.5 Add landscape covariates 5.6 Construct buffers around subsampled points 5.7 Getting area means 5.8 Join landscape data to obs data 5.9 Write data to files", " Section 5 Add covariates to subsampled data 5.1 Prepare libraries and data # load libs library(dplyr); library(readr) library(stringr) library(purrr) library(raster) library(glue) library(velox) library(tidyr) library(sf) # load saved data object load(&quot;data/data_prelim_processing.rdata&quot;) 5.2 Spatial subsampling # grid based spatial thinning gridsize = 1000 # grid size in metres effort_distance_max = 1000 # removing checklists with this distance # make grids across the study site hills &lt;- st_read(&quot;data/spatial/hillsShapefile/Nil_Ana_Pal.shp&quot;) %&gt;% st_transform(32643) grid &lt;- st_make_grid(hills, cellsize = gridsize) # split data by species data_spatial_thin &lt;- split(x = dataGrouped, f = dataGrouped$scientific_name) # spatial thinning on each species retains # site with maximum visits per grid cell data_spatial_thin &lt;- map(data_spatial_thin, function(df){ # count visits per locality df &lt;- group_by(df, locality) %&gt;% mutate(tot_effort = length(sampling_event_identifier)) %&gt;% ungroup() # remove sites with distances above spatial independence df &lt;- df %&gt;% filter(effort_distance_km &lt;= effort_distance_max) %&gt;% st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;)) %&gt;% `st_crs&lt;-`(4326) %&gt;% st_transform(32643) %&gt;% mutate(coordId = 1:nrow(.)) %&gt;% bind_cols(as_tibble(st_coordinates(.))) # whcih cell has which coords grid_contents &lt;- st_contains(grid, df) %&gt;% as_tibble() %&gt;% rename(cell = row.id, coordId = col.id) # what&#39;s the max point in each grid points_max &lt;- left_join(df %&gt;% st_drop_geometry(), grid_contents, by = &quot;coordId&quot;) %&gt;% group_by(cell) %&gt;% filter(tot_effort == max(tot_effort)) return(points_max) }) # remove old data rm(dataGrouped) 5.3 Temporal subsampling Get 10 random (if available) observations of each species at each locality. # subsample data for random 10 observations dataSubsample &lt;- map(data_spatial_thin, function(df){ df &lt;- ungroup(df) df_to_locality &lt;- split(x = df, f = df$locality) df_samples &lt;- map_if(.x = df_to_locality, .p = function(x) {nrow(x) &gt; 10}, .f = function(x) sample_n(x, 10, replace = FALSE)) return(bind_rows(df_samples)) }) # bind all rows for data frame dataSubsample &lt;- bind_rows(dataSubsample) # remove previous data rm(data_spatial_thin) 5.4 Add expertise score # read in obs score and extract numbers expertiseScore &lt;- read_csv(&quot;data/dataObsExpScore.csv&quot;) %&gt;% mutate(numObserver = str_extract(observer, &quot;\\\\d+&quot;)) %&gt;% dplyr::select(-observer) # group seis consist of multiple observers # in this case, seis need to have the highest expertise observer score # as the associated covariate # get unique observers per sei dataSeiScore &lt;- distinct(dataSubsample, sampling_event_identifier, observer_id) %&gt;% # make list column of observers mutate(observers = str_split(observer_id, &quot;,&quot;)) %&gt;% unnest(cols = c(observers)) %&gt;% # add numeric observer id mutate(numObserver = str_extract(observers, &quot;\\\\d+&quot;)) %&gt;% # now get distinct sei and observer id numeric distinct(sampling_event_identifier, numObserver) # now add expertise score to sei dataSeiScore &lt;- left_join(dataSeiScore, expertiseScore, by=&quot;numObserver&quot;) %&gt;% # get max expertise score per sei group_by(sampling_event_identifier) %&gt;% summarise(expertise = max(score)) # add to dataCovar dataSubsample &lt;- left_join(dataSubsample, dataSeiScore, by = &quot;sampling_event_identifier&quot;) # remove data without expertise score dataSubsample &lt;- filter(dataSubsample, !is.na(expertise)) 5.5 Add landscape covariates # list landscape covariate stacks landscape_files &lt;- &quot;data/landcover\\\\landscape_resamp01km.tif&quot; # read in as stacks landscape_data &lt;- stack(landscape_files) # get proper names elev_names &lt;- c(&quot;elev&quot;, &quot;slope&quot;, &quot;aspect&quot;) chelsa_names &lt;- c(&quot;bio1&quot;,&quot;bio2&quot;,&quot;bio3&quot;,&quot;bio4&quot;,&quot;bio5&quot;,&quot;bio6&quot;,&quot;bio7&quot;,&quot;bio8&quot;,&quot;bio9&quot;, &quot;bio10&quot;,&quot;bio11&quot;,&quot;bio12&quot;,&quot;bio13&quot;,&quot;bio14&quot;,&quot;bio15&quot;,&quot;bio16&quot;,&quot;bio17&quot;, &quot;bio18&quot;,&quot;bio19&quot;) names(landscape_data) &lt;- as.character(glue(&#39;{c(elev_names, chelsa_names, &quot;landcover&quot;)}&#39;)) 5.6 Construct buffers around subsampled points # assign neighbourhood radius in m sample_radius &lt;- c(2.5, 10) * 1e3 # get distinct points and make buffer ebird_buff &lt;- dataSubsample %&gt;% ungroup() %&gt;% distinct(X, Y) %&gt;% mutate(id = 1:nrow(.)) %&gt;% crossing(sample_radius) %&gt;% arrange(id) %&gt;% group_by(sample_radius) %&gt;% nest() %&gt;% ungroup() # convert to spatial features ebird_buff &lt;- mutate(ebird_buff, data = map2(data, sample_radius, function(df,rd){ df_sf &lt;- st_as_sf(df, coords = c(&quot;X&quot;, &quot;Y&quot;), crs = 32643) %&gt;% # add long lat bind_cols(as_tibble(st_coordinates(.))) %&gt;% # rename(longitude = X, latitude = Y) %&gt;% # # transform to modis projection # st_transform(crs = 32643) %&gt;% # buffer to create neighborhood around each point st_buffer(dist = rd) })) 5.7 Getting area means 5.7.1 Mean environmental covariates All covariates are 2.5km mean values and prefixed âam_â. # get area mean for all preds except landcover, which is the last one env_area_mean &lt;- purrr::map(ebird_buff$data, function(df){ stk &lt;- landscape_data[[-dim(landscape_data)[3]]] # removing landcover here velstk &lt;- velox(stk) dextr &lt;- velstk$extract(sp = df, df = TRUE, fun = function(x)mean(x, na.rm=T)) # assign names for joining names(dextr) &lt;- c(&quot;id&quot;, names(stk)) return(as_tibble(dextr)) }) # join to buffer data ebird_buff &lt;- ebird_buff %&gt;% mutate(data = map2(data, env_area_mean, inner_join, by = &quot;id&quot;)) 5.7.2 Proportional landcover # get the last element of each stack from the list # this is the landcover at that resolution lc_area_prop &lt;- purrr::map(ebird_buff$data, function(df){ lc &lt;- landscape_data[[dim(landscape_data)[3]]] # accessing landcover here lc_velox &lt;- velox(lc) lc_vals &lt;- lc_velox$extract(sp = df, df = TRUE) names(lc_vals) &lt;- c(&quot;id&quot;, &quot;lc&quot;) # get landcover proportions lc_prop &lt;- count(lc_vals, id, lc) %&gt;% group_by(id) %&gt;% mutate(lc = glue(&#39;lc_{str_pad(lc, 2, pad = &quot;0&quot;)}&#39;), prop = n/sum(n)) %&gt;% dplyr::select(-n) %&gt;% tidyr::pivot_wider(names_from = lc, values_from = prop, values_fill = list(prop = 0)) %&gt;% ungroup() return(lc_prop) }) # join to data ebird_buff &lt;- ebird_buff %&gt;% mutate(data = map2(data, lc_area_prop, inner_join, by = &quot;id&quot;)) 5.8 Join landscape data to obs data # duplicate scale data data_at_scale &lt;- ebird_buff # join the full data to landscape samples at each scale data_at_scale$data &lt;- map(data_at_scale$data, function(df){ df &lt;- st_drop_geometry(df) df &lt;- inner_join(dataSubsample, df, by=c(&quot;X&quot;, &quot;Y&quot;)) return(df) }) 5.9 Write data to files # write to file pmap(data_at_scale, function(sample_radius, data){ write_csv(data, path = glue(&#39;data/dataCovars_{str_pad(sample_radius/1e3, 2, pad = &quot;0&quot;)}km.csv&#39;)) message(glue(&#39;export done: data/dataCovars_{str_pad(sample_radius/1e3, 2, pad = &quot;0&quot;)}km.csv&#39;)) }) "],
["occupancy-modelling.html", "Section 6 Occupancy modelling 6.1 Load necessary libraries 6.2 Load dataframe and scale covariates 6.3 Running a null model 6.4 Identifying covariates necessary to model the detection process 6.5 Land Cover and Climate", " Section 6 Occupancy modelling 6.1 Load necessary libraries # Load libraries library(auk) library(lubridate) library(sf) library(unmarked) library(raster) library(ebirdst) library(MuMIn) library(AICcmodavg) library(fields) library(tidyverse) library(doParallel) library(snow) library(openxlsx) library(data.table) library(dplyr) library(ecodist) # Source necessary functions source(&quot;code/fun_screen_cor.R&quot;) source(&quot;code/fun_model_estimate_collection.r&quot;) 6.2 Load dataframe and scale covariates Here, we load the required dataframe that contains 10 random visits to a site. Please note that this process is repeated for each dataframe where environmental covariates were prepared at a spatial scale of 2.5 and 10 sq.km around each unique locality. We also scaled all covariates (mean around 0 and standard deviation of 1) # Load in the prepared dataframe that contains 10 random visits to each site dat &lt;- fread(&quot;data/dataCovars_10km.csv&quot;,header=T) setDF(dat) head(dat) # Some more pre-processing to get the right data structures # Ensuring that only Traveling and Stationary checklists were considered names(dat) dat &lt;- dat %&gt;% filter(protocol_type %in% c(&quot;Traveling&quot;, &quot;Stationary&quot;)) # We take all stationary counts and give them a distance of 100 m (so 0.1 km), # as that&#39;s approximately the max normal hearing distance for people doing point counts. dat &lt;- dat %&gt;% mutate(effort_distance_km = replace(effort_distance_km, which(effort_distance_km == 0 &amp; protocol_type == &quot;Stationary&quot;), 0.1)) # Converting time observations started to numeric and adding it as a new column # This new column will be minute_observations_started dat &lt;- dat %&gt;% mutate(min_obs_started= strtoi(as.difftime(time_observations_started, format = &quot;%H:%M:%S&quot;, units = &quot;mins&quot;))) # Adding the julian date to the dataframe dat &lt;- dat %&gt;% mutate(julian_date = lubridate::yday(dat$observation_date)) # Removing other unnecessary columns from the dataframe and creating a clean one without the rest names(dat) dat &lt;- dat[,-c(1,4,5,16,18,21,23,24,25,26,28:37,39:45,47)] # Rename column names: names(dat) &lt;- c(&quot;duration_minutes&quot;, &quot;effort_distance_km&quot;,&quot;locality&quot;, &quot;locality_type&quot;,&quot;locality_id&quot;, &quot;observer_id&quot;, &quot;observation_date&quot;, &quot;scientific_name&quot;, &quot;observation_count&quot;, &quot;protocol_type&quot;,&quot;number_observers&quot;,&quot;pres_abs&quot;,&quot;tot_effort&quot;, &quot;longitude&quot;,&quot;latitude&quot;,&quot;expertise&quot;,&quot;bio_1.y&quot;,&quot;bio_12.y&quot;, &quot;lc_02.y&quot;,&quot;lc_06.y&quot;,&quot;lc_01.y&quot;, &quot;lc_07.y&quot;,&quot;lc_04.y&quot;, &quot;lc_05.y&quot;, &quot;min_obs_started&quot;, &quot;julian_date&quot;) dat.1 &lt;- dat %&gt;% mutate(year = year(observation_date), pres_abs = as.integer(pres_abs)) # occupancy modeling requires an integer response dat.1$bio_1.y &lt;- dat.1$bio_1.y/10 # Scaling detection and occupancy covariates dat.scaled &lt;- dat.1 dat.scaled[,c(1,2,11,16:25)] &lt;- scale(dat.scaled[,c(1,2,11,16:25)]) # Scaling and standardizing detection and site-level covariates fwrite(dat.scaled, file = &quot;data/scaledCovs_10km.csv&quot;) dat.scaled &lt;- fread(&quot;data/scaledCovs_2.5km.csv&quot;,header=T) setDF(dat.scaled) head(dat.scaled) # Ensure observation_date column is in the right format dat.scaled$observation_date &lt;- format(as.Date(dat.scaled$observation_date, &quot;%m/%d/%Y&quot;), &#39;%Y-%m-%d&#39;) # Testing for correlations before running further analyses # Most are uncorrelated since we decided to keep only 2 climatic and 6 land cover predictors source(&quot;code/screen_cor.R&quot;) names(dat.scaled) screen.cor(dat.scaled[,c(1,2,11,16:25)], threshold = 0.5) 6.3 Running a null model # All null models are stored in lists below all_null &lt;- list() # Add a progress bar for the loop pb &lt;- txtProgressBar(min = 0, max = length(unique(dat.scaled$scientific_name)), style = 3) #text based bar for(i in 1:length(unique(dat.scaled$scientific_name))){ data &lt;- dat.scaled %&gt;% filter(dat.scaled$scientific_name==unique(dat.scaled$scientific_name)[i]) # Preparing data for the unmarked model occ &lt;- filter_repeat_visits(data, min_obs = 1, max_obs = 10, annual_closure = FALSE, n_days = 2600, # 7 years is considered a period of closure date_var = &quot;observation_date&quot;, site_vars = c(&quot;locality_id&quot;)) obs_covs &lt;- c(&quot;min_obs_started&quot;, &quot;duration_minutes&quot;, &quot;effort_distance_km&quot;, &quot;number_observers&quot;, &quot;protocol_type&quot;, &quot;expertise&quot;, &quot;julian_date&quot;) # format for unmarked occ_wide &lt;- format_unmarked_occu(occ, site_id = &quot;site&quot;, response = &quot;pres_abs&quot;, site_covs = c( &quot;locality_id&quot;,&quot;lc_01.y&quot;,&quot;lc_02.y&quot;, &quot;lc_04.y&quot;, &quot;lc_05.y&quot;, &quot;lc_06.y&quot;, &quot;lc_07.y&quot;,&quot;bio_1.y&quot;, &quot;bio_12.y&quot;), obs_covs = obs_covs) # Convert this dataframe of observations into an unmarked object to start fitting occupancy models occ_um &lt;- formatWide(occ_wide, type = &quot;unmarkedFrameOccu&quot;) # Set up the model all_null[[i]] &lt;- occu(~1~1, data = occ_um) names(all_null)[i] &lt;- unique(dat.scaled$scientific_name)[i] setTxtProgressBar(pb, i) } close(pb) # Store all the model outputs for each species capture.output(all_null ,file=&quot;C:\\\\Users\\\\vr235\\\\Downloads\\\\null_models.csv&quot;) 6.4 Identifying covariates necessary to model the detection process Here, we use the unmarked package in R (Fiske and Chandler 2019) to identify detection level covariates that are important for each species # All models are stored in lists below det_dred &lt;- list() # Subsetting those models whose deltaAIC&lt;4 (Burnham et al., 2011) top_det &lt;- list() # Getting model averaged coefficients and relative importance scores det_avg &lt;- list() det_imp &lt;- list() # Getting model estimates det_modelEst &lt;- list() # Add a progress bar for the loop pb &lt;- txtProgressBar(min = 0, max = length(unique(dat.scaled$scientific_name)), style = 3) #text based bar for(i in 1:length(unique(dat.scaled$scientific_name))){ data &lt;- dat.scaled %&gt;% filter(dat.scaled$scientific_name==unique(dat.scaled$scientific_name)[i]) # Preparing data for the unmarked model occ &lt;- filter_repeat_visits(data, min_obs = 1, max_obs = 10, annual_closure = FALSE, n_days = 2600, # 6 years is considered a period of closure date_var = &quot;observation_date&quot;, site_vars = c(&quot;locality_id&quot;)) obs_covs &lt;- c(&quot;min_obs_started&quot;, &quot;duration_minutes&quot;, &quot;effort_distance_km&quot;, &quot;number_observers&quot;, &quot;protocol_type&quot;, &quot;expertise&quot;, &quot;julian_date&quot;) # format for unmarked occ_wide &lt;- format_unmarked_occu(occ, site_id = &quot;site&quot;, response = &quot;pres_abs&quot;, site_covs = c( &quot;locality_id&quot;,&quot;lc_01.y&quot;,&quot;lc_02.y&quot;,&quot;lc_04.y&quot;, &quot;lc_05.y&quot;, &quot;lc_06.y&quot;, &quot;lc_07.y&quot;,&quot;bio_1.y&quot;,&quot;bio_12.y&quot;),obs_covs = obs_covs) # Convert this dataframe of observations into an unmarked object to start fitting occupancy models occ_um &lt;- formatWide(occ_wide, type = &quot;unmarkedFrameOccu&quot;) # Fit a global model with all detection level covariates global_mod &lt;- occu(~ min_obs_started+ julian_date + duration_minutes + effort_distance_km + number_observers + protocol_type + expertise ~ 1, data = occ_um) # Set up the cluster clusterType &lt;- if(length(find.package(&quot;snow&quot;, quiet = TRUE))) &quot;SOCK&quot; else &quot;PSOCK&quot; clust &lt;- try(makeCluster(getOption(&quot;cl.cores&quot;, 6), type = clusterType)) clusterEvalQ(clust, library(unmarked)) clusterExport(clust, &quot;occ_um&quot;) # Dredging the same det_dred[[i]] &lt;- pdredge(global_mod, clust) names(det_dred)[i] &lt;- unique(dat.scaled$scientific_name)[i] # Get the top models, which we&#39;ll define as those with deltaAICc &lt; 2 top_det[[i]] &lt;- get.models(det_dred[[i]], subset = delta &lt; 2, cluster = clust) names(top_det)[i] &lt;- unique(dat.scaled$scientific_name)[i] # Obtaining model averaged coefficients if(length(top_det[[i]])&gt;1){ a &lt;- model.avg(top_det[[i]], fit = TRUE) det_avg[[i]] &lt;- as.data.frame(a$coefficients) names(det_avg)[i] &lt;- unique(dat.scaled$scientific_name)[i] det_modelEst[[i]] &lt;- data.frame(Coefficient = coefTable(a, full = T)[,1], SE = coefTable(a, full = T)[,2], lowerCI = confint(a)[,1], upperCI = confint(a)[,2], z_value = (summary(a)$coefmat.full)[,3], Pr_z = (summary(a)$coefmat.full)[,4]) names(det_modelEst)[i] &lt;- unique(dat.scaled$scientific_name)[i] det_imp[[i]] &lt;- as.data.frame(MuMIn::importance(a)) names(det_imp)[i] &lt;- unique(dat.scaled$scientific_name)[i] } else { det_avg[[i]] &lt;- as.data.frame(unmarked::coef(top_det[[i]][[1]])) names(det_avg)[i] &lt;- unique(dat.scaled$scientific_name)[i] lowDet &lt;- data.frame(lowerCI=confint(top_det[[i]][[1]], type=&quot;det&quot;)[,1]) upDet &lt;- data.frame(upperCI=confint(top_det[[i]][[1]], type=&quot;det&quot;)[,2]) zDet &lt;- data.frame(summary(top_det[[i]][[1]])$det[,3]) Pr_zDet &lt;- data.frame(summary(top_det[[i]][[1]])$det[,4]) Coefficient = coefTable(top_det[[i]][[1]])[,1] SE = coefTable(top_det[[i]][[1]])[,2] det_modelEst[[i]] &lt;- data.frame(Coefficient = Coefficient[2:9], SE = SE[2:9], lowerCI = lowDet, upperCI = upDet, z_value = zDet, Pr_z = Pr_zDet) names(det_modelEst)[i] &lt;- unique(dat.scaled$scientific_name)[i] } setTxtProgressBar(pb, i) stopCluster(clust) } close(pb) ## Storing output from the above models in excel sheets # 1. Store all the dredged model outputs for each species (variable: det_dred() - see above) write.xlsx(det_dred ,file=&quot;C:\\\\Users\\\\vr235\\\\Downloads\\\\det_dred.xlsx&quot;) # 2. Store all the model averaged outputs for each species and the relative importance score write.xlsx(det_avg, file = &quot;C:\\\\Users\\\\vr235\\\\Downloads\\\\det_avg.xlsx&quot;, rowNames=T, colNames=T) write.xlsx(det_imp, file = &quot;C:\\\\Users\\\\vr235\\\\Downloads\\\\det_imp.xlsx&quot;, rowNames=T, colNames=T) write.xlsx(det_modelEst, file = &quot;C:\\\\Users\\\\vr235\\\\Downloads\\\\det_modelEst.xlsx&quot;, rowNames=T, colNames=T) 6.5 Land Cover and Climate # All dredged models are stored in lists below lc_clim &lt;- list() # Subsetting those models whose deltaAIC&lt;2 (Burnham et al., 2011) top_lc_clim &lt;- list() # Getting model averaged coefficients and relative importance scores lc_clim_avg &lt;- list() lc_clim_imp &lt;- list() # Storing Model estimates lc_clim_modelEst &lt;- list() # Add a progress bar for the loop pb &lt;- txtProgressBar(min = 0, max = length(unique(dat.scaled$scientific_name)), style = 3) #text based bar for(i in 1:length(unique(dat.scaled$scientific_name))){ data &lt;- dat.scaled %&gt;% filter(dat.scaled$scientific_name==unique(dat.scaled$scientific_name)[i]) # Preparing data for the unmarked model occ &lt;- filter_repeat_visits(data, min_obs = 1, max_obs = 10, annual_closure = FALSE, n_days = 2600, # 6 years is considered a period of closure date_var = &quot;observation_date&quot;, site_vars = c(&quot;locality_id&quot;)) obs_covs &lt;- c(&quot;min_obs_started&quot;, &quot;duration_minutes&quot;, &quot;effort_distance_km&quot;, &quot;number_observers&quot;, &quot;protocol_type&quot;, &quot;expertise&quot;, &quot;julian_date&quot;) # format for unmarked occ_wide &lt;- format_unmarked_occu(occ, site_id = &quot;site&quot;, response = &quot;pres_abs&quot;, site_covs = c( &quot;locality_id&quot;,&quot;lc_01.y&quot;,&quot;lc_02.y&quot;,&quot;lc_04.y&quot;, &quot;lc_05.y&quot;, &quot;lc_06.y&quot;, &quot;lc_07.y&quot;,&quot;bio_1.y&quot;, &quot;bio_12.y&quot;), obs_covs = obs_covs) # Convert this dataframe of observations into an unmarked object to start fitting occupancy models occ_um &lt;- formatWide(occ_wide, type = &quot;unmarkedFrameOccu&quot;) model_lc_clim &lt;- occu(~min_obs_started+ julian_date + duration_minutes + effort_distance_km + number_observers + protocol_type + expertise~lc_01.y+lc_02.y+lc_04.y+ lc_05.y+ lc_06.y+ lc_07.y+bio_1.y+bio_12.y, data = occ_um) # Set up the cluster clusterType &lt;- if(length(find.package(&quot;snow&quot;, quiet = TRUE))) &quot;SOCK&quot; else &quot;PSOCK&quot; clust &lt;- try(makeCluster(getOption(&quot;cl.cores&quot;, 6), type = clusterType)) clusterEvalQ(clust, library(unmarked)) clusterExport(clust, &quot;occ_um&quot;) # Detection terms are fixed det_terms &lt;- c(&quot;p(duration_minutes)&quot;,&quot;p(effort_distance_km)&quot;, &quot;p(expertise)&quot;,&quot;p(julian_date)&quot;,&quot;p(min_obs_started)&quot;, &quot;p(number_observers)&quot;,&quot;p(protocol_type)&quot;) # Dredging lc_clim[[i]] &lt;- pdredge(model_lc_clim, clust, fixed=det_terms) names(lc_clim)[i] &lt;- unique(dat.scaled$scientific_name)[i] # Identiying top subset of models based on deltaAIC scores being less than 2 (Burnham et al., 2011) top_lc_clim[[i]] &lt;- get.models(lc_clim[[i]], subset = delta&lt;2, cluster = clust) names(top_lc_clim)[i] &lt;- unique(dat.scaled$scientific_name)[i] # Obtaining model averaged coefficients for both candidate model subsets if(length(top_lc_clim[[i]])&gt;1){ a &lt;- model.avg(top_lc_clim[[i]], fit = TRUE) lc_clim_avg[[i]] &lt;- as.data.frame(a$coefficients) names(lc_clim_avg)[i] &lt;- unique(dat.scaled$scientific_name)[i] lc_clim_modelEst[[i]] &lt;- data.frame(Coefficient = coefTable(a, full = T)[,1], SE = coefTable(a, full = T)[,2], lowerCI = confint(a)[,1], upperCI = confint(a)[,2], z_value = (summary(a)$coefmat.full)[,3], Pr_z = (summary(a)$coefmat.full)[,4]) names(lc_clim_modelEst)[i] &lt;- unique(dat.scaled$scientific_name)[i] lc_clim_imp[[i]] &lt;- as.data.frame(MuMIn::importance(a)) names(lc_clim_imp)[i] &lt;- unique(dat.scaled$scientific_name)[i] } else { lc_clim_avg[[i]] &lt;- as.data.frame(unmarked::coef(top_lc_clim[[i]][[1]])) names(lc_clim_avg)[i] &lt;- unique(dat.scaled$scientific_name)[i] lowSt &lt;- data.frame(lowerCI=confint(top_lc_clim[[i]][[1]], type=&quot;state&quot;)[,1]) lowDet &lt;- data.frame(lowerCI=confint(top_lc_clim[[i]][[1]], type=&quot;det&quot;)[,1]) upSt &lt;- data.frame(upperCI=confint(top_lc_clim[[i]][[1]], type=&quot;state&quot;)[,2]) upDet &lt;- data.frame(upperCI=confint(top_lc_clim[[i]][[1]], type=&quot;det&quot;)[,2]) zSt &lt;- data.frame(z_value= summary(top_lc_clim[[i]][[1]])$state[,3]) zDet &lt;- data.frame(z_value =summary(top_lc_clim[[i]][[1]])$det[,3]) Pr_zSt &lt;- data.frame(Pr_z= summary(top_lc_clim[[i]][[1]])$state[,4]) Pr_zDet &lt;- data.frame(Pr_z =summary(top_lc_clim[[i]][[1]])$det[,4]) lc_clim_modelEst[[i]] &lt;- data.frame(Coefficient = coefTable(top_lc_clim[[i]][[1]])[,1], SE = coefTable(top_lc_clim[[i]][[1]])[,2], lowerCI = rbind(lowSt,lowDet), upperCI = rbind(upSt,upDet), z_value = rbind(zSt,zDet), Pr_z = rbind(Pr_zSt,Pr_zDet)) names(lc_clim_modelEst)[i] &lt;- unique(dat.scaled$scientific_name)[i] } setTxtProgressBar(pb, i) stopCluster(clust) } close(pb) # 1. Store all the dredged model outputs for each species (for both landcover and climate) write.xlsx(lc_clim ,file=&quot;C:\\\\Users\\\\vr235\\\\Downloads\\\\lc_clim.xlsx&quot;) # 2. Store all the model averaged outputs for each species and relative importance scores write.xlsx(lc_clim_avg, file = &quot;C:\\\\Users\\\\vr235\\\\Downloads\\\\lc_avg.xlsx&quot;, rowNames=T, colNames=T) write.xlsx(lc_clim_imp, file = &quot;C:\\\\Users\\\\vr235\\\\Downloads\\\\lc_imp.xlsx&quot;, rowNames=T, colNames=T) # 3. Store all model estimates write.xlsx(lc_clim_modelEst, file = &quot;C:\\\\Users\\\\vr235\\\\Downloads\\\\lc_modelEst.xlsx&quot;, rowNames=T, colNames=T) "],
["results-occupancy-predictors.html", "Section 7 Results: Occupancy predictors 7.1 Prepare libraries 7.2 Read species trait data and the final list of species 7.3 Read model estimates 7.4 Occupancy predictorsâ aggregated effect 7.5 Land cover or climate? 7.6 Elevation summary THIS NEEDS TO BE REMOVED", " Section 7 Results: Occupancy predictors 7.1 Prepare libraries # to load data library(readxl) # to handle data library(dplyr) library(readr) library(forcats) library(tidyr) library(purrr) library(stringr) # to wrangle models source(&quot;code/fun_model_estimate_collection.r&quot;) source(&quot;code/fun_make_resp_data.r&quot;) # nice tables library(knitr) library(kableExtra) # plotting library(ggplot2) library(patchwork) source(&quot;code/fun_plot_interaction.r&quot;) 7.2 Read species trait data and the final list of species # list of species species &lt;- read_csv(&quot;data/species_list.csv&quot;) list_of_species &lt;- as.character(species$scientific_name) # trait data species_trait &lt;- read_csv(&quot;data/data_species_traits.csv&quot;) 7.3 Read model estimates file_read &lt;- c(&quot;data/results/occu-2.5km/occuCovs/modelEst/lc-clim-modelEst.xlsx&quot;, &quot;data/results/occu-10km/occuCovs/modelEst/lc-clim-modelEst.xlsx&quot;) # read data as list column model_est &lt;- map(file_read, function(fr) { md_list &lt;- map(list_of_species, function(sn) { readxl::read_excel(fr, sheet = sn) }) names(md_list) &lt;- list_of_species return(md_list) }) # prepare model data scales = c(&quot;2.5km&quot;, &quot;10km&quot;) model_data &lt;- tibble(crossing(scale = scales, scientific_name = list_of_species)) %&gt;% arrange(desc(scale)) # rename model data components and separate predictors names &lt;- c(&quot;predictor&quot;, &quot;coefficient&quot;, &quot;se&quot;, &quot;ci_lower&quot;, &quot;ci_higher&quot;, &quot;z_value&quot;, &quot;p_value&quot;) # get data for plotting: model_est &lt;- map(model_est, function(l) { map(l, function(df) { colnames(df) &lt;- names df &lt;- separate_interaction_terms(df) df &lt;- make_response_data(df) return(df) }) }) # add names and scales model_est &lt;- map(model_est, function(l) { imap(l, function(.x, .y) { mutate(.x, scientific_name = .y) }) }) # add names to model estimates names(model_est) &lt;- scales model_est &lt;- imap(model_est, function(.x, .y) { bind_rows(.x) %&gt;% mutate(scale = .y) }) # remove modulators model_est &lt;- bind_rows(model_est) %&gt;% select(-matches(&quot;modulator&quot;)) # join data to species name model_data &lt;- model_data %&gt;% left_join(model_est) 7.3.1 Export data to file Export predictor effects. # get predictor effect data data_predictor_effect &lt;- distinct(model_data, scientific_name, scale, predictor, coefficient) # write to file write_csv(data_predictor_effect, path = &quot;data/results/data_predictor_effect.csv&quot;) Export model data. model_data_to_file &lt;- model_data %&gt;% select(predictor, data, scientific_name, scale) %&gt;% unnest(cols = &quot;data&quot;) # remove .y model_data_to_file &lt;- model_data_to_file %&gt;% mutate(predictor = str_remove(predictor, &quot;\\\\.y&quot;)) write_csv(model_data_to_file, &quot;data/results/data_occupancy_predictors.csv&quot;) 7.4 Occupancy predictorsâ aggregated effect # read from file model_data &lt;- read_csv(&quot;data/results/data_predictor_effect.csv&quot;) Plot the number of species affected and the direction of the effect, for each predictor. Split the data along the axes of range size, migratory status, and habitat. 7.4.1 Add trait data and clean # add trait by joining data_predictor_effect &lt;- data_predictor_effect %&gt;% left_join(species_trait, by = &quot;scientific_name&quot;) # remove .y from predictors data_predictor_effect &lt;- data_predictor_effect %&gt;% mutate_at(.vars = c(&quot;predictor&quot;), .funs = function(x){ stringr::str_remove(x, &quot;.y&quot;) }) What is the direction of the predictor effect for each subset of the data by the distribution, habitat, and migratory status? 7.4.2 Get predictor effects # first pivot the data longer data_predictor_long &lt;- data_predictor_effect %&gt;% pivot_longer(cols = c(&quot;range_size&quot;, &quot;migratory_status&quot;, &quot;habitat&quot;, &quot;elev&quot;), names_to = &quot;trait&quot;) # reorder scale data_predictor_long &lt;- data_predictor_long %&gt;% mutate(scale = fct_relevel(scale, &quot;2.5km&quot;, &quot;10km&quot;)) # is the coeff positive? how many positive per scale per predictor per axis of split? data_predictor_long &lt;- mutate(data_predictor_long, direction = coefficient &gt; 0) %&gt;% count(scale, predictor, trait, value, direction) %&gt;% mutate(mag = n * (if_else(direction, 1, -1))) # wrangle data to get nice bars data_predictor_long &lt;- data_predictor_long %&gt;% select(-n) %&gt;% drop_na(direction, value) %&gt;% mutate(direction = ifelse(direction, &quot;positive&quot;, &quot;negative&quot;)) %&gt;% pivot_wider(values_from = &quot;mag&quot;, names_from = &quot;direction&quot;) %&gt;% mutate_at(vars(positive, negative), ~if_else(is.na(.), 0, .)) data_predictor_long &lt;- data_predictor_long %&gt;% pivot_longer(cols = c(&quot;negative&quot;, &quot;positive&quot;), names_to = &quot;effect&quot;, values_to = &quot;magnitude&quot;) # write write_csv(data_predictor_long, path = &quot;data/results/data_predictor_long.csv&quot;) # nest the data by trait data_predictor_long &lt;- data_predictor_long %&gt;% nest(data = -trait) 7.4.3 Manual edits to levels # arrange data predictor long elevation in order of values data_predictor_long$data[[1]]$value &lt;- as.factor(data_predictor_long$data[[1]]$value) levels(data_predictor_long$data[[1]]$value) &lt;- c(&quot;Low&quot;, &quot;Mid&quot;, &quot;High&quot;) 7.4.4 Make figures for predictor effects # visualise the data by mapping over the nested list data_predictor_long &lt;- mutate(data_predictor_long, figs = map2(data, trait, function(df, tr){ ggplot(df)+ geom_hline(yintercept = 0, lty = 2, lwd = 0.2, col = &quot;grey&quot;)+ geom_col(aes(x = factor(predictor), y = magnitude, fill = effect))+ scale_x_discrete(guide = guide_axis(n.dodge = 2)#, # label = 1:length(unique(df$predictor)) )+ scico::scale_fill_scico_d(palette = &quot;berlin&quot;, direction = -1, begin = 0.1, end = 0.9)+ theme_grey(base_family = &quot;TT Arial&quot;)+ theme(legend.position = &quot;none&quot;)+ facet_grid(value ~ scale, labeller = label_both)+ labs(x = &quot;predictor&quot;, y = &quot;species&quot;, title = glue::glue(&#39;trait: {tr}&#39;)) })) fig_predictor_effect &lt;- patchwork::wrap_plots(data_predictor_long$figs, nrow = 2) # save plot ggsave(fig_predictor_effect, filename = &quot;figs/fig_predictor_effect.png&quot;, dpi = 300, width = 12) knitr::include_graphics(&quot;figs/fig_predictor_effect.png&quot;) 7.4.5 Tabulate predictor effect # read again and nest data_predictor_long &lt;- readr::read_csv(&quot;data/results/data_predictor_long.csv&quot;) # nest the data by trait data_predictor_long &lt;- tidyr::nest(data_predictor_long, data = -trait) # first pivot the data columns purrr::pwalk(data_predictor_long, function(trait, data) { predictors &lt;- unique(data$predictor) headers &lt;- c(1, 1, rep(2, length(predictors))) names(headers) &lt;- c(rep(&quot; &quot;, 2), predictors) data %&gt;% tidyr::drop_na() %&gt;% tidyr::pivot_wider(names_from = c(&quot;predictor&quot;, &quot;effect&quot;), values_from = &quot;magnitude&quot;) %&gt;% `colnames&lt;-`(c(&quot;spatial scale&quot;, glue::glue(&quot;{trait} class&quot;), rep(c(&quot;+ve&quot;, &quot;-ve&quot;), length(predictors)))) %&gt;% knitr::kable(caption = glue::glue(&quot;Predictor effects for {trait}&quot;)) %&gt;% kableExtra::kable_styling(&quot;striped&quot;, full_width = F, font_size = 8) %&gt;% kableExtra::add_header_above(headers) %&gt;% print() cat(&quot;\\n&quot;) }) Table 7.1: Predictor effects for elev bio_1 bio_12 lc_01 lc_02 lc_04 lc_05 lc_06 lc_07 spatial scale elev class +ve -ve +ve -ve +ve -ve +ve -ve +ve -ve +ve -ve +ve -ve +ve -ve 2.5km HIGH -9 0 -1 4 -5 1 -2 0 -1 1 -2 1 -1 0 2.5km LOW 0 29 -14 11 0 3 -7 10 -3 0 -11 1 -13 3 -3 1 2.5km MID -20 4 -6 18 -8 5 -6 10 -10 2 -8 1 -10 5 -2 4 10km HIGH -9 0 -3 1 -5 1 -1 1 -1 1 -3 1 0 2 10km LOW 0 23 -15 5 -4 5 -8 7 -8 0 -16 0 -12 3 -7 3 10km MID -21 4 -6 12 -6 8 -5 13 -15 2 -8 3 -13 6 -4 6 Table 7.1: Predictor effects for habitat bio_1 bio_12 lc_01 lc_02 lc_04 lc_05 lc_06 lc_07 spatial scale habitat class +ve -ve +ve -ve +ve -ve +ve -ve +ve -ve +ve -ve +ve -ve +ve -ve 2.5km FOR -16 13 -5 26 -9 2 -3 15 -4 2 -10 0 -13 3 -3 2 2.5km OC -13 20 -16 7 -4 7 -12 5 -10 1 -9 2 -12 6 -3 3 10km FOR -16 9 -6 12 -11 2 -4 13 -8 1 -12 0 -15 5 -1 11 10km OC -14 18 -18 6 -4 12 -9 7 -16 2 -13 4 -13 5 -10 0 Table 7.1: Predictor effects for migratory_status bio_1 bio_12 lc_01 lc_02 lc_04 lc_05 lc_06 lc_07 spatial scale migratory_status class +ve -ve +ve -ve +ve -ve +ve -ve +ve -ve +ve -ve +ve -ve +ve -ve 2.5km MIG -5 4 0 5 -4 0 -1 2 -2 0 -1 0 -2 0 0 1 2.5km RES -24 29 -21 28 -9 9 -14 18 -12 3 -18 2 -23 9 -6 4 10km MIG -6 4 -1 4 -3 0 -1 0 -3 0 -1 1 -3 0 0 1 10km RES -24 23 -23 14 -12 14 -12 20 -21 3 -24 3 -25 10 -11 10 Table 7.1: Predictor effects for range_size bio_1 bio_12 lc_01 lc_02 lc_04 lc_05 lc_06 lc_07 spatial scale range_size class +ve -ve +ve -ve +ve -ve +ve -ve +ve -ve +ve -ve +ve -ve +ve -ve 2.5km LAR -13 18 -8 14 -4 4 -6 6 -4 2 -8 1 -7 4 -2 1 2.5km MOD -9 2 0 15 -5 1 0 10 -3 1 -2 0 -9 1 -1 1 2.5km VLAR -7 13 -13 4 -4 4 -9 4 -7 0 -9 1 -9 4 -3 3 10km LAR -12 14 -10 6 -4 5 -4 8 -7 1 -8 1 -5 5 -2 5 10km MOD -10 1 -1 7 -7 1 -2 7 -5 1 -5 0 -12 2 0 6 10km VLAR -8 12 -13 5 -4 8 -7 5 -12 1 -12 3 -11 3 -9 0 7.5 Land cover or climate? Group the predictor data by two broad classes, landcover or climate. 7.5.1 Get the effect of landcover or climate # remove figs and unnest data_predictor_long &lt;- data_predictor_long %&gt;% select(!matches(&quot;figs&quot;)) %&gt;% unnest(data) # group by predictor data_lc_v_clim &lt;- data_predictor_long %&gt;% mutate(predictor = if_else(str_detect(predictor, &quot;bio&quot;), &quot;climate&quot;, &quot;landcover&quot;)) %&gt;% group_by(trait, scale, predictor, value, effect) %&gt;% summarise_at(.vars = c(&quot;magnitude&quot;), .funs = list(sum)) # nest on trait data_lc_v_clim &lt;- nest(data_lc_v_clim, cols = -trait) 7.5.2 Plot a figure # make list of figures data_lc_v_clim$figs &lt;- pmap(data_lc_v_clim[,c(&quot;trait&quot;, &quot;cols&quot;)], function(trait, cols) { ggplot(cols) + geom_col(aes(predictor, magnitude, fill = effect), width = 0.4) + scico::scale_fill_scico_d(palette = &quot;berlin&quot;, direction = -1, begin = 0.1, end = 0.9)+ theme_grey(base_family = &quot;TT Arial&quot;) + theme(legend.position = &quot;none&quot;) + facet_grid(value ~ scale, labeller = label_both) + labs(title = glue::glue(&#39;trait: {trait}&#39;)) }) fig_lc_v_clim &lt;- patchwork::wrap_plots(data_lc_v_clim$figs, nrow = 2) # save plot ggsave(fig_lc_v_clim, filename = &quot;figs/fig_lc_v_clim.png&quot;, dpi = 300) knitr::include_graphics(&quot;figs/fig_lc_v_clim.png&quot;) 7.6 Elevation summary THIS NEEDS TO BE REMOVED Plot elevation summaries faceted by scale and predictors. Read species elevation summaries. # add elevation data elev_summary &lt;- read_excel(&quot;data/data_species_elevation.xlsx&quot;) # link to species elev_summary &lt;- elev_summary %&gt;% left_join(data_predictor_effect) # arrange in plot order elev_summary &lt;- elev_summary %&gt;% group_by(scale, scientific_name) %&gt;% arrange(median) %&gt;% mutate(plot_order = seq_len(n())) # remove NAs and order scale elev_summary &lt;- select(elev_summary, -`Body mass`) %&gt;% drop_na() %&gt;% mutate(scale = fct_relevel(scale, &quot;2.5km&quot;, &quot;10km&quot;)) # rename median elev_summary &lt;- rename(elev_summary, median_elev = median) # order elev_summary &lt;- elev_summary %&gt;% ungroup() %&gt;% arrange(-median_elev) %&gt;% group_by(scientific_name) # plot elevation pointranges ggplot(elev_summary)+ geom_errorbar(aes(plot_order, ymin = q1, ymax = q3), size = 0.3, width = 0.4)+ geom_point(aes(plot_order, median_elev), shape = 21, col = &quot;grey20&quot;)+ facet_grid(scale ~ predictor, scales = &quot;fixed&quot;, labeller = label_both, as.table = FALSE)+ theme_test()+ theme(legend.position = &quot;none&quot;, legend.key = element_rect(colour = &quot;white&quot;, size = 0.3), # axis.text.y = element_blank(), axis.ticks.y = element_blank())#+ # coord_flip()+ # labs(x = &quot;Species&quot;, # colour = &quot;synthesis_var&quot;) "],
["results-occupancy-predictors-1.html", "Section 8 Results: Occupancy predictors 8.1 Prepare libraries 8.2 Read data", " Section 8 Results: Occupancy predictors 8.1 Prepare libraries # to load data library(readxl) # to handle data library(dplyr) library(readr) library(forcats) library(tidyr) library(purrr) library(stringr) # plotting library(ggplot2) library(patchwork) 8.2 Read data # read data data &lt;- read_csv(&quot;data/results/data_occupancy_predictors.csv&quot;) # drop na data &lt;- select(data, -ci) %&gt;% drop_na() %&gt;% nest(-scientific_name) fig_occupancy &lt;- map2(data$data, data$scientific_name, function(df, name) { ggplot(df)+ geom_line(aes(seq_x, mean, col = predictor))+ facet_grid(~ scale, labeller = label_both) + coord_cartesian(xlim = c(0, 1), ylim = c(0, 1))+ theme_grey(base_size = 6)+ theme(legend.position = &quot;top&quot;)+ scale_colour_manual(values = pals::kovesi.rainbow(8))+ labs(x = &quot;scaled predictor value&quot;, y = &quot;p(occupancy)&quot;, title = name) }) cairo_pdf(filename = &quot;figs/fig_occupancy_predictors.pdf&quot;, onefile = TRUE, width = 6, height = 4) fig_occupancy dev.off() "]
]
