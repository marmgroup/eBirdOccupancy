[["index.html", "Using citizen science to parse climatic and landcover influences on bird occupancy within a tropical biodiversity hotspot Section 1 Introduction 1.1 Attribution 1.2 Data access 1.3 Data processing 1.4 Main Text Figure 1", " Using citizen science to parse climatic and landcover influences on bird occupancy within a tropical biodiversity hotspot Vijay Ramesh Pratik R. Gupte Morgan W. Tingley VV Robin Ruth DeFries 2020-12-18 Section 1 Introduction This is the readable version containing analysis that models associations between environmental predictors (climate and landcover) and citizen science observations of birds across the Nilgiri and Anamalai Hills of the Western Ghats Biodiversity Hotspot. Methods and format are derived from Strimas-Mackey et al.. 1.1 Attribution Please contact the following in case of interest in the project. Vijay Ramesh (lead author) PhD student, Columbia University Pratik Gupte (repo maintainer) PhD student, University of Groningen 1.2 Data access The data used in this work are available from eBird. 1.3 Data processing The data processing for this project is described in the following sections. Navigate through them using the links in the sidebar. 1.4 Main Text Figure 1 Figure prepared in QGIS 3.10. A shaded relief of the study area - the Nilgiri and the Anamalai hills are shown in this figure. This map was made using the SRTM digital elevation model at a spatial resolution of 1km and data from Natural Earth were used to outline boundaries of water bodies. "],["preparing-ebird-data.html", "Section 2 Preparing eBird Data 2.1 Prepare libraries and data sources 2.2 Filter data 2.3 Process filtered data 2.4 Spatial filter 2.5 Handle presence data 2.6 Add decimal time", " Section 2 Preparing eBird Data 2.1 Prepare libraries and data sources # load libs library(tidyverse) library(readr) library(sf) library(auk) library(readxl) # custom sum function sum.no.na &lt;- function(x) { sum(x, na.rm = T) } # set file paths for auk functions f_in_ebd &lt;- file.path(&quot;data/ebd_IN_relApr-2020.txt&quot;) f_in_sampling &lt;- file.path(&quot;data/ebd_sampling_relApr-2020.txt&quot;) 2.2 Filter data # add species of interest specieslist &lt;- read.csv(&quot;data/species_list.csv&quot;) speciesOfInterest &lt;- as.character(specieslist$scientific_name) # run filters using auk packages ebd_filters &lt;- auk_ebd(f_in_ebd, f_in_sampling) %&gt;% auk_species(speciesOfInterest) %&gt;% auk_country(country = &quot;IN&quot;) %&gt;% auk_state(c(&quot;IN-KL&quot;, &quot;IN-TN&quot;, &quot;IN-KA&quot;)) %&gt;% # Restricting geography to TamilNadu, Kerala &amp; Karnataka auk_date(c(&quot;2013-01-01&quot;, &quot;2019-12-31&quot;)) %&gt;% auk_complete() # check filters ebd_filters Below code need not be run if it has been filtered once already and the above path leads to the right dataset. NB: This is a computation heavy process, run with caution. # specify output location and perform filter f_out_ebd &lt;- &quot;data/eBirdDataWG_filtered.txt&quot; f_out_sampling &lt;- &quot;data/eBirdSamplingDataWG_filtered.txt&quot; ebd_filtered &lt;- auk_filter(ebd_filters, file = f_out_ebd, file_sampling = f_out_sampling, overwrite = TRUE ) 2.3 Process filtered data # read in the data ebd &lt;- read_ebd(f_out_ebd) # fill zeroes zf &lt;- auk_zerofill(f_out_ebd, f_out_sampling) new_zf &lt;- collapse_zerofill(zf) # Creates a new zero-filled dataframe with a 0 marked for each checklist when the bird was not observed # choose columns of interest columnsOfInterest &lt;- c( &quot;checklist_id&quot;, &quot;scientific_name&quot;, &quot;common_name&quot;, &quot;observation_count&quot;, &quot;locality&quot;, &quot;locality_id&quot;, &quot;locality_type&quot;, &quot;latitude&quot;, &quot;longitude&quot;, &quot;observation_date&quot;, &quot;time_observations_started&quot;, &quot;observer_id&quot;, &quot;sampling_event_identifier&quot;, &quot;protocol_type&quot;, &quot;duration_minutes&quot;, &quot;effort_distance_km&quot;, &quot;effort_area_ha&quot;, &quot;number_observers&quot;, &quot;species_observed&quot;, &quot;reviewed&quot; ) # make list of presence and absence data and choose cols of interest data &lt;- list(ebd, new_zf) %&gt;% map(function(x) { x %&gt;% select(one_of(columnsOfInterest)) }) # remove zerofills to save working memory rm(zf, new_zf) gc() # check presence and absence in absences df, remove essentially the presences df data[[2]] &lt;- data[[2]] %&gt;% filter(species_observed == F) 2.4 Spatial filter # load shapefiles of hill ranges library(sf) hills &lt;- st_read(&quot;data/spatial/hillsShapefile/Nil_Ana_Pal.shp&quot;) # write a prelim filter by bounding box box &lt;- st_bbox(hills) # get data spatial coordinates dataLocs &lt;- data %&gt;% map(function(x) { select(x, longitude, latitude) %&gt;% filter(between(longitude, box[&quot;xmin&quot;], box[&quot;xmax&quot;]) &amp; between(latitude, box[&quot;ymin&quot;], box[&quot;ymax&quot;])) }) %&gt;% bind_rows() %&gt;% distinct() %&gt;% st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;)) %&gt;% st_set_crs(4326) %&gt;% st_intersection(hills) # get simplified data and drop geometry dataLocs &lt;- mutate(dataLocs, spatialKeep = T) %&gt;% bind_cols(., as_tibble(st_coordinates(dataLocs))) %&gt;% st_drop_geometry() # bind to data and then filter data &lt;- data %&gt;% map(function(x) { left_join(x, dataLocs, by = c(&quot;longitude&quot; = &quot;X&quot;, &quot;latitude&quot; = &quot;Y&quot;)) %&gt;% filter(spatialKeep == T) %&gt;% select(-Id, -spatialKeep) }) # save a temp data file save(data, file = &quot;data/data_temp.rdata&quot;) 2.5 Handle presence data # in the first set, replace X, for presences, with 1 data[[1]] &lt;- data[[1]] %&gt;% mutate(observation_count = ifelse(observation_count == &quot;X&quot;, &quot;1&quot;, observation_count )) # remove records where duration is 0 data &lt;- map(data, function(x) filter(x, duration_minutes &gt; 0)) # group data by site and sampling event identifier # then, summarise relevant variables as the sum dataGrouped &lt;- map(data, function(x) { x %&gt;% group_by(sampling_event_identifier) %&gt;% summarise_at( vars( duration_minutes, effort_distance_km, effort_area_ha ), list(sum.no.na) ) }) # bind rows combining data frames, and filter dataGrouped &lt;- bind_rows(dataGrouped) %&gt;% filter( duration_minutes &lt;= 300, effort_distance_km &lt;= 5, effort_area_ha &lt;= 500 ) # get data identifiers, such as sampling identifier etc dataConstants &lt;- data %&gt;% bind_rows() %&gt;% select( sampling_event_identifier, time_observations_started, locality, locality_type, locality_id, observer_id, observation_date, scientific_name, observation_count, protocol_type, number_observers, longitude, latitude ) # join the summarised data with the identifiers, # using sampling_event_identifier as the key dataGrouped &lt;- left_join(dataGrouped, dataConstants, by = &quot;sampling_event_identifier&quot; ) # remove checklists or seis with more than 10 obervers count(dataGrouped, number_observers &gt; 10) # count how many have 10+ obs dataGrouped &lt;- filter(dataGrouped, number_observers &lt;= 10) 2.6 Add decimal time # assign present or not, and get time in decimal hours since midnight library(lubridate) time_to_decimal &lt;- function(x) { x &lt;- hms(x, quiet = TRUE) hour(x) + minute(x) / 60 + second(x) / 3600 } # will cause issues if using time obs started as a linear effect and not quadratic dataGrouped &lt;- mutate(dataGrouped, pres_abs = observation_count &gt;= 1, decimalTime = time_to_decimal(time_observations_started) ) # check class of dataGrouped, make sure not sf assertthat::assert_that(!&quot;sf&quot; %in% class(dataGrouped)) This data is saved to file. # save a temp data file save(dataGrouped, file = &quot;data/data_prelim_processing.rdata&quot;) "],["preparing-landscape-data.html", "Section 3 Preparing Landscape Data 3.1 Prepare libraries 3.2 Prepare spatial extent 3.3 Prepare terrain rasters 3.4 Prepare CHELSA rasters 3.5 Resample landcover from 10m to 1km 3.6 Resample other rasters to 1km 3.7 Temperature and rainfall in relation to elevation 3.8 Land cover type in relation to elevation 3.9 Main Text Figure 2", " Section 3 Preparing Landscape Data Here, we prepare environmental layers which we have accessed as raster images from remote sensing platforms using Google Earth Engine. The GEE code can be found in a separate file. The goal here is to resample all rasters so that they have the same resolution of 1km cells. 3.1 Prepare libraries We load some common libraries for raster processing and define a custom mode function. # load libs library(raster) library(stringi) library(glue) library(gdalUtils) library(purrr) # prep mode function to aggregate funcMode &lt;- function(x, na.rm = T) { ux &lt;- unique(x) ux[which.max(tabulate(match(x, ux)))] } # a basic test assertthat::assert_that(funcMode(c(2, 2, 2, 2, 3, 3, 3, 4)) == as.character(2), msg = &quot;problem in the mode function&quot; ) # works 3.2 Prepare spatial extent We prepare a 30km buffer around the boundary of the study area. This buffer will be used to mask the landscape rasters. The buffer procedure is done on data transformed to the UTM 43N CRS to avoid distortions. # load hills library(sf) hills &lt;- st_read(&quot;data/spatial/hillsShapefile/Nil_Ana_Pal.shp&quot;) hills &lt;- st_transform(hills, 32643) buffer &lt;- st_buffer(hills, 3e4) %&gt;% st_transform(4326) 3.3 Prepare terrain rasters We prepare the elevation data which is an SRTM raster layer, and derive the slope and aspect from it after cropping it to the extent of the study site buffer. # load elevation and crop to hills size, then mask by hills alt &lt;- raster(&quot;data/spatial/Elevation/alt&quot;) alt.hills &lt;- crop(alt, as(buffer, &quot;Spatial&quot;)) rm(alt) gc() # get slope and aspect slopeData &lt;- terrain(x = alt.hills, opt = c(&quot;slope&quot;, &quot;aspect&quot;)) elevData &lt;- raster::stack(alt.hills, slopeData) rm(alt.hills) gc() 3.4 Prepare CHELSA rasters We prepare the CHELSA rasters for annual temperature and annual precipitation in the same way, reading them in, cropping them to the study site buffer extent, and handling the temperature layer values which we divide by 10. # list chelsa files chelsaFiles &lt;- list.files(&quot;data/chelsa/&quot;, full.names = TRUE, pattern = &quot;*.tif&quot; ) # gather chelsa rasters chelsaData &lt;- purrr::map(chelsaFiles, function(chr) { a &lt;- raster(chr) crs(a) &lt;- crs(elevData) a &lt;- crop(a, as(buffer, &quot;Spatial&quot;)) return(a) }) # divide temperature by 10 chelsaData[[1]] &lt;- chelsaData[[1]] / 10 # stack chelsa data chelsaData &lt;- raster::stack(chelsaData) We stack the terrain and climatic rasters. # stack rasters for efficient reprojection later env_data &lt;- stack(elevData, chelsaData) 3.5 Resample landcover from 10m to 1km We read in a land cover classified image and resample that using the mode function to a 1km resolution. # read in landcover raster location landcover &lt;- &quot;data/landUseClassification/Reprojected Image_26thJan2020_UTM_Ghats.tif&quot; # get extent e &lt;- bbox(raster(landcover)) # init resolution res_init &lt;- res(raster(landcover)) # res to transform to 1000m res_final &lt;- res_init * 100 # use gdalutils gdalwarp for resampling transform # to 1km from 10m gdalUtils::gdalwarp( srcfile = landcover, dstfile = &quot;data/landUseClassification/lc_01000m.tif&quot;, tr = c(res_final), r = &quot;mode&quot;, te = c(e) ) We compare the frequency of landcover classes between the original 10m and the resampled 1km raster to be certain that the resampling has not resulted in drastic misrepresentation of the frequency of any landcover type. This comparison is made using the figure below. 3.6 Resample other rasters to 1km We now resample all other rasters to a resolution of 1km. 3.6.1 Read in resampled landcover Here, we read in the 1km landcover raster and set 0 to NA. lc_data &lt;- raster(&quot;data/landUseClassification/lc_01000m.tif&quot;) lc_data[lc_data == 0] &lt;- NA 3.6.2 Reproject environmental data using landcover as a template # resample to the corresponding landcover data env_data_resamp &lt;- projectRaster( from = env_data, to = lc_data, crs = crs(lc_data), res = res(lc_data) ) # export as raster stack land_stack &lt;- stack(env_data_resamp, lc_data) # get names land_names &lt;- glue(&#39;data/spatial/landscape_resamp{c(&quot;01&quot;)}_km.tif&#39;) # write to file writeRaster(land_stack, filename = as.character(land_names), overwrite = TRUE) 3.7 Temperature and rainfall in relation to elevation 3.7.1 Prepare libraries and CI function # load libs library(dplyr) library(tidyr) library(scales) library(ggplot2) # get ci func ci &lt;- function(x) { qnorm(0.975) * sd(x, na.rm = T) / sqrt(length(x)) } 3.7.2 Load resampled environmental rasters # read landscape prepare for plotting landscape &lt;- stack(&quot;data/spatial/landscape_resamp01_km.tif&quot;) # get proper names elev_names &lt;- c(&quot;elev&quot;, &quot;slope&quot;, &quot;aspect&quot;) chelsa_names &lt;- c(&quot;bio_01&quot;, &quot;bio_12&quot;) names(landscape) &lt;- as.character(glue(&#39;{c(elev_names, chelsa_names, &quot;landcover&quot;)}&#39;)) # make duplicate stack land_data &lt;- landscape[[c(&quot;elev&quot;, chelsa_names)]] # convert to list land_data &lt;- as.list(land_data) # map get values over the stack land_data &lt;- purrr::map(land_data, getValues) names(land_data) &lt;- c(&quot;elev&quot;, chelsa_names) # conver to dataframe and round to 100m land_data &lt;- bind_cols(land_data) land_data &lt;- drop_na(land_data) %&gt;% mutate(elev_round = plyr::round_any(elev, 200)) %&gt;% dplyr::select(-elev) %&gt;% pivot_longer( cols = contains(&quot;bio&quot;), names_to = &quot;clim_var&quot; ) %&gt;% group_by(elev_round, clim_var) %&gt;% summarise_all(.funs = list(~ mean(.), ~ ci(.))) Figure code is hidden in versions rendered as HTML or PDF. 3.8 Land cover type in relation to elevation # get data from landscape rasters lc_elev &lt;- tibble( elev = getValues(landscape[[&quot;elev&quot;]]), landcover = getValues(landscape[[&quot;landcover&quot;]]) ) # process data for proportions lc_elev &lt;- lc_elev %&gt;% filter(!is.na(landcover), !is.na(elev)) %&gt;% mutate(elev = plyr::round_any(elev, 100)) %&gt;% count(elev, landcover) %&gt;% group_by(elev) %&gt;% mutate(prop = n / sum(n)) # fill out lc elev lc_elev_canon &lt;- crossing( elev = unique(lc_elev$elev), landcover = unique(lc_elev$landcover) ) # bind with lcelev lc_elev &lt;- full_join(lc_elev, lc_elev_canon) # convert NA to zero lc_elev &lt;- replace_na(lc_elev, replace = list(n = 0, prop = 0)) Figure code is hidden in versions rendered as HTML and PDF. 3.9 Main Text Figure 2 Annual Mean Temperature varied from ~28C in the plains to &lt;14C at higher elevations. Annual precipitation increased at lower elevations (in the plains) to ~3000mm and ranged between 1500mm and 2200mm at mid- and high elevations across the Nilgiri and the Anamalai hills. (b) The proportion of land cover types varied across the study area as shown in this panel (1 = agriculture; 2 = forest; 3 = grassland; 4 = plantations; 5 = settlements; 6 = tea; 7 = water bodies). "],["examining-spatial-sampling-bias.html", "Section 4 Examining Spatial Sampling Bias 4.1 Prepare libraries 4.2 Prepare data for processing 4.3 Main Text Figure 3 4.4 Distance to nearest neighbouring site 4.5 Main Text Figure 3", " Section 4 Examining Spatial Sampling Bias The goal of this section is to determine how far each checklist location is from the nearest road, and how far each site is from its nearest neighbour. This involves finding the pairwise distance between a large number of unique checklist locations to a vast number of roads, as well as to each other. Parts of this section are thus implemented in Python, using the R-Python interface package, reticulate. 4.1 Prepare libraries # load libraries library(reticulate) library(sf) library(dplyr) library(scales) library(readr) library(purrr) library(ggplot2) library(ggthemes) library(ggspatial) library(scico) # round any function round_any &lt;- function(x, accuracy = 20000) { round(x / accuracy) * accuracy } # ci function ci &lt;- function(x) { qnorm(0.975) * sd(x, na.rm = TRUE) / sqrt(length(x)) } # set python path use_python(&quot;/usr/bin/python3&quot;) Importing python libraries. These libraries need to be installed before use. # import classic python libs import itertools from operator import itemgetter import numpy as np import matplotlib.pyplot as plt import math # libs for dataframes import pandas as pd # import libs for geodata from shapely.ops import nearest_points import geopandas as gpd import rasterio # import ckdtree from scipy.spatial import cKDTree from shapely.geometry import Point, MultiPoint, LineString, MultiLineString 4.2 Prepare data for processing First we read in the roads shapefile, which is obtained from the Open Street Map database. Then we read in the checklsit covariates, and extract the unique coordinate pairs. All data are reprojected to be in the UTM 43N system. We define a custom Python function to separate multi-feature geometries (here, roads which are in parts) into single feature geometries. Then we define a function to use the K-dimensional trees method from scipy to find the distance between two geometries, here, the distance between the locations and the nearest road. We define another function to find the distance between checklist locations and all other checklist locations. We use these functions to find the distance between each checklist location and the nearest road and the next nearest site. 4.2.1 Python functions and distance calculations # read in roads shapefile roads = gpd.read_file(&quot;data/spatial/roads_studysite_2019/roads_studysite_2019.shp&quot;) roads.head() # read in checklist covariates for conversion to gpd # get unique coordinates, assign them to the df # convert df to geo-df chkCovars = pd.read_csv(&quot;data/eBirdChecklistVars.csv&quot;) unique_locs = chkCovars.drop_duplicates(subset=[&#39;longitude&#39;, &#39;latitude&#39;])[[&#39;longitude&#39;, &#39;latitude&#39;]] unique_locs[&#39;coordId&#39;] = np.arange(1, unique_locs.shape[0]+1) chkCovars = chkCovars.merge(unique_locs, on=[&#39;longitude&#39;, &#39;latitude&#39;]) unique_locs = gpd.GeoDataFrame( unique_locs, geometry = gpd.points_from_xy(unique_locs.longitude, unique_locs.latitude)) unique_locs.crs = {&#39;init&#39; :&#39;epsg:4326&#39;} # reproject spatials to 43n epsg 32643 roads = roads.to_crs({&#39;init&#39;: &#39;epsg:32643&#39;}) unique_locs = unique_locs.to_crs({&#39;init&#39;: &#39;epsg:32643&#39;}) # function to simplify multilinestrings def simplify_roads(complex_roads): simpleRoads = [] for i in range(len(complex_roads.geometry)): feature = complex_roads.geometry.iloc[i] if feature.geom_type == &quot;LineString&quot;: simpleRoads.append(feature) elif feature.geom_type == &quot;MultiLineString&quot;: for road_level2 in feature: simpleRoads.append(road_level2) return simpleRoads # function to use ckdtrees to find the nearest road def ckdnearest(gdfA, gdfB): A = np.concatenate( [np.array(geom.coords) for geom in gdfA.geometry.to_list()]) simplified_features = simplify_roads(gdfB) B = [np.array(geom.coords) for geom in simplified_features] B = np.concatenate(B) ckd_tree = cKDTree(B) dist, idx = ckd_tree.query(A, k=1) return dist # function to use ckdtrees for nearest other checklist point def ckdnearest_point(gdfA, gdfB): A = np.concatenate( [np.array(geom.coords) for geom in gdfA.geometry.to_list()]) #simplified_features = simplify_roads(gdfB) B = np.concatenate( [np.array(geom.coords) for geom in gdfB.geometry.to_list()]) #B = np.concatenate(B) ckd_tree = cKDTree(B) dist, idx = ckd_tree.query(A, k=[2]) return dist # get distance to nearest road unique_locs[&#39;dist_road&#39;] = ckdnearest(unique_locs, roads) # get distance to nearest other site unique_locs[&#39;nnb&#39;] = ckdnearest_point(unique_locs, unique_locs) # write to file unique_locs = pd.DataFrame(unique_locs.drop(columns=&#39;geometry&#39;)) unique_locs[&#39;dist_road&#39;] = unique_locs[&#39;dist_road&#39;] unique_locs[&#39;nnb&#39;] = unique_locs[&#39;nnb&#39;] unique_locs.to_csv(path_or_buf = &quot;data/locs_dist_to_road.csv&quot;, index=False) # merge unique locs with chkCovars chkCovars = chkCovars.merge(unique_locs, on=[&#39;latitude&#39;, &#39;longitude&#39;, &#39;coordId&#39;]) 4.2.2 Spatially explicit filter on checklists We filter the checklists by the boundary of the study area. This is not the extent. # extract data from python chkCovars &lt;- py$chkCovars chkCovars &lt;- st_as_sf(chkCovars, coords = c(&quot;longitude&quot;, &quot;latitude&quot;)) %&gt;% `st_crs&lt;-`(4326) %&gt;% st_transform(32643) # read wg wg &lt;- st_read(&quot;data/spatial/hillsShapefile/Nil_Ana_Pal.shp&quot;) %&gt;% st_transform(32643) # spatial subset chkCovars &lt;- chkCovars %&gt;% mutate(id = 1:nrow(.)) %&gt;% filter(id %in% unlist(st_contains(wg, chkCovars))) 4.3 Main Text Figure 3 4.3.1 Prepare histogram of distance to roads Figure code is hidden in versions rendered as HTML or PDF. 4.3.2 Table: Distance to roads # write the mean and ci95 to file chkCovars %&gt;% st_drop_geometry() %&gt;% select(dist_road, nnb) %&gt;% tidyr::pivot_longer( cols = c(&quot;dist_road&quot;, &quot;nnb&quot;), names_to = &quot;variable&quot; ) %&gt;% group_by(variable) %&gt;% summarise_at( vars(value), list(~ mean(.), ~ sd(.), ~ min(.), ~ max(.)) ) %&gt;% write_csv(&quot;data/results/distance_roads_sites.csv&quot;) (#tab:show_dist_roads_nnb)Distance to roads: Summary statistics variable mean sd min max dist_road 390 859 0.279 7637 nnb 297 553 0.137 12850 4.4 Distance to nearest neighbouring site # get unique locations locs &lt;- py$unique_locs Figure code is hidden in versions rendered as HTML and PDF. 4.5 Main Text Figure 3 roads &lt;- st_read(&quot;data/spatial/roads_studysite_2019/roads_studysite_2019.shp&quot;) %&gt;% st_transform(32643) points &lt;- chkCovars %&gt;% bind_cols(as_tibble(st_coordinates(.))) %&gt;% st_drop_geometry() %&gt;% mutate(X = round_any(X, 2500), Y = round_any(Y, 2500)) points &lt;- count(points, X, Y) # add land library(rnaturalearth) land &lt;- ne_countries( scale = 50, type = &quot;countries&quot;, continent = &quot;asia&quot;, country = &quot;india&quot;, returnclass = c(&quot;sf&quot;) ) %&gt;% st_transform(32643) bbox &lt;- st_bbox(wg) Figure code is hidden in versions rendered as HTML and PDF. Spatial sampling bias of eBird observations across the Nilgiri and the Anamalai hills. A large proportion of localities/sites were next to roads and were on average only ~300m from another locality/site. "],["preparing-observer-expertise-scores.html", "Section 5 Preparing Observer Expertise Scores 5.1 Prepare libraries 5.2 Prepare data 5.3 Spatially explicit filter on checklists 5.4 Prepare species of interest 5.5 Prepare checklists for observer score 5.6 Get landcover 5.7 Filter checklist data 5.8 Model observer expertise", " Section 5 Preparing Observer Expertise Scores 5.1 Prepare libraries # load libs library(data.table) library(readxl) library(magrittr) library(stringr) library(dplyr) library(tidyr) library(auk) # get decimal time function library(lubridate) time_to_decimal &lt;- function(x) { x &lt;- lubridate::hms(x, quiet = TRUE) lubridate::hour(x) + lubridate::minute(x) / 60 + lubridate::second(x) / 3600 } 5.2 Prepare data Here, we go through the data preparation process again because we might want to assess observer expertise over a larger area than the study site. # Read in shapefile of study area to subset by bounding box library(sf) wg &lt;- st_read(&quot;data/spatial/hillsShapefile/Nil_Ana_Pal.shp&quot;) %&gt;% st_transform(32643) # set file paths for auk functions f_in_ebd &lt;- file.path(&quot;data/eBirdDataWG_filtered.txt&quot;) f_in_sampling &lt;- file.path(&quot;data/eBirdSamplingDataWG_filtered.txt&quot;) # run filters using auk packages ebd_filters &lt;- auk_ebd(f_in_ebd, f_in_sampling) %&gt;% auk_country(country = &quot;IN&quot;) %&gt;% auk_state(c(&quot;IN-KL&quot;, &quot;IN-TN&quot;, &quot;IN-KA&quot;)) %&gt;% # Restricting geography to TamilNadu, Kerala &amp; Karnataka auk_date(c(&quot;2013-01-01&quot;, &quot;2019-12-31&quot;)) %&gt;% auk_complete() # check filters ebd_filters # specify output location and perform filter f_out_ebd &lt;- &quot;data/ebird_for_expertise.txt&quot; f_out_sampling &lt;- &quot;data/ebird_sampling_for_expertise.txt&quot; ebd_filtered &lt;- auk_filter(ebd_filters, file = f_out_ebd, file_sampling = f_out_sampling, overwrite = TRUE ) ## Process filtered data # read in the data ebd &lt;- fread(f_out_ebd) names &lt;- names(ebd) %&gt;% stringr::str_to_lower() %&gt;% stringr::str_replace_all(&quot; &quot;, &quot;_&quot;) setnames(ebd, names) # choose columns of interest columnsOfInterest &lt;- c( &quot;checklist_id&quot;, &quot;scientific_name&quot;, &quot;observation_count&quot;, &quot;locality&quot;, &quot;locality_id&quot;, &quot;locality_type&quot;, &quot;latitude&quot;, &quot;longitude&quot;, &quot;observation_date&quot;, &quot;time_observations_started&quot;, &quot;observer_id&quot;, &quot;sampling_event_identifier&quot;, &quot;protocol_type&quot;, &quot;duration_minutes&quot;, &quot;effort_distance_km&quot;, &quot;effort_area_ha&quot;, &quot;number_observers&quot;, &quot;species_observed&quot;, &quot;reviewed&quot; ) ebd &lt;- setDF(ebd) %&gt;% as_tibble() %&gt;% dplyr::select(one_of(columnsOfInterest)) setDT(ebd) 5.3 Spatially explicit filter on checklists # get checklist locations ebd_locs &lt;- ebd[, .(longitude, latitude)] ebd_locs &lt;- setDF(ebd_locs) %&gt;% distinct() ebd_locs &lt;- st_as_sf(ebd_locs, coords = c(&quot;longitude&quot;, &quot;latitude&quot;) ) %&gt;% `st_crs&lt;-`(4326) %&gt;% bind_cols(as_tibble(st_coordinates(.))) %&gt;% st_transform(32643) %&gt;% mutate(id = 1:nrow(.)) # check whether to include to_keep &lt;- unlist(st_contains(wg, ebd_locs)) # filter locs ebd_locs &lt;- filter(ebd_locs, id %in% to_keep) %&gt;% bind_cols(as_tibble(st_coordinates(st_as_sf(.)))) %&gt;% st_drop_geometry() ebd &lt;- ebd[longitude %in% ebd_locs$X &amp; latitude %in% ebd_locs$Y, ] 5.4 Prepare species of interest # read in species list specieslist &lt;- read.csv(&quot;data/species_list.csv&quot;) # set species of interest soi &lt;- specieslist$scientific_name ebdSpSum &lt;- ebd[, .( nSp = .N, totSoiSeen = length(intersect(scientific_name, soi)) ), by = list(sampling_event_identifier) ] # write to file and link with checklsit id later fwrite(ebdSpSum, file = &quot;data/dataChecklistSpecies.csv&quot;) 5.5 Prepare checklists for observer score # 1. add new columns of decimal time and julian date ebd[, `:=`( decimalTime = time_to_decimal(time_observations_started), julianDate = yday(as.POSIXct(observation_date)) )] ebdEffChk &lt;- setDF(ebd) %&gt;% mutate(year = year(observation_date)) %&gt;% distinct( sampling_event_identifier, observer_id, year, duration_minutes, effort_distance_km, effort_area_ha, longitude, latitude, locality, locality_id, decimalTime, julianDate, number_observers ) %&gt;% # drop rows with NAs in cols used in the model tidyr::drop_na( sampling_event_identifier, observer_id, duration_minutes, decimalTime, julianDate ) %&gt;% # drop years below 2013 filter(year &gt;= 2013) # 3. join to covariates and remove large groups (&gt; 10) ebdChkSummary &lt;- inner_join(ebdEffChk, ebdSpSum) # remove ebird data rm(ebd) gc() 5.6 Get landcover Read in land cover type data resampled at 1km resolution. # read in 1km landcover and set 0 to NA library(raster) landcover &lt;- raster::raster(&quot;data/landUseClassification/lc_01000m.tif&quot;) landcover[landcover == 0] &lt;- NA # get locs in utm coords locs &lt;- distinct( ebdChkSummary, sampling_event_identifier, longitude, latitude, locality, locality_id ) locs &lt;- st_as_sf(locs, coords = c(&quot;longitude&quot;, &quot;latitude&quot;)) %&gt;% `st_crs&lt;-`(4326) %&gt;% st_transform(32643) %&gt;% st_coordinates() # get for unique points landcoverVec &lt;- raster::extract( x = landcover, y = locs ) # assign to df and overwrite setDT(ebdChkSummary)[, landcover := landcoverVec] 5.7 Filter checklist data # change names for easy handling setnames(ebdChkSummary, c( &quot;sei&quot;, &quot;observer&quot;, &quot;year&quot;, &quot;duration&quot;, &quot;distance&quot;, &quot;area&quot;, &quot;longitude&quot;, &quot;latitude&quot;, &quot;locality&quot;, &quot;locality_id&quot;, &quot;decimalTime&quot;, &quot;julianDate&quot;, &quot;nObs&quot;, &quot;nSp&quot;, &quot;nSoi&quot;, &quot;landcover&quot; )) # count data points per observer obscount &lt;- count(ebdChkSummary, observer) %&gt;% filter(n &gt;= 10) # make factor variables and remove obs not in obscount # also remove 0 durations ebdChkSummary &lt;- ebdChkSummary %&gt;% mutate( distance = ifelse(is.na(distance), 0, distance), duration = if_else(is.na(duration), 0.0, as.double(duration)) ) %&gt;% filter( observer %in% obscount$observer, duration &gt; 0, duration &lt;= 300, nSoi &gt;= 0, distance &lt;= 5, !is.na(nSoi) ) %&gt;% mutate( landcover = as.factor(landcover), observer = as.factor(observer) ) %&gt;% drop_na(landcover) # save to file for later reuse fwrite(ebdChkSummary, file = &quot;data/eBirdChecklistVars.csv&quot;) 5.8 Model observer expertise Our observer expertise model aims to include the random intercept effect of observer identity, with a random slope effect of duration. This models the different rate of species accumulation by different observers, as well as their different starting points. # uses either a subset or all data library(lmerTest) # here we specify a glmm with random effects for observer # time is considered a fixed log predictor and a random slope modObsExp &lt;- glmer(nSoi ~ sqrt(duration) + landcover + sqrt(decimalTime) + I((sqrt(decimalTime))^2) + log(julianDate) + I((log(julianDate)^2)) + (1 | observer) + (0 + duration | observer), data = ebdChkSummary, family = &quot;poisson&quot; ) # make dir if absent if (!dir.exists(&quot;data/modOutput&quot;)) { dir.create(&quot;data/modOutput&quot;) } # write model output to text file { writeLines(R.utils::captureOutput(list(Sys.time(), summary(modObsExp))), con = &quot;data/modOutput/modOutExpertise.txt&quot; ) } # make df with means observer &lt;- unique(ebdChkSummary$observer) # predict at 60 mins on the most common landcover dfPredict &lt;- ebdChkSummary %&gt;% summarise_at(vars(duration, decimalTime, julianDate), list(~ mean(.))) %&gt;% mutate(duration = 60, landcover = as.factor(6)) %&gt;% tidyr::crossing(observer) # run predict from model on it dfPredict &lt;- mutate(dfPredict, score = predict(modObsExp, newdata = dfPredict, type = &quot;response&quot;, allow.new.levels = TRUE ) ) %&gt;% mutate(score = scales::rescale(score)) fwrite(dfPredict %&gt;% dplyr::select(observer, score), file = &quot;data/dataObsExpScore.csv&quot; ) "],["adding-covariates-to-checklist-data.html", "Section 6 Adding Covariates to Checklist Data 6.1 Prepare libraries and data 6.2 Spatial subsampling 6.3 Temporal subsampling 6.4 Add checklist calibration index 6.5 Add landscape covariates 6.6 Spatial buffers around selected checklists 6.7 Spatial buffer-wide covariates", " Section 6 Adding Covariates to Checklist Data 6.1 Prepare libraries and data # load libs library(dplyr) library(readr) library(stringr) library(purrr) library(raster) library(glue) library(velox) library(tidyr) library(sf) # load saved data object load(&quot;data/data_prelim_processing.rdata&quot;) 6.2 Spatial subsampling # grid based spatial thinning gridsize &lt;- 1000 # grid size in metres effort_distance_max &lt;- 1000 # removing checklists with this distance # make grids across the study site hills &lt;- st_read(&quot;data/spatial/hillsShapefile/Nil_Ana_Pal.shp&quot;) %&gt;% st_transform(32643) grid &lt;- st_make_grid(hills, cellsize = gridsize) # split data by species data_spatial_thin &lt;- split(x = dataGrouped, f = dataGrouped$scientific_name) # spatial thinning on each species retains # site with maximum visits per grid cell data_spatial_thin &lt;- map(data_spatial_thin, function(df) { # count visits per locality df &lt;- group_by(df, locality) %&gt;% mutate(tot_effort = length(sampling_event_identifier)) %&gt;% ungroup() # remove sites with distances above spatial independence df &lt;- df %&gt;% filter(effort_distance_km &lt;= effort_distance_max) %&gt;% st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;)) %&gt;% `st_crs&lt;-`(4326) %&gt;% st_transform(32643) %&gt;% mutate(coordId = 1:nrow(.)) %&gt;% bind_cols(as_tibble(st_coordinates(.))) # whcih cell has which coords grid_contents &lt;- st_contains(grid, df) %&gt;% as_tibble() %&gt;% rename(cell = row.id, coordId = col.id) # what&#39;s the max point in each grid points_max &lt;- left_join(df %&gt;% st_drop_geometry(), grid_contents, by = &quot;coordId&quot; ) %&gt;% group_by(cell) %&gt;% filter(tot_effort == max(tot_effort)) return(points_max) }) # remove old data rm(dataGrouped) 6.3 Temporal subsampling Get 10 random (if available) observations of each species at each locality. # subsample data for random 10 observations dataSubsample &lt;- map(data_spatial_thin, function(df) { df &lt;- ungroup(df) df_to_locality &lt;- split(x = df, f = df$locality) df_samples &lt;- map_if( .x = df_to_locality, .p = function(x) { nrow(x) &gt; 10 }, .f = function(x) sample_n(x, 10, replace = FALSE) ) return(bind_rows(df_samples)) }) # bind all rows for data frame dataSubsample &lt;- bind_rows(dataSubsample) # remove previous data rm(data_spatial_thin) 6.4 Add checklist calibration index # read in obs score and extract numbers expertiseScore &lt;- read_csv(&quot;data/dataObsExpScore.csv&quot;) %&gt;% mutate(numObserver = str_extract(observer, &quot;\\\\d+&quot;)) %&gt;% dplyr::select(-observer) # group seis consist of multiple observers # in this case, seis need to have the highest expertise observer score # as the associated covariate # get unique observers per sei dataSeiScore &lt;- distinct( dataSubsample, sampling_event_identifier, observer_id ) %&gt;% # make list column of observers mutate(observers = str_split(observer_id, &quot;,&quot;)) %&gt;% unnest(cols = c(observers)) %&gt;% # add numeric observer id mutate(numObserver = str_extract(observers, &quot;\\\\d+&quot;)) %&gt;% # now get distinct sei and observer id numeric distinct(sampling_event_identifier, numObserver) # now add expertise score to sei dataSeiScore &lt;- left_join(dataSeiScore, expertiseScore, by = &quot;numObserver&quot; ) %&gt;% # get max expertise score per sei group_by(sampling_event_identifier) %&gt;% summarise(expertise = max(score)) # add to dataCovar dataSubsample &lt;- left_join(dataSubsample, dataSeiScore, by = &quot;sampling_event_identifier&quot; ) # remove data without expertise score dataSubsample &lt;- filter(dataSubsample, !is.na(expertise)) 6.5 Add landscape covariates # list landscape covariate stacks landscape_files &lt;- &quot;data/landcover\\\\landscape_resamp01km.tif&quot; # read in as stacks landscape_data &lt;- stack(landscape_files) # get proper names elev_names &lt;- c(&quot;elev&quot;, &quot;slope&quot;, &quot;aspect&quot;) chelsa_names &lt;- c(&quot;bio1&quot;,&quot;bio12&quot;) names(landscape_data) &lt;- as.character(glue(&#39;{c(elev_names, chelsa_names, &quot;landcover&quot;)}&#39;)) 6.6 Spatial buffers around selected checklists # assign neighbourhood radius in m sample_radius &lt;- 2.5 * 1e3 # get distinct points and make buffer ebird_buff &lt;- dataSubsample %&gt;% ungroup() %&gt;% distinct(X, Y) %&gt;% mutate(id = 1:nrow(.)) %&gt;% crossing(sample_radius) %&gt;% arrange(id) %&gt;% group_by(sample_radius) %&gt;% nest() %&gt;% ungroup() # convert to spatial features ebird_buff &lt;- mutate(ebird_buff, data = map2( data, sample_radius, function(df, rd) { df_sf &lt;- st_as_sf(df, coords = c(&quot;X&quot;, &quot;Y&quot;), crs = 32643) %&gt;% # add long lat bind_cols(as_tibble(st_coordinates(.))) %&gt;% # rename(longitude = X, latitude = Y) %&gt;% # # transform to modis projection # st_transform(crs = 32643) %&gt;% # buffer to create neighborhood around each point st_buffer(dist = rd) } ) ) 6.7 Spatial buffer-wide covariates 6.7.1 Mean environmental covariates All covariates are 2.5km mean values and prefixed “am_”. # get area mean for all preds except landcover, which is the last one env_area_mean &lt;- purrr::map(ebird_buff$data, function(df) { stk &lt;- landscape_data[[-dim(landscape_data)[3]]] # removing landcover here velstk &lt;- velox(stk) dextr &lt;- velstk$extract( sp = df, df = TRUE, fun = function(x) mean(x, na.rm = T) ) # assign names for joining names(dextr) &lt;- c(&quot;id&quot;, names(stk)) return(as_tibble(dextr)) }) # join to buffer data ebird_buff &lt;- ebird_buff %&gt;% mutate(data = map2(data, env_area_mean, inner_join, by = &quot;id&quot;)) 6.7.2 Proportions of land cover type # get the last element of each stack from the list # this is the landcover at that resolution lc_area_prop &lt;- purrr::map(ebird_buff$data, function(df) { lc &lt;- landscape_data[[dim(landscape_data)[3]]] # accessing landcover here lc_velox &lt;- velox(lc) lc_vals &lt;- lc_velox$extract(sp = df, df = TRUE) names(lc_vals) &lt;- c(&quot;id&quot;, &quot;lc&quot;) # get landcover proportions lc_prop &lt;- count(lc_vals, id, lc) %&gt;% group_by(id) %&gt;% mutate( lc = glue(&#39;lc_{str_pad(lc, 2, pad = &quot;0&quot;)}&#39;), prop = n / sum(n) ) %&gt;% dplyr::select(-n) %&gt;% tidyr::pivot_wider( names_from = lc, values_from = prop, values_fill = list(prop = 0) ) %&gt;% ungroup() return(lc_prop) }) # join to data ebird_buff &lt;- ebird_buff %&gt;% mutate(data = map2(data, lc_area_prop, inner_join, by = &quot;id&quot;)) 6.7.3 Link landscape covariates to checklists # duplicate scale data data_at_scale &lt;- ebird_buff # join the full data to landscape samples at each scale data_at_scale$data &lt;- map(data_at_scale$data, function(df) { df &lt;- st_drop_geometry(df) df &lt;- inner_join(dataSubsample, df, by = c(&quot;X&quot;, &quot;Y&quot;)) return(df) }) Save data to file. # write to file pmap(data_at_scale, function(sample_radius, data) { write_csv(data, path = glue(&#39;data/dataCovars_{str_pad(sample_radius/1e3, 2, pad = &quot;0&quot;)}km.csv&#39;)) message(glue(&#39;export done: data/dataCovars_{str_pad(sample_radius/1e3, 2, pad = &quot;0&quot;)}km.csv&#39;)) }) "],["modelling-species-occupancy.html", "Section 7 Modelling Species Occupancy 7.1 Load dataframe and scale covariates 7.2 Running a null model 7.3 Identifying covariates necessary to model the detection process 7.4 Land Cover and Climate 7.5 Goodness-of-fit tests", " Section 7 Modelling Species Occupancy 7.0.1 Load necessary libraries # Load libraries library(auk) library(lubridate) library(sf) library(unmarked) library(raster) library(ebirdst) library(MuMIn) library(AICcmodavg) library(fields) library(tidyverse) library(doParallel) library(snow) library(openxlsx) library(data.table) library(dplyr) library(ecodist) # Source necessary functions source(&quot;code/fun_screen_cor.R&quot;) source(&quot;code/fun_model_estimate_collection.r&quot;) 7.1 Load dataframe and scale covariates Here, we load the required dataframe that contains 10 random visits to a site. Please note that this process is repeated for each dataframe where environmental covariates were prepared at a spatial scale of 2.5 sq.km around each unique locality. We also scaled all covariates (mean around 0 and standard deviation of 1) # Load in the prepared dataframe that contains 10 random visits to each site dat &lt;- fread(&quot;data/dataCovars_2.5km.csv&quot;, header = T) setDF(dat) head(dat) # Some more pre-processing to get the right data structures # Ensuring that only Traveling and Stationary checklists were considered names(dat) dat &lt;- dat %&gt;% filter(protocol_type %in% c(&quot;Traveling&quot;, &quot;Stationary&quot;)) # We take all stationary counts and give them a distance of 100 m (so 0.1 km), # as that&#39;s approximately the max normal hearing distance for people doing point counts. dat &lt;- dat %&gt;% mutate(effort_distance_km = replace( effort_distance_km, which(effort_distance_km == 0 &amp; protocol_type == &quot;Stationary&quot;), 0.1 )) # Converting time observations started to numeric and adding it as a new column # This new column will be minute_observations_started dat &lt;- dat %&gt;% mutate(min_obs_started = strtoi(as.difftime(time_observations_started, format = &quot;%H:%M:%S&quot;, units = &quot;mins&quot; ))) # Adding the julian date to the dataframe dat &lt;- dat %&gt;% mutate(julian_date = lubridate::yday(dat$observation_date)) # Removing other unnecessary columns from the dataframe and creating a clean one without the rest names(dat) dat &lt;- dat[, -c(1, 4, 5, 16, 18, 21, 23, 24, 25, 26, 28:37, 39:45, 47)] # Rename column names: names(dat) &lt;- c( &quot;duration_minutes&quot;, &quot;effort_distance_km&quot;, &quot;locality&quot;, &quot;locality_type&quot;, &quot;locality_id&quot;, &quot;observer_id&quot;, &quot;observation_date&quot;, &quot;scientific_name&quot;, &quot;observation_count&quot;, &quot;protocol_type&quot;, &quot;number_observers&quot;, &quot;pres_abs&quot;, &quot;tot_effort&quot;, &quot;longitude&quot;, &quot;latitude&quot;, &quot;expertise&quot;, &quot;bio_1.y&quot;, &quot;bio_12.y&quot;, &quot;lc_02.y&quot;, &quot;lc_06.y&quot;, &quot;lc_01.y&quot;, &quot;lc_07.y&quot;, &quot;lc_04.y&quot;, &quot;lc_05.y&quot;, &quot;min_obs_started&quot;, &quot;julian_date&quot; ) dat.1 &lt;- dat %&gt;% mutate( year = year(observation_date), pres_abs = as.integer(pres_abs) ) # occupancy modeling requires an integer response # Dividing Annual Mean Temperature by 10 to arrive at accurate values of temperature dat.1$bio_1.y &lt;- dat.1$bio_1.y / 10 # Scaling detection and occupancy covariates dat.scaled &lt;- dat.1 dat.scaled[, c(1, 2, 11, 16:25)] &lt;- scale(dat.scaled[, c(1, 2, 11, 16:25)]) # Scaling and standardizing detection and site-level covariates fwrite(dat.scaled, file = &quot;data/scaled-covars-2.5km.csv&quot;) dat.scaled &lt;- fread(&quot;data/05_scaled-covars-2.5km.csv&quot;, header = T) setDF(dat.scaled) head(dat.scaled) # Ensure observation_date column is in the right format dat.scaled$observation_date &lt;- format( as.Date( dat.scaled$observation_date, &quot;%m/%d/%Y&quot; ), &quot;%Y-%m-%d&quot; ) # Testing for correlations before running further analyses # Most are uncorrelated since we decided to keep only 2 climatic and 6 land cover predictors source(&quot;code/screen_cor.R&quot;) names(dat.scaled) screen.cor(dat.scaled[, c(1, 2, 11, 16:25)], threshold = 0.5) 7.2 Running a null model # All null models are stored in lists below all_null &lt;- list() # Add a progress bar for the loop pb &lt;- txtProgressBar( min = 0, max = length(unique(dat.scaled$scientific_name)), style = 3 ) # text based bar for (i in 1:length(unique(dat.scaled$scientific_name))) { data &lt;- dat.scaled %&gt;% filter(dat.scaled$scientific_name == unique(dat.scaled$scientific_name)[i]) # Preparing data for the unmarked model occ &lt;- filter_repeat_visits(data, min_obs = 1, max_obs = 10, annual_closure = FALSE, n_days = 2600, # 7 years is considered a period of closure date_var = &quot;observation_date&quot;, site_vars = c(&quot;locality_id&quot;) ) obs_covs &lt;- c( &quot;min_obs_started&quot;, &quot;duration_minutes&quot;, &quot;effort_distance_km&quot;, &quot;number_observers&quot;, &quot;protocol_type&quot;, &quot;expertise&quot;, &quot;julian_date&quot; ) # format for unmarked occ_wide &lt;- format_unmarked_occu(occ, site_id = &quot;site&quot;, response = &quot;pres_abs&quot;, site_covs = c(&quot;locality_id&quot;, &quot;lc_01.y&quot;, &quot;lc_02.y&quot;, &quot;lc_04.y&quot;, &quot;lc_05.y&quot;, &quot;lc_06.y&quot;, &quot;lc_07.y&quot;, &quot;bio_1.y&quot;, &quot;bio_12.y&quot;), obs_covs = obs_covs ) # Convert this dataframe of observations into an unmarked object to start fitting occupancy models occ_um &lt;- formatWide(occ_wide, type = &quot;unmarkedFrameOccu&quot;) # Set up the model all_null[[i]] &lt;- occu(~1 ~ 1, data = occ_um) names(all_null)[i] &lt;- unique(dat.scaled$scientific_name)[i] setTxtProgressBar(pb, i) } close(pb) # Store all the model outputs for each species capture.output(all_null, file = &quot;data\\results\\null_models.csv&quot;) 7.3 Identifying covariates necessary to model the detection process Here, we use the unmarked package in R (Fiske and Chandler 2019) to identify detection level covariates that are important for each species # All models are stored in lists below det_dred &lt;- list() # Subsetting those models whose deltaAIC&lt;4 (Burnham et al., 2011) top_det &lt;- list() # Getting model averaged coefficients and relative importance scores det_avg &lt;- list() det_imp &lt;- list() # Getting model estimates det_modelEst &lt;- list() # Add a progress bar for the loop pb &lt;- txtProgressBar(min = 0, max = length(unique(dat.scaled$scientific_name)), style = 3) # text based bar for (i in 1:length(unique(dat.scaled$scientific_name))) { data &lt;- dat.scaled %&gt;% filter(dat.scaled$scientific_name == unique(dat.scaled$scientific_name)[i]) # Preparing data for the unmarked model occ &lt;- filter_repeat_visits(data, min_obs = 1, max_obs = 10, annual_closure = FALSE, n_days = 2600, # 6 years is considered a period of closure date_var = &quot;observation_date&quot;, site_vars = c(&quot;locality_id&quot;) ) obs_covs &lt;- c( &quot;min_obs_started&quot;, &quot;duration_minutes&quot;, &quot;effort_distance_km&quot;, &quot;number_observers&quot;, &quot;protocol_type&quot;, &quot;expertise&quot;, &quot;julian_date&quot; ) # format for unmarked occ_wide &lt;- format_unmarked_occu(occ, site_id = &quot;site&quot;, response = &quot;pres_abs&quot;, site_covs = c(&quot;locality_id&quot;, &quot;lc_01.y&quot;, &quot;lc_02.y&quot;, &quot;lc_04.y&quot;, &quot;lc_05.y&quot;, &quot;lc_06.y&quot;, &quot;lc_07.y&quot;, &quot;bio_1.y&quot;, &quot;bio_12.y&quot;), obs_covs = obs_covs ) # Convert this dataframe of observations into an unmarked object to start fitting occupancy models occ_um &lt;- formatWide(occ_wide, type = &quot;unmarkedFrameOccu&quot;) # Fit a global model with all detection level covariates global_mod &lt;- occu(~ min_obs_started + julian_date + duration_minutes + effort_distance_km + number_observers + protocol_type + expertise ~ 1, data = occ_um) # Set up the cluster clusterType &lt;- if (length(find.package(&quot;snow&quot;, quiet = TRUE))) &quot;SOCK&quot; else &quot;PSOCK&quot; clust &lt;- try(makeCluster(getOption(&quot;cl.cores&quot;, 6), type = clusterType)) clusterEvalQ(clust, library(unmarked)) clusterExport(clust, &quot;occ_um&quot;) # Dredging the same det_dred[[i]] &lt;- pdredge(global_mod, clust) names(det_dred)[i] &lt;- unique(dat.scaled$scientific_name)[i] # Get the top models, which we&#39;ll define as those with deltaAICc &lt; 2 top_det[[i]] &lt;- get.models(det_dred[[i]], subset = delta &lt; 2, cluster = clust) names(top_det)[i] &lt;- unique(dat.scaled$scientific_name)[i] # Obtaining model averaged coefficients if (length(top_det[[i]]) &gt; 1) { a &lt;- model.avg(top_det[[i]], fit = TRUE) det_avg[[i]] &lt;- as.data.frame(a$coefficients) names(det_avg)[i] &lt;- unique(dat.scaled$scientific_name)[i] det_modelEst[[i]] &lt;- data.frame( Coefficient = coefTable(a, full = T)[, 1], SE = coefTable(a, full = T)[, 2], lowerCI = confint(a)[, 1], upperCI = confint(a)[, 2], z_value = (summary(a)$coefmat.full)[, 3], Pr_z = (summary(a)$coefmat.full)[, 4] ) names(det_modelEst)[i] &lt;- unique(dat.scaled$scientific_name)[i] det_imp[[i]] &lt;- as.data.frame(MuMIn::importance(a)) names(det_imp)[i] &lt;- unique(dat.scaled$scientific_name)[i] } else { det_avg[[i]] &lt;- as.data.frame(unmarked::coef(top_det[[i]][[1]])) names(det_avg)[i] &lt;- unique(dat.scaled$scientific_name)[i] lowDet &lt;- data.frame(lowerCI = confint(top_det[[i]][[1]], type = &quot;det&quot;)[, 1]) upDet &lt;- data.frame(upperCI = confint(top_det[[i]][[1]], type = &quot;det&quot;)[, 2]) zDet &lt;- data.frame(summary(top_det[[i]][[1]])$det[, 3]) Pr_zDet &lt;- data.frame(summary(top_det[[i]][[1]])$det[, 4]) Coefficient &lt;- coefTable(top_det[[i]][[1]])[, 1] SE &lt;- coefTable(top_det[[i]][[1]])[, 2] det_modelEst[[i]] &lt;- data.frame( Coefficient = Coefficient[2:9], SE = SE[2:9], lowerCI = lowDet, upperCI = upDet, z_value = zDet, Pr_z = Pr_zDet ) names(det_modelEst)[i] &lt;- unique(dat.scaled$scientific_name)[i] } setTxtProgressBar(pb, i) stopCluster(clust) } close(pb) ## Storing output from the above models in excel sheets # 1. Store all the dredged model outputs for each species (variable: det_dred() - see above) write.xlsx(det_dred, file = &quot;data\\results\\det_dred.xlsx&quot;) # 2. Store all the model averaged outputs for each species and the relative importance score write.xlsx(det_avg, file = &quot;data\\results\\det_avg.xlsx&quot;, rowNames = T, colNames = T) write.xlsx(det_imp, file = &quot;data\\results\\det_imp.xlsx&quot;, rowNames = T, colNames = T) write.xlsx(det_modelEst, file = &quot;data\\results\\det_modelEst.xlsx&quot;, rowNames = T, colNames = T) 7.4 Land Cover and Climate # All dredged models are stored in lists below lc_clim &lt;- list() # Subsetting those models whose deltaAIC&lt;2 (Burnham et al., 2011) top_lc_clim &lt;- list() # Getting model averaged coefficients and relative importance scores lc_clim_avg &lt;- list() lc_clim_imp &lt;- list() # Storing Model estimates lc_clim_modelEst &lt;- list() # Add a progress bar for the loop pb &lt;- txtProgressBar(min = 0, max = length(unique(dat.scaled$scientific_name)), style = 3) # text based bar for (i in 1:length(unique(dat.scaled$scientific_name))) { data &lt;- dat.scaled %&gt;% filter(dat.scaled$scientific_name == unique(dat.scaled$scientific_name)[1]) # Preparing data for the unmarked model occ &lt;- filter_repeat_visits(data, min_obs = 1, max_obs = 10, annual_closure = FALSE, n_days = 2600, # 6 years is considered a period of closure date_var = &quot;observation_date&quot;, site_vars = c(&quot;locality_id&quot;) ) obs_covs &lt;- c( &quot;min_obs_started&quot;, &quot;duration_minutes&quot;, &quot;effort_distance_km&quot;, &quot;number_observers&quot;, &quot;protocol_type&quot;, &quot;expertise&quot;, &quot;julian_date&quot; ) # format for unmarked occ_wide &lt;- format_unmarked_occu(occ, site_id = &quot;site&quot;, response = &quot;pres_abs&quot;, site_covs = c(&quot;locality_id&quot;, &quot;lc_01.y&quot;, &quot;lc_02.y&quot;, &quot;lc_04.y&quot;, &quot;lc_05.y&quot;, &quot;lc_06.y&quot;, &quot;lc_07.y&quot;, &quot;bio_1.y&quot;, &quot;bio_12.y&quot;), obs_covs = obs_covs ) # Convert this dataframe of observations into an unmarked object to start fitting occupancy models occ_um &lt;- formatWide(occ_wide, type = &quot;unmarkedFrameOccu&quot;) model_lc_clim &lt;- occu(~ min_obs_started + julian_date + duration_minutes + effort_distance_km + number_observers + protocol_type + expertise ~ lc_01.y + lc_02.y + lc_04.y + lc_05.y + lc_06.y + lc_07.y + bio_1.y + bio_12.y, data = occ_um) # Set up the cluster clusterType &lt;- if (length(find.package(&quot;snow&quot;, quiet = TRUE))) &quot;SOCK&quot; else &quot;PSOCK&quot; clust &lt;- try(makeCluster(getOption(&quot;cl.cores&quot;, 6), type = clusterType)) clusterEvalQ(clust, library(unmarked)) clusterExport(clust, &quot;occ_um&quot;) # Detection terms are fixed det_terms &lt;- c( &quot;p(duration_minutes)&quot;, &quot;p(effort_distance_km)&quot;, &quot;p(expertise)&quot;, &quot;p(julian_date)&quot;, &quot;p(min_obs_started)&quot;, &quot;p(number_observers)&quot;, &quot;p(protocol_type)&quot; ) # Dredging lc_clim[[i]] &lt;- pdredge(model_lc_clim, clust, fixed = det_terms) names(lc_clim)[i] &lt;- unique(dat.scaled$scientific_name)[i] # Identiying top subset of models based on deltaAIC scores being less than 2 (Burnham et al., 2011) top_lc_clim[[i]] &lt;- get.models(lc_clim[[i]], subset = delta &lt; 2, cluster = clust) names(top_lc_clim)[i] &lt;- unique(dat.scaled$scientific_name)[i] # Obtaining model averaged coefficients for both candidate model subsets if (length(top_lc_clim[[i]]) &gt; 1) { a &lt;- model.avg(top_lc_clim[[i]], fit = TRUE) lc_clim_avg[[i]] &lt;- as.data.frame(a$coefficients) names(lc_clim_avg)[i] &lt;- unique(dat.scaled$scientific_name)[i] lc_clim_modelEst[[i]] &lt;- data.frame( Coefficient = coefTable(a, full = T)[, 1], SE = coefTable(a, full = T)[, 2], lowerCI = confint(a)[, 1], upperCI = confint(a)[, 2], z_value = (summary(a)$coefmat.full)[, 3], Pr_z = (summary(a)$coefmat.full)[, 4] ) names(lc_clim_modelEst)[i] &lt;- unique(dat.scaled$scientific_name)[i] lc_clim_imp[[i]] &lt;- as.data.frame(MuMIn::importance(a)) names(lc_clim_imp)[i] &lt;- unique(dat.scaled$scientific_name)[i] } else { lc_clim_avg[[i]] &lt;- as.data.frame(unmarked::coef(top_lc_clim[[i]][[1]])) names(lc_clim_avg)[i] &lt;- unique(dat.scaled$scientific_name)[i] lowSt &lt;- data.frame(lowerCI = confint(top_lc_clim[[i]][[1]], type = &quot;state&quot;)[, 1]) lowDet &lt;- data.frame(lowerCI = confint(top_lc_clim[[i]][[1]], type = &quot;det&quot;)[, 1]) upSt &lt;- data.frame(upperCI = confint(top_lc_clim[[i]][[1]], type = &quot;state&quot;)[, 2]) upDet &lt;- data.frame(upperCI = confint(top_lc_clim[[i]][[1]], type = &quot;det&quot;)[, 2]) zSt &lt;- data.frame(z_value = summary(top_lc_clim[[i]][[1]])$state[, 3]) zDet &lt;- data.frame(z_value = summary(top_lc_clim[[i]][[1]])$det[, 3]) Pr_zSt &lt;- data.frame(Pr_z = summary(top_lc_clim[[i]][[1]])$state[, 4]) Pr_zDet &lt;- data.frame(Pr_z = summary(top_lc_clim[[i]][[1]])$det[, 4]) lc_clim_modelEst[[i]] &lt;- data.frame( Coefficient = coefTable(top_lc_clim[[i]][[1]])[, 1], SE = coefTable(top_lc_clim[[i]][[1]])[, 2], lowerCI = rbind(lowSt, lowDet), upperCI = rbind(upSt, upDet), z_value = rbind(zSt, zDet), Pr_z = rbind(Pr_zSt, Pr_zDet) ) names(lc_clim_modelEst)[i] &lt;- unique(dat.scaled$scientific_name)[i] } setTxtProgressBar(pb, i) stopCluster(clust) } close(pb) # 1. Store all the dredged model outputs for each species (for both landcover and climate) write.xlsx(lc_clim, file = &quot;data\\results\\lc-clim.xlsx&quot;) # 2. Store all the model averaged outputs for each species and relative importance scores write.xlsx(lc_clim_avg, file = &quot;data\\results\\lc-clim-avg.xlsx&quot;, rowNames = T, colNames = T) write.xlsx(lc_clim_imp, file = &quot;data\\results\\lc-clim-imp.xlsx&quot;, rowNames = T, colNames = T) # 3. Store all model estimates write.xlsx(lc_clim_modelEst, file = &quot;data\\results\\lc-clim-modelEst.xlsx&quot;, rowNames = T, colNames = T) 7.5 Goodness-of-fit tests goodness_of_fit &lt;- data.frame() # Add a progress bar for the loop pb &lt;- txtProgressBar(min = 0, max = length(unique(dat.scaled$scientific_name)), style = 3) # text based bar for (i in 1:length(unique(dat.scaled$scientific_name))) { data &lt;- dat.scaled %&gt;% filter(dat.scaled$scientific_name == unique(dat.scaled$scientific_name)[i]) # Preparing data for the unmarked model occ &lt;- filter_repeat_visits(data, min_obs = 1, max_obs = 10, annual_closure = FALSE, n_days = 2600, # 6 years is considered a period of closure date_var = &quot;observation_date&quot;, site_vars = c(&quot;locality_id&quot;) ) obs_covs &lt;- c( &quot;min_obs_started&quot;, &quot;duration_minutes&quot;, &quot;effort_distance_km&quot;, &quot;number_observers&quot;, &quot;protocol_type&quot;, &quot;expertise&quot;, &quot;julian_date&quot; ) # format for unmarked occ_wide &lt;- format_unmarked_occu(occ, site_id = &quot;site&quot;, response = &quot;pres_abs&quot;, site_covs = c(&quot;locality_id&quot;, &quot;lc_01.y&quot;, &quot;lc_02.y&quot;, &quot;lc_04.y&quot;, &quot;lc_05.y&quot;, &quot;lc_06.y&quot;, &quot;lc_07.y&quot;, &quot;bio_1.y&quot;, &quot;bio_12.y&quot;), obs_covs = obs_covs ) # Convert this dataframe of observations into an unmarked object to start fitting occupancy models occ_um &lt;- formatWide(occ_wide, type = &quot;unmarkedFrameOccu&quot;) model_lc_clim &lt;- occu(~ min_obs_started + julian_date + duration_minutes + effort_distance_km + number_observers + protocol_type + expertise ~ lc_01.y + lc_02.y + lc_04.y + lc_05.y + lc_06.y + lc_07.y + bio_1.y + bio_12.y, data = occ_um) occ_gof &lt;- mb.gof.test(model_lc_clim, nsim = 5000, plot.hist = FALSE) p.value &lt;- occ_gof$p.value c.hat &lt;- occ_gof$c.hat.est scientific_name &lt;- unique(data$scientific_name) a &lt;- data.frame(scientific_name, p.value, c.hat) goodness_of_fit &lt;- rbind(a, goodness_of_fit) setTxtProgressBar(pb, i) } close(pb) write.csv(goodness_of_fit, &quot;data\\results\\05_goodness-of-fit-2.5km.csv&quot;) "],["visualizing-occupancy-predictor-effects.html", "Section 8 Visualizing Occupancy Predictor Effects 8.1 Prepare libraries 8.2 Load species list 8.3 Show AIC weight importance 8.4 Prepare model coefficient data 8.5 Get predictor effects 8.6 Main Text Figure 4", " Section 8 Visualizing Occupancy Predictor Effects In this section, we will visualize the cumulative AIC weights and the magnitude and direction of species-specific probability of occupancy 8.1 Prepare libraries # to load data library(readxl) # to handle data library(dplyr) library(readr) library(forcats) library(tidyr) library(purrr) library(stringr) library(data.table) # to wrangle models source(&quot;code/fun_model_estimate_collection.r&quot;) source(&quot;code/fun_make_resp_data.r&quot;) # nice tables library(knitr) library(kableExtra) # plotting library(ggplot2) library(patchwork) source(&quot;code/fun_plot_interaction.r&quot;) 8.2 Load species list # list of species species &lt;- read_csv(&quot;data/species_list.csv&quot;) list_of_species &lt;- as.character(species$scientific_name) 8.3 Show AIC weight importance 8.3.1 Read in AIC weight data # which files to read file_names &lt;- c(&quot;data/results/lc-clim-imp.xlsx&quot;) # read in sheets by species model_imp &lt;- map(file_names, function(f) { md_list &lt;- map(list_of_species, function(sn) { # some sheets are not found tryCatch({ readxl::read_excel(f, sheet = sn) %&gt;% `colnames&lt;-`(c(&quot;predictor&quot;, &quot;AIC_weight&quot;)) %&gt;% filter(str_detect(predictor, &quot;psi&quot;)) %&gt;% mutate(predictor = stringr::str_extract(predictor, pattern = stringr::regex(&quot;\\\\((.*?)\\\\)&quot;)), predictor = stringr::str_replace_all(predictor, &quot;[//(//)]&quot;, &quot;&quot;), predictor = stringr::str_remove(predictor, &quot;\\\\.y&quot;)) }, error = function(e) { message(as.character(e)) } ) }) names(md_list) &lt;- list_of_species return(md_list) }) 8.3.2 Prepare cumulative AIC weight data # assign scale - minimum spatial scale at which the analysis was carried out to account for observer effort names(model_imp) &lt;- c(&quot;2.5km&quot;) model_imp &lt;- imap(model_imp, function(.x, .y) { .x &lt;- bind_rows(.x) .x$scale &lt;- .y return(.x) }) # bind rows model_imp &lt;- map(model_imp, bind_rows) %&gt;% bind_rows() # convert to numeric model_imp$AIC_weight &lt;- as.numeric(model_imp$AIC_weight) model_imp$scale &lt;- as.factor(model_imp$scale) levels(model_imp$scale) &lt;- c(&quot;2.5km&quot;) # Let&#39;s get a summary of cumulative variable importance model_imp &lt;-group_by(model_imp, predictor) %&gt;% summarise(mean_AIC = mean(AIC_weight), sd_AIC=sd(AIC_weight), min_AIC = min(AIC_weight), max_AIC = max(AIC_weight), med_AIC = median(AIC_weight)) # write to file write_csv(model_imp, file = &quot;data/results/cumulative_AIC_weights.csv&quot;) Read data back in. # read data and make factor model_imp &lt;- read_csv(&quot;data/results/cumulative_AIC_weights.csv&quot;) model_imp$predictor &lt;- as_factor(model_imp$predictor) # make nice names predictor_name &lt;- tibble(predictor = levels(model_imp$predictor), pred_name = c(&quot;Annual Mean Temperature (°C)&quot;, &quot;Annual Precipitation (mm)&quot;, &quot;% Agriculture&quot;, &quot;% Forests&quot;, &quot;% Plantations&quot;, &quot;% Settlements&quot;, &quot;% Tea&quot;, &quot;% Water Bodies&quot;)) # rename predictor model_imp &lt;- left_join(model_imp, predictor_name) Prepare figure for cumulative AIC weight. Figure code is hidden in versions rendered as HTML and PDF. fig_aic &lt;- ggplot(model_imp)+ geom_pointrange(aes(x = reorder(predictor, mean_AIC), y = mean_AIC, ymin = mean_AIC - sd_AIC, ymax = mean_AIC + sd_AIC))+ geom_text(aes(x = predictor, y = 0.2, label = pred_name), angle = 0, hjust = &quot;inward&quot;, vjust = 2)+ # scale_y_continuous(breaks = seq(45, 75, 10))+ scale_x_discrete(labels = NULL)+ # scale_color_brewer(palette = &quot;RdBu&quot;, values = c(0.5, 1))+ coord_flip( # ylim = c(45, 75) )+ theme_test()+ theme(legend.position = &quot;none&quot;)+ labs(x = &quot;Predictor&quot;, y = &quot;Cumulative AIC weight&quot;) ggsave(fig_aic, filename = &quot;figs/fig_aic_weight.png&quot;, device = png(), dpi = 300, width = 79, height = 120, units = &quot;mm&quot;) 8.4 Prepare model coefficient data file_read &lt;- c(&quot;data/results/lc-clim-modelEst.xlsx&quot;) # read data as list column model_est &lt;- map(file_read, function(fr) { md_list &lt;- map(list_of_species, function(sn) { readxl::read_excel(fr, sheet = sn) }) names(md_list) &lt;- list_of_species return(md_list) }) # prepare model data scales = c(&quot;2.5km&quot;) model_data &lt;- tibble(scale = scales, scientific_name = list_of_species) %&gt;% arrange(desc(scale)) # rename model data components and separate predictors names &lt;- c(&quot;predictor&quot;, &quot;coefficient&quot;, &quot;se&quot;, &quot;ci_lower&quot;, &quot;ci_higher&quot;, &quot;z_value&quot;, &quot;p_value&quot;) # get data for plotting: model_est &lt;- map(model_est, function(l) { map(l, function(df) { colnames(df) &lt;- names df &lt;- separate_interaction_terms(df) df &lt;- make_response_data(df) return(df) }) }) # add names and scales model_est &lt;- map(model_est, function(l) { imap(l, function(.x, .y) { mutate(.x, scientific_name = .y) }) }) # add names to model estimates names(model_est) &lt;- scales model_est &lt;- imap(model_est, function(.x, .y) { bind_rows(.x) %&gt;% mutate(scale = .y) }) # remove modulators model_est &lt;- bind_rows(model_est) %&gt;% select(-matches(&quot;modulator&quot;)) # join data to species name model_data &lt;- model_data %&gt;% left_join(model_est) # Keep only those predictors whose p-vlaues are significant: model_data &lt;- model_data %&gt;% filter(p_value &lt; 0.05) Export predictor effects. # get predictor effect data data_predictor_effect &lt;- distinct(model_data, scientific_name, se, predictor, coefficient) # write to file write_csv(data_predictor_effect, path = &quot;data/results/data_predictor_effect.csv&quot;) Export model data. model_data_to_file &lt;- model_data %&gt;% select(predictor, data, scientific_name, scale) %&gt;% unnest(cols = &quot;data&quot;) # remove .y model_data_to_file &lt;- model_data_to_file %&gt;% mutate(predictor = str_remove(predictor, &quot;\\\\.y&quot;)) write_csv(model_data_to_file, &quot;data/results/data_occupancy_predictors.csv&quot;) Read in data after clearing R session. # read from file model_data &lt;- read_csv(&quot;data/results/data_predictor_effect.csv&quot;) Fix predictor name. # remove .y from predictors model_data &lt;- model_data %&gt;% mutate_at(.vars = c(&quot;predictor&quot;), .funs = function(x){ stringr::str_remove(x, &quot;.y&quot;) }) 8.5 Get predictor effects # is the coeff positive? how many positive per scale per predictor per axis of split? data_predictor &lt;- mutate(model_data, direction = coefficient &gt; 0) %&gt;% count(predictor, direction) %&gt;% mutate(mag = n * (if_else(direction, 1, -1))) # wrangle data to get nice bars data_predictor &lt;- data_predictor%&gt;% select(-n) %&gt;% drop_na(direction) %&gt;% mutate(direction = ifelse(direction, &quot;positive&quot;, &quot;negative&quot;)) %&gt;% pivot_wider(values_from = &quot;mag&quot;, names_from = &quot;direction&quot;) %&gt;% mutate_at(vars(positive, negative), ~if_else(is.na(.), 0, .)) data_predictor_long &lt;- data_predictor %&gt;% pivot_longer(cols = c(&quot;negative&quot;, &quot;positive&quot;), names_to = &quot;effect&quot;, values_to = &quot;magnitude&quot;) # write write_csv(data_predictor_long, path = &quot;data/results/data_predictor_direction_nSpecies.csv&quot;) Prepare data to determine the direction (positive or negative) of the effect of each predictor. How many species are affected in either direction? # join with predictor names and relative AIC data_predictor_long &lt;- left_join(data_predictor_long, model_imp) Prepare figure of the number of species affected in each direction. Figure code is hidden in versions rendered as HTML and PDF. 8.6 Main Text Figure 4 library(patchwork) # wrap fig_predictor_effect &lt;- wrap_plots(fig_aic, fig_predictor)+ plot_annotation(tag_levels = &quot;a&quot;, tag_prefix = &quot;(&quot;, tag_suffix = &quot;)&quot;) # save ggsave(fig_predictor_effect, filename = &quot;figs/fig_04_aic_weight_effect.png&quot;, dpi = 300, width = 168, height = 130, units = &quot;mm&quot;) (a) Cumulative AIC weights suggest that climatic predictors have higher relative importance when compared to landscape predictors. (b) The direction of association between species-specific probability of occupancy and climatic and landscape is shown here. While climatic predictors were both positively and negatively associated with the probability of occupancy for a number of species, human-associated land cover types were largely negatively associated with species-specific probability of occupancy. "]]
