[["index.html", "Source code for Using citizen science to parse climatic and landcover influences on bird occupancy within a tropical biodiversity hotspot Section 1 Introduction 1.1 Attribution 1.2 Data access 1.3 Data processing 1.4 Main Text Figure 1", " Source code for Using citizen science to parse climatic and landcover influences on bird occupancy within a tropical biodiversity hotspot Vijay Ramesh Pratik R. Gupte Morgan W. Tingley VV Robin Ruth DeFries 2021-06-17 Section 1 Introduction This is the readable version containing analysis that models associations between environmental predictors (climate and landcover) and citizen science observations of birds across the Nilgiri and Anamalai Hills of the Western Ghats Biodiversity Hotspot. Methods and format are derived from Strimas-Mackey et al.. 1.1 Attribution Please contact the following in case of interest in the project. Vijay Ramesh (lead author) PhD student, Columbia University Pratik Gupte (repo maintainer) PhD student, University of Groningen 1.2 Data access The data used in this work are available from eBird. 1.3 Data processing The data processing for this project is described in the following sections. Navigate through them using the links in the sidebar. 1.4 Main Text Figure 1 Figure prepared in QGIS 3.10. A shaded relief of the study area - the Nilgiri and the Anamalai hills are shown in this figure. This map was made using the SRTM digital elevation model at a spatial resolution of 1km and data from Natural Earth were used to outline boundaries of water bodies. "],["preparing-ebird-data.html", "Section 2 Preparing eBird Data 2.1 Prepare libraries and data sources 2.2 Filter data 2.3 Process filtered data 2.4 Spatial filter 2.5 Handle presence data 2.6 Add decimal time", " Section 2 Preparing eBird Data 2.1 Prepare libraries and data sources Here, we will load the necessary libraries required for preparing the eBird data. Please download the latest versions of the eBird Basic Dataset (for India) and the eBird Sampling dataset from https://ebird.org/data/download. # load libraries library(tidyverse) library(readr) library(sf) library(auk) library(readxl) library(lubridate) # custom sum function sum.no.na &lt;- function(x) { sum(x, na.rm = T) } # set file paths for auk functions # To use these two datasets, please download the latest versions from https://ebird.org/data/download and set the file path accordingly. Since these two datasets are extremely large, we have not uploaded the same on github. f_in_ebd &lt;- file.path(&quot;data/ebd_IN_relMar-2021.txt&quot;) f_in_sampling &lt;- file.path(&quot;data/ebd_sampling_relMar-2021.txt&quot;) 2.2 Filter data Insert the list of species that we will be analyzing in this study. We initially chose those species that occurred in at least 5% of all checklists across 50% of the 25 x 25 km cells from where they have been reported, resulting in a total of 93 species. To arrive at this final list of species, we carried out further pre-processing which can be found in Section 2 of the Supplementary material. # add species of interest specieslist &lt;- read.csv(&quot;data/species_list.csv&quot;) speciesOfInterest &lt;- as.character(specieslist$scientific_name) Here, we set broad spatial filters for the states of Kerala, Tamil Nadu and Karnataka and keep only those checklists for our list of species that were reported between 1st Jan 2013 and 31st Dec 2020. # run filters using auk packages ebd_filters &lt;- auk_ebd(f_in_ebd, f_in_sampling) %&gt;% auk_species(speciesOfInterest) %&gt;% auk_country(country = &quot;IN&quot;) %&gt;% auk_state(c(&quot;IN-KL&quot;, &quot;IN-TN&quot;, &quot;IN-KA&quot;)) %&gt;% # Restricting geography to TamilNadu, Kerala &amp; Karnataka auk_date(c(&quot;2013-01-01&quot;, &quot;2020-12-31&quot;)) %&gt;% auk_complete() # check filters ebd_filters Below code need not be run if it has been filtered once already and the above path leads to the right dataset. NB: This is a computation heavy process, run with caution. # specify output location and perform filter f_out_ebd &lt;- &quot;data/01_ebird-filtered-EBD-westernGhats.txt&quot; f_out_sampling &lt;- &quot;data/01_ebird-filtered-sampling-westernGhats.txt&quot; ebd_filtered &lt;- auk_filter(ebd_filters, file = f_out_ebd, file_sampling = f_out_sampling, overwrite = TRUE ) 2.3 Process filtered data The data has been filtered above using the auk functions. We will now work with the filtered checklist observations (Please note that we have not yet spatially filtered the checklists to the confines of our study area, which is the Nilgiris and the Anamalai hills. This step is carried out further on). # read in the data ebd &lt;- read_ebd(f_out_ebd) eBird checklists only suggest whether a species was reported at a particular location. To arrive at absence data, we use a process known as zero-filling (Johnston et al. 2019), wherein a new dataframe is created with a 0 marked for each checklist when the bird was not observed. # fill zeroes zf &lt;- auk_zerofill(f_out_ebd, f_out_sampling) new_zf &lt;- collapse_zerofill(zf) Let us now choose specific columns necessary for further analysis. # choose columns of interest columnsOfInterest &lt;- c( &quot;checklist_id&quot;, &quot;scientific_name&quot;, &quot;common_name&quot;, &quot;observation_count&quot;, &quot;locality&quot;, &quot;locality_id&quot;, &quot;locality_type&quot;, &quot;latitude&quot;, &quot;longitude&quot;, &quot;observation_date&quot;, &quot;time_observations_started&quot;, &quot;observer_id&quot;, &quot;sampling_event_identifier&quot;, &quot;protocol_type&quot;, &quot;duration_minutes&quot;, &quot;effort_distance_km&quot;, &quot;effort_area_ha&quot;, &quot;number_observers&quot;, &quot;species_observed&quot;, &quot;reviewed&quot; ) # make list of presence and absence data and choose cols of interest data &lt;- list(ebd, new_zf) %&gt;% map(function(x) { x %&gt;% select(one_of(columnsOfInterest)) }) # remove zerofills to save working memory rm(zf, new_zf) gc() # check for presences and absence in absences df, remove essentially the presences df which may lead to erroneous analysis data[[2]] &lt;- data[[2]] %&gt;% filter(species_observed == F) 2.4 Spatial filter A spatial filter is now supplied to further restrict our list of observations to the confines of the Nilgiris and the Anamalai hills of the Western Ghats biodiversity hotspot. # load shapefile of the study area library(sf) hills &lt;- st_read(&quot;data/spatial/hillsShapefile/Nil_Ana_Pal.shp&quot;) # write a prelim filter by bounding box box &lt;- st_bbox(hills) # get data spatial coordinates dataLocs &lt;- data %&gt;% map(function(x) { select(x, longitude, latitude) %&gt;% filter(between(longitude, box[&quot;xmin&quot;], box[&quot;xmax&quot;]) &amp; between(latitude, box[&quot;ymin&quot;], box[&quot;ymax&quot;])) }) %&gt;% bind_rows() %&gt;% distinct() %&gt;% st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;)) %&gt;% st_set_crs(4326) %&gt;% st_intersection(hills) # get simplified data and drop geometry dataLocs &lt;- mutate(dataLocs, spatialKeep = T) %&gt;% bind_cols(., as_tibble(st_coordinates(dataLocs))) %&gt;% st_drop_geometry() # bind to data and then filter data &lt;- data %&gt;% map(function(x) { left_join(x, dataLocs, by = c(&quot;longitude&quot; = &quot;X&quot;, &quot;latitude&quot; = &quot;Y&quot;)) %&gt;% filter(spatialKeep == T) %&gt;% select(-Id, -spatialKeep) }) Save temporary data created so far. # save a temp data file save(data, file = &quot;data/01_data_temp.rdata&quot;) 2.5 Handle presence data Further pre-processing is required in the case of many checklists where species abundance is often unknown and an X is denoted in such cases. Here, we convert all X notations to a 1, suggesting a presence (as we are not concerned with abundance data in this analysis). We also removed those checklists where the duration in minutes is either not recorded or listed as zero. Lastly, we added an sampling effort based filter following (Johnston et al. 2019), wherein we considered only those checklists with duration in minutes is less than 300 and distance in kilometres traveled is less than 5km. Lastly, we excluded those group checklists where the number of observers was greater than 10. Upon receiving comments from reviewers, we restrict all our checklists between December 1st and May 31st (non-rainy months)and checklists recorded between 5am and 7pm. # in the first set, replace X, for presences, with 1 data[[1]] &lt;- data[[1]] %&gt;% mutate(observation_count = ifelse(observation_count == &quot;X&quot;, &quot;1&quot;, observation_count )) # remove records where duration is 0 data &lt;- map(data, function(x) filter(x, duration_minutes &gt; 0)) # group data by site and sampling event identifier # then, summarise relevant variables as the sum dataGrouped &lt;- map(data, function(x) { x %&gt;% group_by(sampling_event_identifier) %&gt;% summarise_at( vars( duration_minutes, effort_distance_km, effort_area_ha ), list(sum.no.na) ) }) # bind rows combining data frames, and filter dataGrouped &lt;- bind_rows(dataGrouped) %&gt;% filter( duration_minutes &lt;= 300, effort_distance_km &lt;= 5, effort_area_ha &lt;= 500 ) # get data identifiers, such as sampling identifier etc dataConstants &lt;- data %&gt;% bind_rows() %&gt;% select( sampling_event_identifier, time_observations_started, locality, locality_type, locality_id, observer_id, observation_date, scientific_name, observation_count, protocol_type, number_observers, longitude, latitude ) # join the summarised data with the identifiers, # using sampling_event_identifier as the key dataGrouped &lt;- left_join(dataGrouped, dataConstants, by = &quot;sampling_event_identifier&quot; ) # remove checklists or seis with more than 10 obervers count(dataGrouped, number_observers &gt; 10) # count how many have 10+ obs dataGrouped &lt;- filter(dataGrouped, number_observers &lt;= 10) # keep only checklists between 5AM and 7PM dataGrouped &lt;- filter(dataGrouped, time_observations_started &gt;= &quot;05:00:00&quot; &amp; time_observations_started &lt;= &quot;19:00:00&quot;) # keep only checklists between December 1st and May 31st dataGrouped &lt;- filter(dataGrouped, month(observation_date) %in% c(1, 2, 3, 4, 5, 12)) 2.6 Add decimal time We added a column where time is denoted in decimal hours since midnight. # assign present or not, and get time in decimal hours since midnight library(lubridate) time_to_decimal &lt;- function(x) { x &lt;- hms(x, quiet = TRUE) hour(x) + minute(x) / 60 + second(x) / 3600 } # will cause issues if using time obs started as a linear effect and not quadratic dataGrouped &lt;- mutate(dataGrouped, pres_abs = observation_count &gt;= 1, decimalTime = time_to_decimal(time_observations_started) ) # check class of dataGrouped, make sure not sf assertthat::assert_that(!&quot;sf&quot; %in% class(dataGrouped)) The above data is saved to a file. # save a temp data file save(dataGrouped, file = &quot;data/01_data_prelim_processing.Rdata&quot;) References "],["preparing-environmental-predictors.html", "Section 3 Preparing Environmental Predictors 3.1 Prepare libraries 3.2 Prepare spatial extent 3.3 Prepare terrain rasters 3.4 Prepare Bioclim 4a and Bioclim 15 3.5 Prepare CHELSA rasters 3.6 Resample landcover from 10m to 1km 3.7 Resample other rasters to 1km 3.8 Temperature and rainfall in relation to elevation 3.9 Land cover type in relation to elevation 3.10 Main Text Figure 2", " Section 3 Preparing Environmental Predictors In this script, we processed climatic and landscape predictors for occupancy modeling. All climatic data was obtained from https://chelsa-climate.org/bioclim/ All landscape data was derived from a high resolution land cover map (Roy et al 2015). This map provides sufficient classes to achieve a high land cover resolution. The goal here is to resample all rasters so that they have the same resolution of 1km cells. We also tested for spatial autocorrelation among climatic predictors, which can be found in Section 4 of the Supplementary Material. 3.1 Prepare libraries We load some common libraries for raster processing and define a custom mode function. # load libs library(raster) library(stringi) library(glue) library(gdalUtils) library(purrr) library(dplyr) library(tidyr) library(tibble) # for plotting library(viridis) library(tmap) library(scales) library(ggplot2) library(patchwork) # prep mode function to aggregate funcMode &lt;- function(x, na.rm = T) { ux &lt;- unique(x) ux[which.max(tabulate(match(x, ux)))] } # a basic test assertthat::assert_that(funcMode(c(2, 2, 2, 2, 3, 3, 3, 4)) == as.character(2), msg = &quot;problem in the mode function&quot; ) # works # get ci func ci &lt;- function(x) { qnorm(0.975) * sd(x, na.rm = T) / sqrt(length(x)) } 3.2 Prepare spatial extent We prepare a 30km buffer around the boundary of the study area. This buffer will be used to mask the landscape rasters.The buffer procedure is done on data transformed to the UTM 43N CRS to avoid distortions. # load hills library(sf) hills &lt;- st_read(&quot;data/spatial/hillsShapefile/Nil_Ana_Pal.shp&quot;) hills &lt;- st_transform(hills, 32643) buffer &lt;- st_buffer(hills, 3e4) %&gt;% st_transform(4326) 3.3 Prepare terrain rasters We prepare the elevation data which is an SRTM raster layer, and derive the slope and aspect from it after cropping it to the extent of the study site buffer. Please download the latest version of the SRTM raster layer from https://www.worldclim.org/data/worldclim21.html # load elevation and crop to hills size, then mask by hills alt &lt;- raster(&quot;data/spatial/Elevation/alt&quot;) # this layer is not added to github as a result of its large size and can be downloaded from the above link alt.hills &lt;- crop(alt, as(buffer, &quot;Spatial&quot;)) rm(alt) gc() # get slope and aspect slopeData &lt;- terrain(x = alt.hills, opt = c(&quot;slope&quot;, &quot;aspect&quot;)) elevData &lt;- raster::stack(alt.hills, slopeData) rm(alt.hills) gc() 3.4 Prepare Bioclim 4a and Bioclim 15 # list chelsa files chelsaFiles &lt;- list.files(&quot;data/chelsa/&quot;, recursive = TRUE, pattern = &quot;tif&quot;) # load Bio 1 and 12 bio_1 &lt;- raster(&quot;data/chelsa/envicloud/chelsa/chelsa_V1/climatologies/bio/CHELSA_bio10_01.tif&quot;) bio_12 &lt;- raster(&quot;data/chelsa/envicloud/chelsa/chelsa_V1/climatologies/bio/CHELSA_bio10_12.tif&quot;) 3.5 Prepare CHELSA rasters CHELSA rasters can be downloaded using the get_chelsa.sh shell script, which is a wget command pointing to the envidatS3.txt file. 3.5.1 Prepare BIOCLIM 4a and 15 We prepare the CHELSA rasters for seasonality in temperature (Bio 4a) and seasonality in precipitation (Bio 15) in the same way, reading them in, cropping them to the study site buffer extent, and handling the temperature layer values which we divide by 10. The CHELSA rasters can be downloaded from https://chelsa-climate.org/bioclim/ # list chelsa files # the chelsa data can be downloaded from the aforementioned link. They haven&#39;t been uploaded to github as a result of its large size. chelsaFiles &lt;- list.files(&quot;data/chelsa/&quot;, full.names = TRUE, recursive = TRUE, pattern = &quot;bio10&quot; ) # gather chelsa rasters chelsaData &lt;- purrr::map(chelsaFiles, function(chr) { a &lt;- raster(chr) crs(a) &lt;- crs(elevData) a &lt;- crop(a, as(buffer, &quot;Spatial&quot;)) return(a) }) # divide temperature by 10 chelsaData[[1]] &lt;- chelsaData[[1]] / 10 # stack chelsa data chelsaData &lt;- raster::stack(chelsaData) 3.5.2 Prepare BIOCLIM 4a if (file.exists(&quot;data/chelsa/CHELSA_bio10_4a.tif&quot;)) { message(&quot;Bio 4a already exists, will be overwritten&quot;) } Bioclim 4a, the coefficient of variation temperature seasonality is calculated as \\[Bio\\ 4a = \\frac{SD\\{ Tkavg_1, \\ldots Tkavg_{12} \\}}{(Bio\\ 1 + 273.15)} \\times 100\\] where \\(Tkavg_i = (Tkmin_i + Tkmax_i) / 2\\) Here, we use only the months of December and Jan  May for winter temperature variation. # list rasters by pattern patterns &lt;- c(&quot;tmin&quot;, &quot;tmax&quot;) # list the filepaths tkAvg &lt;- map(patterns, function(pattern) { # list the paths files &lt;- list.files( path = &quot;data/chelsa&quot;, full.names = TRUE, recursive = TRUE, pattern = pattern ) }) # print crs elev data for sanity check --- basic WGS84 crs(elevData) # now run over the paths and read as rasters and crop by buffer tkAvg &lt;- map(tkAvg, function(paths) { # going over the file paths, read them in as rasters, convert CRS and crop tempData &lt;- map(paths, function(path) { # read in a &lt;- raster(path) # assign crs crs(a) &lt;- crs(elevData) # crop by buffer, will throw error if CRS doesn&#39;t match a &lt;- crop(a, as(buffer, &quot;Spatial&quot;)) # return a a }) # convert each to kelvin, first dividing by 10 to get celsius tempData &lt;- map(tempData, function(tmpRaster) { tmpRaster &lt;- (tmpRaster / 10) + 273.15 }) }) # assign names names(tkAvg) &lt;- patterns # go over the tmin and tmax and get the average monthly temp tkAvg &lt;- map2(tkAvg[[&quot;tmin&quot;]], tkAvg[[&quot;tmax&quot;]], function(tmin, tmax) { # return the mean of the corresponding tmin and tmax # still in kelvin calc(stack(tmin, tmax), fun = mean) }) # calculate Bio 4a bio_4a &lt;- (calc(stack(tkAvg), fun = sd) / (chelsaData[[1]] + 273.15)) * 100 names(bio_4a) &lt;- &quot;CHELSA_bio10_4a&quot; # save bio_4a writeRaster(bio_4a, filename = &quot;data/chelsa/CHELSA_bio10_4a.tif&quot;, overwrite = T) 3.5.3 Prepare Bioclim 15 if (file.exists(&quot;data/chelsa/CHELSA_bio10_15.tif&quot;)) { message(&quot;Bio 15 already exists, will be overwritten&quot;) } Bioclim 15, the coefficient of variation precipitation (in our area, rainfall) seasonality is calculated as \\[Bio\\ 15 = \\frac{SD\\{ PPT_1, \\ldots PPT_{12} \\}}{1 + (Bio\\ 12 / 12)} \\times 100\\] where \\(PPT_i\\) is the monthly precipitation. Here, we use only the months of December and Jan  May for winter rainfall variation. # list rasters by pattern pattern &lt;- &quot;prec&quot; # list the filepaths pptTotal &lt;- list.files( path = &quot;data/chelsa&quot;, full.names = TRUE, recursive = TRUE, pattern = pattern ) # print crs elev data for sanity check --- basic WGS84 crs(elevData) # now run over the paths and read as rasters and crop by buffer pptTotal &lt;- map(pptTotal, function(path) { a &lt;- raster(path) # assign crs crs(a) &lt;- crs(elevData) # crop by buffer, will throw error if CRS doesn&#39;t match a &lt;- crop(a, as(buffer, &quot;Spatial&quot;)) # return a a }) # calculate Bio 4a bio_15 &lt;- (calc(stack(pptTotal), fun = sd) / (1 + (chelsaData[[2]] / 12))) * 100 names(bio_15) &lt;- &quot;CHELSA_bio10_15&quot; # save bio_4a writeRaster(bio_15, filename = &quot;data/chelsa/CHELSA_bio10_15.tif&quot;, overwrite = T) 3.5.4 Stack terrain and climate We stack the terrain and climatic rasters. # stack rasters for efficient reprojection later env_data &lt;- stack(elevData, bio_4a, bio_15) 3.6 Resample landcover from 10m to 1km We read in a land cover classified image and resample that using the mode function to a 1km resolution. Please note that the resampling process need not be carried out as it has been done already and the resampled raster can be loaded with the subsequent code chunk. # read in landcover raster location landcover &lt;- &quot;data/landUseClassification/landcover_roy_2015/&quot; # read in and crop landcover &lt;- raster(landcover) buffer_utm &lt;- st_transform(buffer, 32643) landcover &lt;- crop( landcover, as( buffer_utm, &quot;Spatial&quot; ) ) # read reclassification matrix reclassification_matrix &lt;- read.csv(&quot;data/landUseClassification/LandCover_ReclassifyMatrix_2015.csv&quot;) reclassification_matrix &lt;- as.matrix(reclassification_matrix[, c(&quot;V1&quot;, &quot;To&quot;)]) # reclassify landcover_reclassified &lt;- reclassify( x = landcover, rcl = reclassification_matrix ) # write to file writeRaster(landcover_reclassified, filename = &quot;data/landUseClassification/landcover_roy_2015_reclassified.tif&quot;, overwrite = TRUE ) # check reclassification plot(landcover_reclassified) # get extent e &lt;- bbox(raster(landcover)) # init resolution res_init &lt;- res(raster(landcover)) # res to transform to 1000m res_final &lt;- res_init * (1000 / res_init) # use gdalutils gdalwarp for resampling transform # to 1km from 10m gdalUtils::gdalwarp( srcfile = &quot;data/landUseClassification/landcover_roy_2015_reclassified.tif&quot;, dstfile = &quot;data/landUseClassification/lc_01000m.tif&quot;, tr = c(res_final), r = &quot;mode&quot;, te = c(e) ) We compare the frequency of landcover classes between the original raster and the resampled 1km raster to be certain that the resampling has not resulted in drastic misrepresentation of the frequency of any landcover type. This comparison is made using the figure below. 3.7 Resample other rasters to 1km We now resample all other rasters to a resolution of 1km. 3.7.1 Read in resampled landcover Here, we read in the 1km landcover raster and set 0 to NA. lc_data &lt;- raster(&quot;data/landUseClassification/lc_01000m.tif&quot;) lc_data[lc_data == 0] &lt;- NA 3.7.2 Reproject environmental data using landcover as a template # resample to the corresponding landcover data env_data_resamp &lt;- projectRaster( from = env_data, to = lc_data, crs = crs(lc_data), res = res(lc_data) ) # export as raster stack land_stack &lt;- stack(env_data_resamp, lc_data) # get names land_names &lt;- glue(&#39;data/spatial/landscape_resamp{c(&quot;01&quot;)}_km.tif&#39;) # write to file writeRaster(land_stack, filename = as.character(land_names), overwrite = TRUE) 3.8 Temperature and rainfall in relation to elevation 3.8.1 Load resampled environmental rasters # read landscape prepare for plotting landscape &lt;- stack(&quot;data/spatial/landscape_resamp01_km.tif&quot;) # get proper names elev_names &lt;- c(&quot;elev&quot;, &quot;slope&quot;, &quot;aspect&quot;) chelsa_names &lt;- c(&quot;bio_4a&quot;, &quot;bio_15&quot;) names(landscape) &lt;- glue(&#39;{c(elev_names, chelsa_names, &quot;landcover&quot;)}&#39;) # make duplicate stack land_data &lt;- landscape[[c(&quot;elev&quot;, chelsa_names)]] # convert to list land_data &lt;- as.list(land_data) # map get values over the stack land_data &lt;- purrr::map(land_data, getValues) names(land_data) &lt;- c(&quot;elev&quot;, chelsa_names) # conver to dataframe and round to 100m land_data &lt;- bind_cols(land_data) land_data &lt;- drop_na(land_data) %&gt;% mutate(elev_round = plyr::round_any(elev, 200)) %&gt;% dplyr::select(-elev) %&gt;% pivot_longer( cols = contains(&quot;bio&quot;), names_to = &quot;clim_var&quot; ) %&gt;% group_by(elev_round, clim_var) %&gt;% summarise_all(.funs = list(~ mean(.), ~ ci(.))) Figure code is hidden in versions rendered as HTML or PDF. 3.9 Land cover type in relation to elevation # get data from landscape rasters lc_elev &lt;- tibble( elev = getValues(landscape[[&quot;elev&quot;]]), landcover = getValues(landscape[[&quot;landcover&quot;]]) ) # process data for proportions lc_elev &lt;- lc_elev %&gt;% filter(!is.na(landcover), !is.na(elev)) %&gt;% mutate(elev = plyr::round_any(elev, 100)) %&gt;% count(elev, landcover) %&gt;% group_by(elev) %&gt;% mutate(prop = n / sum(n)) # fill out lc elev lc_elev_canon &lt;- crossing( elev = unique(lc_elev$elev), landcover = unique(lc_elev$landcover) ) # bind with lcelev lc_elev &lt;- full_join(lc_elev, lc_elev_canon) # convert NA to zero lc_elev &lt;- replace_na(lc_elev, replace = list(n = 0, prop = 0)) Figure code is hidden in versions rendered as HTML and PDF. 3.10 Main Text Figure 2 (a) Seasonality in precipitation and seasonality in temperature as a function of elevation is shown here. The coefficient of variation was calculated. (b) The proportion of land cover types varied across the study area as shown in this panel. "],["preparing-observer-expertise-scores.html", "Section 4 Preparing Observer Expertise Scores 4.1 Prepare libraries 4.2 Prepare data 4.3 Spatially explicit filter on checklists 4.4 Prepare species of interest 4.5 Prepare checklists for observer score 4.6 Get landcover 4.7 Filter checklist data 4.8 Model observer expertise", " Section 4 Preparing Observer Expertise Scores Differences in local avifaunal expertise among citizen scientists can lead to biased species detection when compared with data collected by a consistent set of trained observers (van Strien, van Swaay, and Termaat 2013). Including observer expertise as a detection covariate in occupancy models using eBird data can help account for this variation (Johnston et al. 2018). Observer-specific expertise in local avifauna was calculated following (Kelling et al. 2015) as the normalized predicted number of species reported by an observer after 60 minutes of sampling across the most common land cover type within the study area. This score was calculated by examining checklists from anonymized observers across the study area. We modified Kelling et al. (2015) formulation by including only observations of the 82 species of interest in our calculations. An observer with a higher number of species of interest reported within 60 minutes would have a higher observer-specific expertise score, with respect to the study area. Plots with respect to how observer expertise varied over time (2013-2020) for the list of species considered in this study (across the study area alone) can be accessed in Section 7 of the Supplementary Material. 4.1 Prepare libraries # load libs library(data.table) library(readxl) library(magrittr) library(stringr) library(dplyr) library(tidyr) library(auk) # get decimal time function library(lubridate) time_to_decimal &lt;- function(x) { x &lt;- lubridate::hms(x, quiet = TRUE) lubridate::hour(x) + lubridate::minute(x) / 60 + lubridate::second(x) / 3600 } 4.2 Prepare data Here, we go through the data preparation process again because we might want to assess observer expertise over a larger area than the study site. # Read in shapefile of study area to subset by bounding box library(sf) wg &lt;- st_read(&quot;data/spatial/hillsShapefile/Nil_Ana_Pal.shp&quot;) %&gt;% st_transform(32643) # set file paths for auk functions f_in_ebd &lt;- file.path(&quot;data/01_ebird-filtered-EBD-westernGhats.txt&quot;) f_in_sampling &lt;- file.path(&quot;data/01_ebird-filtered-sampling-westernGhats.txt&quot;) # run filters using auk packages ebd_filters &lt;- auk_ebd(f_in_ebd, f_in_sampling) %&gt;% auk_country(country = &quot;IN&quot;) %&gt;% auk_state(c(&quot;IN-KL&quot;, &quot;IN-TN&quot;, &quot;IN-KA&quot;)) %&gt;% # Restricting geography to TamilNadu, Kerala &amp; Karnataka auk_date(c(&quot;2013-01-01&quot;, &quot;2020-12-31&quot;)) %&gt;% auk_complete() # check filters ebd_filters # specify output location and perform filter f_out_ebd &lt;- &quot;data/ebird_for_expertise.txt&quot; f_out_sampling &lt;- &quot;data/ebird_sampling_for_expertise.txt&quot; ebd_filtered &lt;- auk_filter(ebd_filters, file = f_out_ebd, file_sampling = f_out_sampling, overwrite = TRUE ) Load in the filtered data and columns of interest. ## Process filtered data # read in the data ebd &lt;- fread(f_out_ebd) names &lt;- names(ebd) %&gt;% stringr::str_to_lower() %&gt;% stringr::str_replace_all(&quot; &quot;, &quot;_&quot;) setnames(ebd, names) # choose columns of interest columnsOfInterest &lt;- c( &quot;global_unique_identifier&quot;, &quot;scientific_name&quot;, &quot;observation_count&quot;, &quot;locality&quot;, &quot;locality_id&quot;, &quot;locality_type&quot;, &quot;latitude&quot;, &quot;longitude&quot;, &quot;observation_date&quot;, &quot;time_observations_started&quot;, &quot;observer_id&quot;, &quot;sampling_event_identifier&quot;, &quot;protocol_type&quot;, &quot;duration_minutes&quot;, &quot;effort_distance_km&quot;, &quot;effort_area_ha&quot;, &quot;number_observers&quot;, &quot;all_species_reported&quot;, &quot;reviewed&quot; ) ebd &lt;- setDF(ebd) %&gt;% as_tibble() %&gt;% dplyr::select(one_of(columnsOfInterest)) setDT(ebd) # remove checklists or seis with more than 10 obervers ebd &lt;- filter(ebd, number_observers &lt;= 10) # keep only checklists between 5AM and 7PM ebd &lt;- filter(ebd, time_observations_started &gt;= &quot;05:00:00&quot; &amp; time_observations_started &lt;= &quot;19:00:00&quot;) # keep only checklists between December 1st and May 31st ebd &lt;- filter(ebd, month(observation_date) %in% c(1, 2, 3, 4, 5, 12)) 4.3 Spatially explicit filter on checklists # get checklist locations ebd_locs &lt;- ebd[, .(longitude, latitude)] ebd_locs &lt;- setDF(ebd_locs) %&gt;% distinct() ebd_locs &lt;- st_as_sf(ebd_locs, coords = c(&quot;longitude&quot;, &quot;latitude&quot;) ) %&gt;% `st_crs&lt;-`(4326) %&gt;% bind_cols(as_tibble(st_coordinates(.))) %&gt;% st_transform(32643) %&gt;% mutate(id = 1:nrow(.)) # check whether to include to_keep &lt;- unlist(st_contains(wg, ebd_locs)) # filter locs ebd_locs &lt;- filter(ebd_locs, id %in% to_keep) %&gt;% bind_cols(as_tibble(st_coordinates(st_as_sf(.)))) %&gt;% st_drop_geometry() names(ebd_locs) &lt;- c(&quot;longitudeWGS&quot;, &quot;latitudeWGS&quot;, &quot;id&quot;, &quot;longitudeUTM&quot;, &quot;latitudeUTM&quot;) ebd &lt;- ebd[longitude %in% ebd_locs$longitudeWGS &amp; latitude %in% ebd_locs$latitudeWGS, ] 4.4 Prepare species of interest # read in species list specieslist &lt;- read.csv(&quot;data/species_list.csv&quot;) # set species of interest soi &lt;- specieslist$scientific_name ebdSpSum &lt;- ebd[, .( nSp = .N, totSoiSeen = length(intersect(scientific_name, soi)) ), by = list(sampling_event_identifier) ] # write to file and link with checklist id later fwrite(ebdSpSum, file = &quot;data/03_data-nspp-per-chk.csv&quot;) 4.5 Prepare checklists for observer score # 1. add new columns of decimal time and julian date ebd[, `:=`( decimalTime = time_to_decimal(time_observations_started), julianDate = yday(as.POSIXct(observation_date)) )] ebdEffChk &lt;- setDF(ebd) %&gt;% mutate(year = year(observation_date)) %&gt;% distinct( sampling_event_identifier, observer_id, year, duration_minutes, effort_distance_km, effort_area_ha, longitude, latitude, locality, locality_id, decimalTime, julianDate, number_observers ) %&gt;% # drop rows with NAs in cols used in the model tidyr::drop_na( sampling_event_identifier, observer_id, duration_minutes, decimalTime, julianDate ) %&gt;% # drop years below 2013 filter(year &gt;= 2013) # 3. join to covariates and remove large groups (&gt; 10) ebdChkSummary &lt;- inner_join(ebdEffChk, ebdSpSum) # remove ebird data rm(ebd) gc() 4.6 Get landcover Read in land cover type data resampled at 1km resolution. # read in 1km landcover and set 0 to NA library(raster) landcover &lt;- raster::raster(&quot;data/landUseClassification/lc_01000m.tif&quot;) landcover[landcover == 0] &lt;- NA # get locs in utm coords locs &lt;- distinct( ebdChkSummary, sampling_event_identifier, longitude, latitude, locality, locality_id ) locs &lt;- st_as_sf(locs, coords = c(&quot;longitude&quot;, &quot;latitude&quot;)) %&gt;% `st_crs&lt;-`(4326) %&gt;% st_transform(32643) %&gt;% st_coordinates() # get for unique points landcoverVec &lt;- raster::extract( x = landcover, y = locs ) # assign to df and overwrite setDT(ebdChkSummary)[, landcover := landcoverVec] 4.7 Filter checklist data # change names for easy handling setnames(ebdChkSummary, c( &quot;locality&quot;, &quot;locality_id&quot;, &quot;latitude&quot;, &quot;longitude&quot;, &quot;observer&quot;, &quot;sei&quot;, &quot;duration&quot;, &quot;distance&quot;, &quot;area&quot;, &quot;nObs&quot;, &quot;decimalTime&quot;, &quot;julianDate&quot;, &quot;year&quot;, &quot;nSp&quot;, &quot;nSoi&quot;, &quot;landcover&quot; )) # count data points per observer obscount &lt;- count(ebdChkSummary, observer) %&gt;% filter(n &gt;= 10) # make factor variables and remove obs not in obscount # also remove 0 durations ebdChkSummary &lt;- ebdChkSummary %&gt;% mutate( distance = ifelse(is.na(distance), 0, distance), duration = if_else(is.na(duration), 0.0, as.double(duration)) ) %&gt;% filter( observer %in% obscount$observer, duration &gt; 0, duration &lt;= 300, nSoi &gt;= 0, distance &lt;= 5, !is.na(nSoi) ) %&gt;% mutate( landcover = as.factor(landcover), observer = as.factor(observer) ) %&gt;% drop_na(landcover) # editing julian date to model it in a linear fashion unique(ebdChkSummary$julianDate) ebdChkSummary &lt;- ebdChkSummary %&gt;% mutate( newjulianDate = case_when( julianDate &gt;= 334 &amp; julianDate &lt;= 365 ~ (julianDate - 333), julianDate &gt;= 1 &amp; julianDate &lt;= 152 ~ (julianDate + 31) ) ) %&gt;% drop_na(newjulianDate) # save to file for later reuse fwrite(ebdChkSummary, file = &quot;data/03_data-covars-perChklist.csv&quot;) 4.8 Model observer expertise Our observer expertise model aims to include the random intercept effect of observer identity, with a random slope effect of duration. This models the different rate of species accumulation by different observers, as well as their different starting points. # uses either a subset or all data library(lmerTest) # here we specify a glmm with random effects for observer # time is considered a fixed log predictor and a random slope modObsExp &lt;- glmer(nSoi ~ sqrt(duration) + landcover + sqrt(decimalTime) + I((sqrt(decimalTime))^2) + log(newjulianDate) + I((log(newjulianDate)^2)) + (1 | observer) + (0 + duration | observer), data = ebdChkSummary, family = &quot;poisson&quot; ) # make dir if absent if (!dir.exists(&quot;data/modOutput&quot;)) { dir.create(&quot;data/modOutput&quot;) } # write model output to text file { writeLines(R.utils::captureOutput(list(Sys.time(), summary(modObsExp))), con = &quot;data/modOutput/03_model-output-expertise.txt&quot; ) } # make df with means observer &lt;- unique(ebdChkSummary$observer) # predict at 60 mins on the most common landcover (deciduous forests) dfPredict &lt;- ebdChkSummary %&gt;% summarise_at(vars(duration, decimalTime, newjulianDate), list(~ mean(.))) %&gt;% mutate(duration = 60, landcover = as.factor(2)) %&gt;% tidyr::crossing(observer) # run predict from model on it dfPredict &lt;- mutate(dfPredict, score = predict(modObsExp, newdata = dfPredict, type = &quot;response&quot;, allow.new.levels = TRUE ) ) %&gt;% mutate(score = scales::rescale(score)) fwrite(dfPredict %&gt;% dplyr::select(observer, score), file = &quot;data/03_data-obsExpertise-score.csv&quot; ) References "],["examining-spatial-sampling-bias.html", "Section 5 Examining Spatial Sampling Bias 5.1 Prepare libraries 5.2 Read checklist data 5.3 Prepare Main Text Figure 3 5.4 Main Text Figure 3", " Section 5 Examining Spatial Sampling Bias The goal of this section is to show how far each checklist location is from the nearest road, and how far each site is from its nearest neighbour. This follows finding the pairwise distance between a large number of unique checklist locations to a vast number of roads, as well as to each other. 5.1 Prepare libraries # load libraries # for data library(sf) library(rnaturalearth) library(dplyr) library(readr) library(purrr) # for plotting library(scales) library(ggplot2) library(ggspatial) library(scico) # round any function round_any &lt;- function(x, accuracy = 20000) { round(x / accuracy) * accuracy } # ci function ci &lt;- function(x) { qnorm(0.975) * sd(x, na.rm = TRUE) / sqrt(length(x)) } 5.2 Read checklist data Read in checklist data with distance to nearest neighbouring site, and the distance to the nearest road. # read from local file chkCovars &lt;- read_csv(&quot;data/03_data-covars-perChklist.csv&quot;) 5.2.1 Spatially explicit filter on checklists We filter the checklists by the boundary of the study area. This is not the extent. chkCovars &lt;- st_as_sf(chkCovars, coords = c(&quot;longitude&quot;, &quot;latitude&quot;)) %&gt;% `st_crs&lt;-`(4326) %&gt;% st_transform(32643) # read wg wg &lt;- st_read(&quot;data/spatial/hillsShapefile/Nil_Ana_Pal.shp&quot;) %&gt;% st_transform(32643) # get bounding box bbox &lt;- st_bbox(wg) # spatial subset chkCovars &lt;- chkCovars %&gt;% mutate(id = 1:nrow(.)) %&gt;% filter(id %in% unlist(st_contains(wg, chkCovars))) 5.2.2 Get background land for plotting # add land land &lt;- ne_countries( scale = 50, type = &quot;countries&quot;, continent = &quot;asia&quot;, country = &quot;india&quot;, returnclass = c(&quot;sf&quot;) ) %&gt;% st_transform(32643) # add roads data roads &lt;- st_read(&quot;data/spatial/roads_studysite_2019/roads_studysite_2019.shp&quot;) %&gt;% st_transform(32643) 5.3 Prepare Main Text Figure 3 5.3.1 Prepare histogram of distance to roads Figure code is hidden in versions rendered as HTML or PDF. 5.3.2 Table: Distance to roads # write the mean and ci95 to file chkCovars %&gt;% st_drop_geometry() %&gt;% select(dist_road, nnb) %&gt;% tidyr::pivot_longer( cols = c(&quot;dist_road&quot;, &quot;nnb&quot;), names_to = &quot;variable&quot; ) %&gt;% group_by(variable) %&gt;% summarise_at( vars(value), list(~ mean(.), ~ sd(.), ~ min(.), ~ max(.)) ) %&gt;% write_csv(&quot;data/results/distance_roads_sites.csv&quot;) 5.3.3 Distance to nearest neighbouring site # get unique locations from checklists locs_unique &lt;- cbind( st_drop_geometry(chkCovars), st_coordinates(chkCovars) ) %&gt;% as_tibble() locs_unique &lt;- distinct(locs_unique, X, Y, .keep_all = T) Figure code is hidden in versions rendered as HTML and PDF. 5.3.4 Spatial distribution of distances to neighbours 5.4 Main Text Figure 3 # get locations points &lt;- chkCovars %&gt;% bind_cols(as_tibble(st_coordinates(.))) %&gt;% st_drop_geometry() %&gt;% mutate(X = round_any(X, 2500), Y = round_any(Y, 2500)) # count points points &lt;- count(points, X, Y) Figure code is hidden in versions rendered as HTML and PDF. Spatial sampling bias of eBird observations across the Nilgiri and the Anamalai hills. A large proportion of localities/sites were next to roads and were on average only ~300m from another locality/site. Each cell here is 2.5km x 2.5km "],["checking-temporal-sampling-frequency.html", "Section 6 Checking Temporal Sampling Frequency 6.1 Load libraries 6.2 Load checklist data 6.3 Get time differences per grid cell 6.4 Time Since Previous Checklist 6.5 Checklists per Month", " Section 6 Checking Temporal Sampling Frequency How often are checklists recorded in each grid cell? 6.1 Load libraries # load libraries library(tidyverse) library(sf) # for plotting library(ggplot2) library(scico) library(ggthemes) library(ggspatial) 6.2 Load checklist data Here we load filtered checklist data and convert to UTM 43N coordinates. # load checklist data load(&quot;data/01_ebird_data_prelim_processing.rdata&quot;) # get checklists data &lt;- distinct( dataGrouped, sampling_event_identifier, observation_date, longitude, latitude ) # remove old data rm(dataGrouped) # transform to UTM 43N data &lt;- st_as_sf(data, coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4326) data &lt;- st_transform(data, crs = 32643) # get coordinates and bind to data data &lt;- cbind( st_drop_geometry(data), st_coordinates(data) ) # bin to 1000m data &lt;- mutate(data, X = plyr::round_any(X, 2500), Y = plyr::round_any(Y, 2500) ) 6.3 Get time differences per grid cell # get time differences in days data &lt;- mutate(data, observation_date = as.POSIXct(observation_date)) data &lt;- nest(data, data = c(&quot;sampling_event_identifier&quot;, &quot;observation_date&quot;)) # map over data data &lt;- mutate(data, lag_metrics = lapply(data, function(df) { df &lt;- arrange(df, observation_date) lag &lt;- as.numeric(diff(df$observation_date, na.rm = TRUE) / (24 * 3600)) data &lt;- tibble( mean_lag = mean(lag, na.rm = TRUE), median_lag = median(lag, na.rm = TRUE), sd_lag = sd(lag, na.rm = TRUE), n_chk = nrow(df) ) data }) ) # unnest lag metrics data_lag &lt;- select(data, -data) data_lag &lt;- unnest(data_lag, cols = &quot;lag_metrics&quot;) # set the mean and median to infinity if nchk is 1 data_lag &lt;- mutate(data_lag, mean_lag = ifelse(n_chk == 1, Inf, mean_lag), median_lag = ifelse(n_chk == 1, Inf, median_lag), sd_lag = ifelse(n_chk == 1, Inf, sd_lag) ) # set all 0 to 1 data_lag &lt;- mutate(data_lag, mean_lag = mean_lag + 1, median_lag = median_lag + 1 ) # melt data by tile # data_lag = pivot_longer(data_lag, cols = c(&quot;mean_lag&quot;, &quot;median_lag&quot;, &quot;sd_lag&quot;)) 6.4 Time Since Previous Checklist 6.4.1 Get aux data # hills data wg &lt;- st_read(&quot;data/spatial/hillsShapefile/Nil_Ana_Pal.shp&quot;) %&gt;% st_transform(32643) roads &lt;- st_read(&quot;data/spatial/roads_studysite_2019/roads_studysite_2019.shp&quot;) %&gt;% st_transform(32643) # add land library(rnaturalearth) land &lt;- ne_countries( scale = 50, type = &quot;countries&quot;, continent = &quot;asia&quot;, country = &quot;india&quot;, returnclass = c(&quot;sf&quot;) ) %&gt;% st_transform(32643) bbox &lt;- st_bbox(wg) 6.4.2 Histogram of lags # get lags data &lt;- mutate(data, lag_hist = lapply(data, function(df) { df &lt;- arrange(df, observation_date) lag &lt;- as.numeric(diff(df$observation_date, na.rm = TRUE) / (24 * 3600)) data &lt;- tibble( lag = lag + 1, index = seq(lag) ) data }) ) # unnest lags data_hist &lt;- select(data, X, Y, lag_hist) %&gt;% unnest(cols = &quot;lag_hist&quot;) fig_hist_lag &lt;- ggplot(data_hist) + geom_histogram( aes(x = lag), bins = 10, size = 0.2, fill = &quot;steelblue&quot; ) + scale_x_log10() + scale_y_continuous( labels = scales::label_number( scale = 0.001, accuracy = 1, suffix = &quot;K&quot; ), limits = c(0, 10.5e3) ) + theme_few() + theme( plot.background = element_rect(fill = &quot;white&quot;, colour = 1), panel.background = element_blank(), panel.border = element_blank(), axis.line = element_blank(), axis.text.y = element_text( angle = 90, hjust = 0.5 ) ) + labs( x = &quot;Days since prev. checklist&quot;, y = &quot;# checklists&quot; ) # make plot fig_lag_spatial &lt;- ggplot(data_lag) + geom_sf(data = land, fill = &quot;grey90&quot;, col = NA) + geom_sf( data = wg, fill = NA ) + annotation_custom( grob = fig_hist_lag %&gt;% ggplotGrob(), xmin = bbox[&quot;xmax&quot;] - (bbox[&quot;xmax&quot;] - bbox[&quot;xmin&quot;]) / 2.5, xmax = bbox[&quot;xmax&quot;], ymin = bbox[&quot;ymax&quot;] - (bbox[&quot;ymax&quot;] - bbox[&quot;ymin&quot;]) / 3, ymax = bbox[&quot;ymax&quot;] ) + geom_tile( aes(X, Y, fill = mean_lag ), col = &quot;grey90&quot; ) + geom_sf( data = roads, size = 0.2, col = &quot;indianred&quot; ) + scale_fill_scico( palette = &quot;oslo&quot;, direction = 1, trans = &quot;log10&quot;, na.value = alpha(&quot;gold&quot;) ) + annotation_north_arrow( location = &quot;br&quot;, which_north = &quot;true&quot;, pad_x = unit(0.1, &quot;in&quot;), pad_y = unit(0.5, &quot;in&quot;), style = north_arrow_fancy_orienteering ) + annotation_scale(location = &quot;br&quot;, width_hint = 0.4, text_cex = 1) + coord_sf( expand = FALSE, xlim = bbox[c(&quot;xmin&quot;, &quot;xmax&quot;)], ylim = bbox[c(&quot;ymin&quot;, &quot;ymax&quot;)] ) + ggthemes::theme_few() + theme( legend.position = c(0.9, 0.53), legend.background = element_blank(), legend.key = element_rect(fill = &quot;grey90&quot;), axis.text.y = element_text( angle = 90, hjust = 0.5 ), axis.title = element_blank(), panel.background = element_rect(fill = &quot;lightblue&quot;) ) + labs( fill = &quot;Mean days\\nb/w checklists&quot; ) # save figure ggsave( filename = &quot;figs/fig_lag_spatial.png&quot;, width = 6.5, height = 6.5, device = png() ) 6.5 Checklists per Month # get two week period by date data &lt;- select(data, X, Y, data) # unnest data &lt;- unnest(data, cols = &quot;data&quot;) # get fortnight library(lubridate) data &lt;- mutate(data, week = week(observation_date), week = plyr::round_any(week, 2), year = year(observation_date), month = month(observation_date) ) # count checklists per fortnight data_count &lt;- count(data, month, year) ggplot(data_count) + geom_boxplot( aes( x = factor(month), y = n ), fill = &quot;steelblue&quot; ) + scale_y_log10( limits = c(10, NA) ) + theme_classic() + theme( axis.text.y = element_text( angle = 90, hjust = 0.5 ) ) + labs( x = &quot;Month&quot;, y = &quot;# checklists&quot; ) ggsave(filename = &quot;figs/fig_chk_per_month.png&quot;) "],["adding-covariates-to-checklist-data.html", "Section 7 Adding Covariates to Checklist Data 7.1 Prepare libraries and data 7.2 Spatial subsampling 7.3 Temporal subsampling 7.4 Add checklist calibration index 7.5 Add climatic and landscape covariates 7.6 Spatial buffers around selected checklists 7.7 Spatial buffer-wide covariates", " Section 7 Adding Covariates to Checklist Data In this section, we prepare a final list of covariates, after taking into account spatial sampling bias (examined in the previous section), temporal bias and observer expertise scores. 7.1 Prepare libraries and data # load libs for data library(dplyr) library(readr) library(stringr) library(purrr) library(glue) library(tidyr) # check for velox and install library(devtools) if (!&quot;velox&quot; %in% installed.packages()) { install_github(&quot;hunzikp/velox&quot;) } # load spatial library(raster) library(rgeos) library(velox) library(sf) # load saved data object load(&quot;data/01_data_prelim_processing.rdata&quot;) 7.2 Spatial subsampling Sampling bias can be introduced into citizen science due to the often ad-hoc nature of data collection (Sullivan et al. 2014). For eBird, this translates into checklists reported when convenient, rather than at regular or random points in time and space, leading to non-independence in the data if observations are spatio-temporally clustered (Johnston et al. 2019). Spatio-temporal autocorrelation in the data can be reduced by sub-sampling at an appropriate spatial resolution, and by avoiding temporal clustering. We estimated two simple measures of spatial clustering: the distance from each site to the nearest road (road data from OpenStreetMap; (OpenStreetMap contributors 2019)), and the nearest-neighbor distance for each site. Sites were strongly tied to roads (mean distance to road ± SD = 390.77 ± 859.15 m; range = 0.28 m  7.64 km) and were on average only 297 m away from another site (SD = 553 m; range = 0.14 m  12.85 km) (Figure 3). This analysis was done in the previous section. Here, to further reduce spatial autocorrelation, we divided the study area into a grid of 1km wide square cells and picked checklists from one site at random within each grid cell. Prior to running this analysis, we checked how many checklists/data would be retained given a particular value of distance to account for spatial independence. This analysis can be accessed in Section 8 of the Supplementary Material. We show that over 80% of checklists are retained with a distance cutoff of 1km. In addition, a number of thinning approaches were tested to determine which method retained the highest proportion of points, while accounting for sampling effort (time and distance). This analysis can be accessed in Section 9 of the Supplementary Material. # grid based spatial thinning gridsize &lt;- 1000 # grid size in metres effort_distance_max &lt;- 1000 # removing checklists with this distance # make grids across the study site hills &lt;- st_read(&quot;data/spatial/hillsShapefile/Nil_Ana_Pal.shp&quot;) %&gt;% st_transform(32643) grid &lt;- st_make_grid(hills, cellsize = gridsize) # split data by species data_spatial_thin &lt;- split(x = dataGrouped, f = dataGrouped$scientific_name) # spatial thinning on each species retains # site with maximum visits per grid cell data_spatial_thin &lt;- map(data_spatial_thin, function(df) { # count visits per locality df &lt;- group_by(df, locality) %&gt;% mutate(tot_effort = length(sampling_event_identifier)) %&gt;% ungroup() # remove sites with distances above spatial independence df &lt;- df %&gt;% filter(effort_distance_km &lt;= effort_distance_max) %&gt;% st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;)) %&gt;% `st_crs&lt;-`(4326) # transform to regional UTM 43N and add id df &lt;- df %&gt;% st_transform(32643) %&gt;% mutate(coordId = 1:nrow(.)) %&gt;% bind_cols(as_tibble(st_coordinates(.))) # whcih cell has which coords grid_overlap &lt;- st_contains(grid, df) %&gt;% unclass() %&gt;% discard(.p = is_empty) # count length of grid overlap list # this is the number of cells with points in them sampled_cells &lt;- length(grid_overlap) # make tibble grid_overlap &lt;- tibble( uid_cell = seq(length(grid_overlap)), # the uid_cell is specific to this sp. coordId = grid_overlap ) # unnest grid_overlap &lt;- unnest(grid_overlap, cols = &quot;coordId&quot;) # join grid cell overlap with coordinate data df &lt;- left_join(df, grid_overlap, by = &quot;coordId&quot; ) %&gt;% st_drop_geometry() # for each uid_cell, select coord where effort is max points_max &lt;- df %&gt;% group_by(uid_cell) %&gt;% filter(tot_effort == max(tot_effort)) %&gt;% # there may be multiple rows with max effort, select first filter(row_number() == 1) # check for number of samples assertthat::assert_that( assertthat::are_equal(sampled_cells, nrow(points_max), msg = &quot;spatial thinning error: more samples than\\\\ sampled cells&quot; ) ) # check that there is one sample per cell assertthat::assert_that( assertthat::are_equal( max(count(points_max, uid_cell)$n), 1 ) ) points_max }) # remove old data rm(dataGrouped) 7.3 Temporal subsampling Additionally, from each selected site, we randomly selected a maximum of 10 checklists, which reduced temporal autocorrelation. # subsample data for random 10 observations dataSubsample &lt;- map(data_spatial_thin, function(df) { df &lt;- ungroup(df) df_to_locality &lt;- split(x = df, f = df$locality) df_samples &lt;- map_if( .x = df_to_locality, .p = function(x) { nrow(x) &gt; 10 }, .f = function(x) sample_n(x, 10, replace = FALSE) ) bind_rows(df_samples) }) # bind all rows for data frame dataSubsample &lt;- bind_rows(dataSubsample) # remove previous data rm(data_spatial_thin) 7.4 Add checklist calibration index Load the CCI computed in the previous section. The CCI was the lone observers expertise score for single-observer checklists, and the highest expertise score among observers for group checklists. # read in obs score and extract numbers expertiseScore &lt;- read_csv(&quot;data/03_data-obsExpertise-score.csv&quot;) %&gt;% mutate(numObserver = str_extract(observer, &quot;\\\\d+&quot;)) %&gt;% dplyr::select(-observer) # group seis consist of multiple observers # in this case, seis need to have the highest expertise observer score # as the associated covariate # get unique observers per sei dataSeiScore &lt;- distinct( dataSubsample, sampling_event_identifier, observer_id ) %&gt;% # make list column of observers mutate(observers = str_split(observer_id, &quot;,&quot;)) %&gt;% unnest(cols = c(observers)) %&gt;% # add numeric observer id mutate(numObserver = str_extract(observers, &quot;\\\\d+&quot;)) %&gt;% # now get distinct sei and observer id numeric distinct(sampling_event_identifier, numObserver) # now add expertise score to sei dataSeiScore &lt;- left_join(dataSeiScore, expertiseScore, by = &quot;numObserver&quot; ) %&gt;% # get max expertise score per sei group_by(sampling_event_identifier) %&gt;% summarise(expertise = max(score)) # add to dataCovar dataSubsample &lt;- left_join(dataSubsample, dataSeiScore, by = &quot;sampling_event_identifier&quot; ) # remove data without expertise score dataSubsample &lt;- filter(dataSubsample, !is.na(expertise)) 7.5 Add climatic and landscape covariates Reload climate and land cover predictors prepared previously. # list landscape covariate stacks landscape_files &lt;- &quot;data/spatial/landscape_resamp01_km.tif&quot; # read in as stacks landscape_data &lt;- stack(landscape_files) # get proper names elev_names &lt;- c(&quot;elev&quot;, &quot;slope&quot;, &quot;aspect&quot;) chelsa_names &lt;- c(&quot;bio15a&quot;, &quot;bio4a&quot;) names(landscape_data) &lt;- as.character(glue(&#39;{c(elev_names, chelsa_names, &quot;landcover&quot;)}&#39;)) 7.6 Spatial buffers around selected checklists Every checklist on eBird is associated with a latitude and longitude. However, the coordinates entered by an observer may not accurately depict the location at which a species was detected. This can occur for two reasons: first, traveling checklists are often associated with a single location along the route travelled by observers; and second, checklist locations could be assigned to a hotspot  a location that is marked by eBird as being frequented by multiple observers. In many cases, an observation might be assigned to a hotspot even though the observation was not made at the precise location of the hotspot (J. 2017). Johnston et al., (2019) showed that a large proportion of observations occurred within a 3km grid, even for those checklists up to 5km in length. Hence to adjust for spatial precision, we considered a minimum radius of 2.5km around each unique locality when sampling environmental covariate values. # assign neighbourhood radius in m sample_radius &lt;- 2.5 * 1e3 # get distinct points and make buffer ebird_buff &lt;- dataSubsample %&gt;% ungroup() %&gt;% distinct(X, Y) %&gt;% mutate(id = 1:nrow(.)) %&gt;% crossing(sample_radius) %&gt;% arrange(id) %&gt;% group_by(sample_radius) %&gt;% nest() %&gt;% ungroup() # convert to spatial features ebird_buff &lt;- mutate(ebird_buff, data = map2( data, sample_radius, function(df, rd) { df_sf &lt;- st_as_sf(df, coords = c(&quot;X&quot;, &quot;Y&quot;), crs = 32643) %&gt;% # add long lat bind_cols(as_tibble(st_coordinates(.))) %&gt;% # rename(longitude = X, latitude = Y) %&gt;% # # transform to modis projection # st_transform(crs = 32643) %&gt;% # buffer to create neighborhood around each point st_buffer(dist = rd) } ) ) 7.7 Spatial buffer-wide covariates 7.7.1 Mean climatic covariates All climatic covariates are sampled by considering the mean values within a 2.5km radius as discussed above and prefixed am_. # get area mean for all preds except landcover, which is the last one env_area_mean &lt;- purrr::map(ebird_buff$data, function(df) { stk &lt;- landscape_data[[-dim(landscape_data)[3]]] # removing landcover here velstk &lt;- velox(stk) dextr &lt;- velstk$extract( sp = df, df = TRUE, fun = function(x) mean(x, na.rm = T) ) # assign names for joining names(dextr) &lt;- c(&quot;id&quot;, names(stk)) return(as_tibble(dextr)) }) # join to buffer data ebird_buff &lt;- ebird_buff %&gt;% mutate(data = map2(data, env_area_mean, inner_join, by = &quot;id&quot;)) 7.7.2 Proportions of land cover type All land cover covariates were sampled by considering the proportion of each land cover type within a 2.5km radius. # get the last element of each stack from the list # this is the landcover at that resolution lc_area_prop &lt;- purrr::map(ebird_buff$data, function(df) { lc &lt;- landscape_data[[dim(landscape_data)[3]]] # accessing landcover here lc_velox &lt;- velox(lc) lc_vals &lt;- lc_velox$extract(sp = df, df = TRUE) names(lc_vals) &lt;- c(&quot;id&quot;, &quot;lc&quot;) # get landcover proportions lc_prop &lt;- count(lc_vals, id, lc) %&gt;% group_by(id) %&gt;% mutate( lc = glue(&#39;lc_{str_pad(lc, 2, pad = &quot;0&quot;)}&#39;), prop = n / sum(n) ) %&gt;% dplyr::select(-n) %&gt;% tidyr::pivot_wider( names_from = lc, values_from = prop, values_fill = list(prop = 0) ) %&gt;% ungroup() return(lc_prop) }) # join to data ebird_buff &lt;- ebird_buff %&gt;% mutate(data = map2(data, lc_area_prop, inner_join, by = &quot;id&quot;)) 7.7.3 Link environmental covariates to checklists # duplicate scale data data_at_scale &lt;- ebird_buff # join the full data to landscape samples at each scale data_at_scale$data &lt;- map(data_at_scale$data, function(df) { df &lt;- st_drop_geometry(df) df &lt;- inner_join(dataSubsample, df, by = c(&quot;X&quot;, &quot;Y&quot;)) return(df) }) Save data to file. # write to file pmap(data_at_scale, function(sample_radius, data) { write_csv(data, path = glue(&#39;data/04_data-covars-{str_pad(sample_radius/1e3, 2, pad = &quot;0&quot;)}km.csv&#39;)) message(glue(&#39;export done: data/04_data-covars-{str_pad(sample_radius/1e3, 2, pad = &quot;0&quot;)}km.csv&#39;)) }) References "],["modelling-species-occupancy.html", "Section 8 Modelling Species Occupancy 8.1 Load dataframe and scale covariates 8.2 Running a null model 8.3 Identifying covariates necessary to model the detection process 8.4 Land Cover and Climate 8.5 Goodness-of-fit tests", " Section 8 Modelling Species Occupancy 8.0.1 Load necessary libraries # Load libraries library(auk) library(lubridate) library(sf) library(unmarked) library(raster) library(ebirdst) library(MuMIn) library(AICcmodavg) library(fields) library(tidyverse) library(doParallel) library(snow) library(openxlsx) library(data.table) library(dplyr) library(ecodist) # Source necessary functions source(&quot;R/fun_screen_cor.R&quot;) source(&quot;R/fun_model_estimate_collection.r&quot;) 8.1 Load dataframe and scale covariates Here, we load the required dataframe that contains 10 random visits to a site and environmental covariates that were prepared at a spatial scale of 2.5 sq.km. We also scaled all covariates (mean around 0 and standard deviation of 1). Next, we ensured that only Traveling and Stationary checklists were considered. Even though stationary counts have no distance traveled, we defaulted all stationary accounts to an effective distance of 100m, which we consider the average maximum detection radius for most bird species in our area. Following this, we excluded predictors with a Pearson coefficient \\(&gt;\\) 0.5. Please note that species-specific plots of probabilities of occupancy as a function of environmental data can be accessed in Section 10 of the Supplementary Material. # Load in the prepared dataframe that contains 10 random visits to each site dat &lt;- fread(&quot;data/04_data-covars-2.5km.csv&quot;, header = T) setDF(dat) head(dat) # Some more pre-processing to get the right data structures # Ensuring that only Traveling and Stationary checklists were considered names(dat) dat &lt;- dat %&gt;% filter(protocol_type %in% c(&quot;Traveling&quot;, &quot;Stationary&quot;)) # We take all stationary counts and give them a distance of 100 m (so 0.1 km), # as that&#39;s approximately the max normal hearing distance for people doing point # counts. dat &lt;- dat %&gt;% mutate(effort_distance_km = replace( effort_distance_km, which(effort_distance_km == 0 &amp; protocol_type == &quot;Stationary&quot;), 0.1 )) # Converting time observations started to numeric and adding it as a new column # This new column will be minute_observations_started dat &lt;- dat %&gt;% mutate( min_obs_started = strtoi( as.difftime( time_observations_started, format = &quot;%H:%M:%S&quot;, units = &quot;mins&quot; ) ) ) # Adding the julian date to the dataframe dat &lt;- dat %&gt;% mutate(julian_date = lubridate::yday(dat$observation_date)) # recode julian date to model it as a linear predictor dat &lt;- dat %&gt;% mutate( newjulianDate = case_when( (julian_date &gt;= 334 &amp; julian_date) &lt;= 365 ~ (julian_date - 333), (julian_date &gt;= 1 &amp; julian_date) &lt;= 152 ~ (julian_date + 31) ) ) %&gt;% drop_na(newjulianDate) # recode time observations started to model it as a linear predictor dat &lt;- dat %&gt;% mutate( newmin_obs_started = case_when( min_obs_started &gt;= 300 &amp; min_obs_started &lt;= 720 ~ abs(min_obs_started - 720), min_obs_started &gt;= 720 &amp; min_obs_started &lt;= 1140 ~ abs(720 - min_obs_started) ) ) %&gt;% drop_na(newmin_obs_started) # Removing other unnecessary columns from the dataframe and creating a clean one without the rest names(dat) dat &lt;- dat[, -c(1, 4, 5, 16, 18, 21, 23, 25, 26, 36, 37)] # Rename column names: names(dat) &lt;- c( &quot;duration_minutes&quot;, &quot;effort_distance_km&quot;, &quot;locality&quot;, &quot;locality_type&quot;, &quot;locality_id&quot;, &quot;observer_id&quot;, &quot;observation_date&quot;, &quot;scientific_name&quot;, &quot;observation_count&quot;, &quot;protocol_type&quot;, &quot;number_observers&quot;, &quot;pres_abs&quot;, &quot;tot_effort&quot;, &quot;longitude&quot;, &quot;latitude&quot;, &quot;expertise&quot;, &quot;elev&quot;, &quot;bio15a.y&quot;, &quot;bio4a.y&quot;, &quot;lc_01.y&quot;, &quot;lc_02.y&quot;, &quot;lc_05.y&quot;, &quot;lc_04.y&quot;, &quot;lc_09.y&quot;, &quot;lc_07.y&quot;, &quot;lc_03.y&quot;, &quot;julian_date&quot;, &quot;min_obs_started&quot; ) dat.1 &lt;- dat %&gt;% mutate( year = year(observation_date), pres_abs = as.integer(pres_abs) ) # occupancy modeling requires an integer response # Scaling detection and occupancy covariates dat.scaled &lt;- dat.1 dat.scaled[, c(1, 2, 11, 16:28)] &lt;- scale(dat.scaled[, c(1, 2, 11, 16:28)]) # Scaling and standardizing detection and site-level covariates fwrite(dat.scaled, file = &quot;data/05_scaled-covars-2.5km.csv&quot;) # Reload the scaled covariate data dat.scaled &lt;- fread(&quot;data/05_scaled-covars-2.5km.csv&quot;, header = T) setDF(dat.scaled) head(dat.scaled) # Ensure observation_date column is in the right format dat.scaled$observation_date &lt;- format( as.Date( dat.scaled$observation_date, &quot;%m/%d/%Y&quot; ), &quot;%Y-%m-%d&quot; ) # Testing for correlations before running further analyses # Majority are uncorrelated since we decided to keep climatic and land cover predictors and removed elevation. source(&quot;R/fun_screen_cor.R&quot;) names(dat.scaled) screen.cor(dat.scaled[, c(1, 2, 11, 16:28)], threshold = 0.3) 8.2 Running a null model # All null models are stored in lists below all_null &lt;- list() # Add a progress bar for the loop pb &lt;- txtProgressBar( min = 0, max = length(unique(dat.scaled$scientific_name)), style = 3 ) # text based bar for (i in 1:length(unique(dat.scaled$scientific_name))) { data &lt;- dat.scaled %&gt;% filter(dat.scaled$scientific_name == unique(dat.scaled$scientific_name)[i]) # Preparing data for the unmarked model occ &lt;- filter_repeat_visits(data, min_obs = 1, max_obs = 10, annual_closure = FALSE, n_days = 3000, # 7 years is considered a period of closure date_var = &quot;observation_date&quot;, site_vars = c(&quot;locality_id&quot;) ) obs_covs &lt;- c( &quot;min_obs_started&quot;, &quot;duration_minutes&quot;, &quot;effort_distance_km&quot;, &quot;number_observers&quot;, &quot;expertise&quot;, &quot;julian_date&quot; ) # format for unmarked occ_wide &lt;- format_unmarked_occu(occ, site_id = &quot;site&quot;, response = &quot;pres_abs&quot;, site_covs = c( &quot;locality_id&quot;, &quot;lc_01.y&quot;, &quot;lc_02.y&quot;, &quot;lc_05.y&quot;, &quot;lc_04.y&quot;, &quot;lc_09.y&quot;, &quot;lc_07.y&quot;, &quot;lc_03.y&quot;, &quot;bio15a.y&quot;, &quot;bio4a.y&quot; ), obs_covs = obs_covs ) # Convert this dataframe of observations into an unmarked object to start fitting occupancy models occ_um &lt;- formatWide(occ_wide, type = &quot;unmarkedFrameOccu&quot;) # Set up the model all_null[[i]] &lt;- occu(~1 ~ 1, data = occ_um) names(all_null)[i] &lt;- unique(dat.scaled$scientific_name)[i] setTxtProgressBar(pb, i) } close(pb) # Store all the model outputs for each species capture.output(all_null, file = &quot;data/results/null_models.csv&quot;) 8.3 Identifying covariates necessary to model the detection process Here, we use the unmarked package in R (Fiske and Chandler 2011) to identify detection level covariates that are important for each species. We use AIC criteria to select top models (Burnham, Anderson, and Huyvaert 2011). # All models are stored in lists below det_dred &lt;- list() # Subsetting those models whose deltaAIC&lt;2 (Burnham et al., 2011) top_det &lt;- list() # Getting model averaged coefficients and relative importance scores det_avg &lt;- list() det_imp &lt;- list() # Getting model estimates det_modelEst &lt;- list() # Add a progress bar for the loop pb &lt;- txtProgressBar( min = 0, max = length(unique(dat.scaled$scientific_name)), style = 3 ) # text based bar for (i in 1:length(unique(dat.scaled$scientific_name))) { data &lt;- dat.scaled %&gt;% filter(dat.scaled$scientific_name == unique(dat.scaled$scientific_name)[i]) # Preparing data for the unmarked model occ &lt;- filter_repeat_visits(data, min_obs = 1, max_obs = 10, annual_closure = FALSE, n_days = 3000, # 7 years is considered a period of closure date_var = &quot;observation_date&quot;, site_vars = c(&quot;locality_id&quot;) ) obs_covs &lt;- c( &quot;min_obs_started&quot;, &quot;duration_minutes&quot;, &quot;effort_distance_km&quot;, &quot;number_observers&quot;, &quot;expertise&quot;, &quot;julian_date&quot; ) # format for unmarked occ_wide &lt;- format_unmarked_occu(occ, site_id = &quot;site&quot;, response = &quot;pres_abs&quot;, site_covs = c( &quot;locality_id&quot;, &quot;lc_01.y&quot;, &quot;lc_02.y&quot;, &quot;lc_05.y&quot;, &quot;lc_04.y&quot;, &quot;lc_09.y&quot;, &quot;lc_07.y&quot;, &quot;lc_03.y&quot;, &quot;bio15a.y&quot;, &quot;bio4a.y&quot; ), obs_covs = obs_covs ) # Convert this dataframe of observations into an unmarked object to start fitting occupancy models occ_um &lt;- formatWide(occ_wide, type = &quot;unmarkedFrameOccu&quot;) # Fit a global model with all detection level covariates global_mod &lt;- occu(~ min_obs_started + julian_date + duration_minutes + effort_distance_km + number_observers + expertise ~ 1, data = occ_um) # Set up the cluster clusterType &lt;- if (length(find.package(&quot;snow&quot;, quiet = TRUE))) &quot;SOCK&quot; else &quot;PSOCK&quot; clust &lt;- try(makeCluster(getOption(&quot;cl.cores&quot;, 6), type = clusterType)) clusterEvalQ(clust, library(unmarked)) clusterExport(clust, &quot;occ_um&quot;) det_dred[[i]] &lt;- pdredge(global_mod, clust) names(det_dred)[i] &lt;- unique(dat.scaled$scientific_name)[i] # Get the top models, which we&#39;ll define as those with deltaAICc &lt; 2 top_det[[i]] &lt;- get.models(det_dred[[i]], subset = delta &lt; 2, cluster = clust) names(top_det)[i] &lt;- unique(dat.scaled$scientific_name)[i] # Obtaining model averaged coefficients if (length(top_det[[i]]) &gt; 1) { a &lt;- model.avg(top_det[[i]], fit = TRUE) det_avg[[i]] &lt;- as.data.frame(a$coefficients) names(det_avg)[i] &lt;- unique(dat.scaled$scientific_name)[i] det_modelEst[[i]] &lt;- data.frame( Coefficient = coefTable(a, full = T)[, 1], SE = coefTable(a, full = T)[, 2], lowerCI = confint(a)[, 1], upperCI = confint(a)[, 2], z_value = (summary(a)$coefmat.full)[, 3], Pr_z = (summary(a)$coefmat.full)[, 4] ) names(det_modelEst)[i] &lt;- unique(dat.scaled$scientific_name)[i] det_imp[[i]] &lt;- as.data.frame(MuMIn::importance(a)) names(det_imp)[i] &lt;- unique(dat.scaled$scientific_name)[i] } else { det_avg[[i]] &lt;- as.data.frame(unmarked::coef(top_det[[i]][[1]])) names(det_avg)[i] &lt;- unique(dat.scaled$scientific_name)[i] lowDet &lt;- data.frame(lowerCI = confint(top_det[[i]][[1]], type = &quot;det&quot;)[, 1]) upDet &lt;- data.frame(upperCI = confint(top_det[[i]][[1]], type = &quot;det&quot;)[, 2]) zDet &lt;- data.frame(summary(top_det[[i]][[1]])$det[, 3]) Pr_zDet &lt;- data.frame(summary(top_det[[i]][[1]])$det[, 4]) Coefficient &lt;- coefTable(top_det[[i]][[1]])[, 1] SE &lt;- coefTable(top_det[[i]][[1]])[, 2] det_modelEst[[i]] &lt;- data.frame( Coefficient = Coefficient[2:length(Coefficient)], SE = SE[2:length(SE)], lowerCI = lowDet, upperCI = upDet, z_value = zDet, Pr_z = Pr_zDet ) names(det_modelEst)[i] &lt;- unique(dat.scaled$scientific_name)[i] } setTxtProgressBar(pb, i) stopCluster(clust) } close(pb) ## Storing output from the above models in excel sheets # 1. Store all the model outputs for each species (variable: det_dred() - see above) write.xlsx(det_dred, file = &quot;data/results/det-dred.xlsx&quot;) # 2. Store all the model averaged outputs for each species and the relative importance score write.xlsx(det_avg, file = &quot;data/results/det-avg.xlsx&quot;, rowNames = T, colNames = T) write.xlsx(det_imp, file = &quot;data/results/det-imp.xlsx&quot;, rowNames = T, colNames = T) write.xlsx(det_modelEst, file = &quot;data/results/det-modelEst.xlsx&quot;, rowNames = T, colNames = T) # Note if you are unable to write to a file, use (for example) purrr::map(det_imp, ~ purrr::compact(.)) %&gt;% purrr::keep(~ length(.) != 0) 8.4 Land Cover and Climate Occupancy models estimate the probability of occurrence of a given species while controlling for the probability of detection and allow us to model the factors affecting occurrence and detection independently (Johnston et al. 2018; MacKenzie et al. 2002). The flexible eBird observation process contributes to the largest source of variation in the likelihood of detecting a particular species (Johnston et al. 2019); hence, we included seven covariates that influence the probability of detection for each checklist: ordinal day of year, duration of observation, distance travelled, protocol type, time observations started, number of observers and the checklist calibration index (CCI). Using a multi-model information-theoretic approach, we tested how strongly our occurrence data fit our candidate set of environmental covariates (Burnham and Anderson 2002). We fitted single-species occupancy models for each species, to simultaneously estimate a probability of detection (p) and a probability of occupancy (\\(\\psi\\)) (Fiske and Chandler 2011; MacKenzie et al. 2002). For each species, we fit 256 models, each with a unique combination of the eight (climate and land cover) occupancy covariates and all seven detection covariates (Appendix S5). Across the 256 models tested for each species, the model with highest support was determined using AICc scores. However, across the majority of the species, no single model had overwhelming support. Hence, for each species, we examined those models which had \\(\\Delta\\)AICc &lt; 2, as these top models were considered to explain a large proportion of the association between the species-specific probability of occupancy and environmental drivers (Burnham, Anderson, and Huyvaert 2011; Elsen et al. 2017). Using these restricted model sets for each species; we created a model-averaged coefficient estimate for each predictor and assessed its direction and significance (Barto 2020). We considered a predictor to be significantly associated with occupancy if the range of the 95% confidence interval around the model-averaged coefficient did not contain zero. Next, we obtained a measure of relative importance of climatic and landscape predictors by calculating cumulative variable importance scores. These scores were calculated by obtaining the sum of model weights (AIC weights) across all models (including the top models) for each predictor across all species. # All models are stored in lists below lc_clim &lt;- list() # Subsetting those models whose deltaAIC&lt;2 (Burnham et al., 2011) top_lc_clim &lt;- list() # Getting model averaged coefficients and relative importance scores lc_clim_avg &lt;- list() lc_clim_imp &lt;- list() # Storing Model estimates lc_clim_modelEst &lt;- list() # Add a progress bar for the loop pb &lt;- txtProgressBar(min = 0, max = length(unique(dat.scaled$scientific_name)), style = 3) # text based bar for (i in 1:length(unique(dat.scaled$scientific_name))) { data &lt;- dat.scaled %&gt;% filter(dat.scaled$scientific_name == unique(dat.scaled$scientific_name)[i]) # Preparing data for the unmarked model occ &lt;- filter_repeat_visits(data, min_obs = 1, max_obs = 10, annual_closure = FALSE, n_days = 3000, # 6 years is considered a period of closure date_var = &quot;observation_date&quot;, site_vars = c(&quot;locality_id&quot;) ) obs_covs &lt;- c( &quot;min_obs_started&quot;, &quot;duration_minutes&quot;, &quot;effort_distance_km&quot;, &quot;number_observers&quot;, &quot;expertise&quot;, &quot;julian_date&quot; ) # format for unmarked occ_wide &lt;- format_unmarked_occu(occ, site_id = &quot;site&quot;, response = &quot;pres_abs&quot;, site_covs = c( &quot;locality_id&quot;, &quot;lc_01.y&quot;, &quot;lc_02.y&quot;, &quot;lc_05.y&quot;, &quot;lc_04.y&quot;, &quot;lc_09.y&quot;, &quot;lc_07.y&quot;, &quot;lc_03.y&quot;, &quot;bio15a.y&quot;, &quot;bio4a.y&quot; ), obs_covs = obs_covs ) # Convert this dataframe of observations into an unmarked object to start fitting occupancy models occ_um &lt;- formatWide(occ_wide, type = &quot;unmarkedFrameOccu&quot;) model_lc_clim &lt;- occu(~ min_obs_started + julian_date + duration_minutes + effort_distance_km + number_observers + expertise ~ lc_01.y + lc_02.y + lc_05.y + lc_04.y + lc_09.y + lc_07.y + lc_03.y + bio15a.y + bio4a.y, data = occ_um) # Set up the cluster clusterType &lt;- if (length(find.package(&quot;snow&quot;, quiet = TRUE))) &quot;SOCK&quot; else &quot;PSOCK&quot; clust &lt;- try(makeCluster(getOption(&quot;cl.cores&quot;, 6), type = clusterType)) clusterEvalQ(clust, library(unmarked)) clusterExport(clust, &quot;occ_um&quot;) # Detection terms are fixed det_terms &lt;- c( &quot;p(duration_minutes)&quot;, &quot;p(effort_distance_km)&quot;, &quot;p(expertise)&quot;, &quot;p(julian_date)&quot;, &quot;p(min_obs_started)&quot;, &quot;p(number_observers)&quot; ) lc_clim[[i]] &lt;- pdredge(model_lc_clim, clust, fixed = det_terms) names(lc_clim)[i] &lt;- unique(dat.scaled$scientific_name)[i] # Identiying top subset of models based on deltaAIC scores being less than 2 (Burnham et al., 2011) top_lc_clim[[i]] &lt;- get.models(lc_clim[[i]], subset = delta &lt; 2, cluster = clust) names(top_lc_clim)[i] &lt;- unique(dat.scaled$scientific_name)[i] # Obtaining model averaged coefficients for both candidate model subsets if (length(top_lc_clim[[i]]) &gt; 1) { a &lt;- model.avg(top_lc_clim[[i]], fit = TRUE) lc_clim_avg[[i]] &lt;- as.data.frame(a$coefficients) names(lc_clim_avg)[i] &lt;- unique(dat.scaled$scientific_name)[i] lc_clim_modelEst[[i]] &lt;- data.frame( Coefficient = coefTable(a, full = T)[, 1], SE = coefTable(a, full = T)[, 2], lowerCI = confint(a)[, 1], upperCI = confint(a)[, 2], z_value = (summary(a)$coefmat.full)[, 3], Pr_z = (summary(a)$coefmat.full)[, 4] ) names(lc_clim_modelEst)[i] &lt;- unique(dat.scaled$scientific_name)[i] lc_clim_imp[[i]] &lt;- as.data.frame(MuMIn::importance(a)) names(lc_clim_imp)[i] &lt;- unique(dat.scaled$scientific_name)[i] } else { lc_clim_avg[[i]] &lt;- as.data.frame(unmarked::coef(top_lc_clim[[i]][[1]])) names(lc_clim_avg)[i] &lt;- unique(dat.scaled$scientific_name)[i] lowSt &lt;- data.frame(lowerCI = confint(top_lc_clim[[i]][[1]], type = &quot;state&quot;)[, 1]) lowDet &lt;- data.frame(lowerCI = confint(top_lc_clim[[i]][[1]], type = &quot;det&quot;)[, 1]) upSt &lt;- data.frame(upperCI = confint(top_lc_clim[[i]][[1]], type = &quot;state&quot;)[, 2]) upDet &lt;- data.frame(upperCI = confint(top_lc_clim[[i]][[1]], type = &quot;det&quot;)[, 2]) zSt &lt;- data.frame(z_value = summary(top_lc_clim[[i]][[1]])$state[, 3]) zDet &lt;- data.frame(z_value = summary(top_lc_clim[[i]][[1]])$det[, 3]) Pr_zSt &lt;- data.frame(Pr_z = summary(top_lc_clim[[i]][[1]])$state[, 4]) Pr_zDet &lt;- data.frame(Pr_z = summary(top_lc_clim[[i]][[1]])$det[, 4]) lc_clim_modelEst[[i]] &lt;- data.frame( Coefficient = coefTable(top_lc_clim[[i]][[1]])[, 1], SE = coefTable(top_lc_clim[[i]][[1]])[, 2], lowerCI = rbind(lowSt, lowDet), upperCI = rbind(upSt, upDet), z_value = rbind(zSt, zDet), Pr_z = rbind(Pr_zSt, Pr_zDet) ) names(lc_clim_modelEst)[i] &lt;- unique(dat.scaled$scientific_name)[i] } setTxtProgressBar(pb, i) stopCluster(clust) } close(pb) # 1. Store all the model outputs for each species (for both landcover and climate) write.xlsx(lc_clim, file = &quot;data/results/lc-clim.xlsx&quot;) # 2. Store all the model averaged outputs for each species and relative importance scores write.xlsx(lc_clim_avg, file = &quot;data/results/lc-clim-avg.xlsx&quot;, rowNames = T, colNames = T) write.xlsx(lc_clim_imp, file = &quot;data/results/lc-clim-imp.xlsx&quot;, rowNames = T, colNames = T) # 3. Store all model estimates write.xlsx(lc_clim_modelEst, file = &quot;data/results/lc-clim-modelEst.xlsx&quot;, rowNames = T, colNames = T) 8.5 Goodness-of-fit tests Adequate model fit was assessed using a chi-square goodness-of-fit test using 5000 parametric bootstrap simulations on a global model that included all occupancy and detection covariates (MacKenzie &amp; Bailey, 2004). goodness_of_fit &lt;- data.frame() # Add a progress bar for the loop pb &lt;- txtProgressBar(min = 0, max = length(unique(dat.scaled$scientific_name)), style = 3) # text based bar for (i in 1:length(unique(dat.scaled$scientific_name))) { data &lt;- dat.scaled %&gt;% filter(dat.scaled$scientific_name == unique(dat.scaled$scientific_name)[i]) # Preparing data for the unmarked model occ &lt;- filter_repeat_visits(data, min_obs = 1, max_obs = 10, annual_closure = FALSE, n_days = 3000, # 6 years is considered a period of closure date_var = &quot;observation_date&quot;, site_vars = c(&quot;locality_id&quot;) ) obs_covs &lt;- c( &quot;min_obs_started&quot;, &quot;duration_minutes&quot;, &quot;effort_distance_km&quot;, &quot;number_observers&quot;, &quot;protocol_type&quot;, &quot;expertise&quot;, &quot;julian_date&quot; ) # format for unmarked occ_wide &lt;- format_unmarked_occu(occ, site_id = &quot;site&quot;, response = &quot;pres_abs&quot;, site_covs = c( &quot;locality_id&quot;, &quot;lc_01.y&quot;, &quot;lc_02.y&quot;, &quot;lc_05.y&quot;, &quot;lc_04.y&quot;, &quot;lc_09.y&quot;, &quot;lc_07.y&quot;, &quot;lc_03.y&quot;, &quot;bio15a.y&quot;, &quot;bio4a.y&quot; ), obs_covs = obs_covs ) # Convert this dataframe of observations into an unmarked object to start fitting occupancy models occ_um &lt;- formatWide(occ_wide, type = &quot;unmarkedFrameOccu&quot;) model_lc_clim &lt;- occu(~ min_obs_started + julian_date + duration_minutes + effort_distance_km + number_observers + protocol_type + expertise ~ lc_01.y + lc_02.y + lc_05.y + lc_04.y + lc_09.y + lc_07.y + lc_03.y + bio15a.y + bio4a.y, data = occ_um) occ_gof &lt;- mb.gof.test(model_lc_clim, nsim = 5000, plot.hist = FALSE) p.value &lt;- occ_gof$p.value c.hat &lt;- occ_gof$c.hat.est scientific_name &lt;- unique(data$scientific_name) a &lt;- data.frame(scientific_name, p.value, c.hat) goodness_of_fit &lt;- rbind(a, goodness_of_fit) setTxtProgressBar(pb, i) } close(pb) write.csv(goodness_of_fit, &quot;data/results/05_goodness-of-fit-2.5km.csv&quot;) References "],["visualizing-occupancy-predictor-effects.html", "Section 9 Visualizing Occupancy Predictor Effects 9.1 Prepare libraries 9.2 Load species list 9.3 Show AIC weight importance 9.4 Prepare model coefficient data 9.5 Get predictor effects 9.6 Main Text Figure 4", " Section 9 Visualizing Occupancy Predictor Effects In this section, we will visualize the cumulative AIC weights and the magnitude and direction of species-specific probability of occupancy. To get cumulative AIC weights, we first obtained a measure of relative importance of climatic and landscape predictors by calculating cumulative variable importance scores. These scores were calculated by obtaining the sum of model weights (AIC weights) across all models (including the top models) for each predictor across all species. We then calculated the mean cumulative variable importance score and a standard deviation for each predictor (Burnham and Anderson 2002). 9.1 Prepare libraries # to load data library(readxl) # to handle data library(dplyr) library(readr) library(forcats) library(tidyr) library(purrr) library(stringr) # library(data.table) # to wrangle models source(&quot;R/fun_model_estimate_collection.r&quot;) source(&quot;R/fun_make_resp_data.r&quot;) # nice tables library(knitr) library(kableExtra) # plotting library(ggplot2) library(patchwork) source(&quot;R/fun_plot_interaction.r&quot;) 9.2 Load species list # list of species species &lt;- read_csv(&quot;data/post_analysis_species_list.csv&quot;) list_of_species &lt;- as.character(species$scientific_name) 9.3 Show AIC weight importance 9.3.1 Read in AIC weight data # which files to read file_names &lt;- c(&quot;data/results/lc-clim-imp.xlsx&quot;) # read in sheets by species model_imp &lt;- map(file_names, function(f) { md_list &lt;- map(list_of_species, function(sn) { # some sheets are not found tryCatch( { readxl::read_excel(f, sheet = sn) %&gt;% `colnames&lt;-`(c(&quot;predictor&quot;, &quot;AIC_weight&quot;)) %&gt;% filter(str_detect(predictor, &quot;psi&quot;)) %&gt;% mutate( predictor = stringr::str_extract(predictor, pattern = stringr::regex(&quot;\\\\((.*?)\\\\)&quot;) ), predictor = stringr::str_replace_all(predictor, &quot;[//(//)]&quot;, &quot;&quot;), predictor = stringr::str_remove(predictor, &quot;\\\\.y&quot;) ) }, error = function(e) { message(as.character(e)) } ) }) names(md_list) &lt;- list_of_species return(md_list) }) 9.3.2 Prepare cumulative AIC weight data # assign scale - minimum spatial scale at which the analysis was carried out to account for observer effort names(model_imp) &lt;- c(&quot;2.5km&quot;) model_imp &lt;- imap(model_imp, function(.x, .y) { .x &lt;- bind_rows(.x) .x$scale &lt;- .y return(.x) }) # bind rows model_imp &lt;- map(model_imp, bind_rows) %&gt;% bind_rows() # convert to numeric model_imp$AIC_weight &lt;- as.numeric(model_imp$AIC_weight) model_imp$scale &lt;- as.factor(model_imp$scale) levels(model_imp$scale) &lt;- c(&quot;2.5km&quot;) # Let&#39;s get a summary of cumulative variable importance model_imp &lt;- group_by(model_imp, predictor) %&gt;% summarise( mean_AIC = mean(AIC_weight), sd_AIC = sd(AIC_weight), min_AIC = min(AIC_weight), max_AIC = max(AIC_weight), med_AIC = median(AIC_weight) ) # write to file write_csv(model_imp, file = &quot;data/results/cumulative_AIC_weights.csv&quot; ) Read data back in. # read data and make factor model_imp &lt;- read_csv(&quot;data/results/cumulative_AIC_weights.csv&quot;) model_imp$predictor &lt;- as_factor(model_imp$predictor) # make nice names predictor_name &lt;- tibble( predictor = levels(model_imp$predictor), pred_name = c( &quot;Precipitation Seasonality (CV)&quot;, &quot;Temperature Seasonality (CV)&quot;, &quot;% Evergreen Forest&quot;, &quot;% Deciduous Forest&quot;, &quot;% Mixed/Degraded Forest&quot;, &quot;% Agriculture/Settlements&quot;, &quot;% Grassland&quot;, &quot;% Plantations&quot;, &quot;% Water Bodies&quot; ) ) # rename predictor model_imp &lt;- left_join(model_imp, predictor_name) Prepare figure for cumulative AIC weight. Figure code is hidden in versions rendered as HTML and PDF. fig_aic &lt;- ggplot(model_imp) + geom_pointrange(aes( x = reorder(predictor, mean_AIC), y = mean_AIC, ymin = mean_AIC - sd_AIC, ymax = mean_AIC + sd_AIC )) + geom_text(aes( x = predictor, y = 1.2, label = pred_name ), size = 3, angle = 0, hjust = 1, vjust = 2 ) + # scale_y_continuous(breaks = seq(45, 75, 10))+ scale_x_discrete(labels = NULL) + # scale_color_brewer(palette = &quot;RdBu&quot;, values = c(0.5, 1))+ coord_flip( ylim = c(0, 1.25) # ylim = c(45, 75) ) + theme_test() + theme(legend.position = &quot;none&quot;) + labs( x = &quot;Predictor&quot;, y = &quot;Cumulative AIC weight&quot; ) ggsave(fig_aic, filename = &quot;figs/fig_aic_weight.png&quot;, device = png(), dpi = 300, width = 79, height = 120, units = &quot;mm&quot; ) 9.4 Prepare model coefficient data For each species, we examined those models which had AICc &lt; 2, as these top models were considered to explain a large proportion of the association between the species-specific probability of occupancy and environmental drivers (Burnham, Anderson, and Huyvaert 2011; Elsen et al. 2017). Using these restricted model sets for each species; we created a model-averaged coefficient estimate for each predictor and assessed its direction and significance (Barto 2020). We considered a predictor to be significantly associated with occupancy if the range of the 95% confidence interval around the model-averaged coefficient did not contain zero. file_read &lt;- c(&quot;data/results/lc-clim-modelEst.xlsx&quot;) # read data as list column model_est &lt;- map(file_read, function(fr) { md_list &lt;- map(list_of_species, function(sn) { readxl::read_excel(fr, sheet = sn) }) names(md_list) &lt;- list_of_species return(md_list) }) # prepare model data scales &lt;- c(&quot;2.5km&quot;) model_data &lt;- tibble( scale = scales, scientific_name = list_of_species ) %&gt;% arrange(desc(scale)) # rename model data components and separate predictors names &lt;- c( &quot;predictor&quot;, &quot;coefficient&quot;, &quot;se&quot;, &quot;ci_lower&quot;, &quot;ci_higher&quot;, &quot;z_value&quot;, &quot;p_value&quot; ) # get data for plotting: model_est &lt;- map(model_est, function(l) { map(l, function(df) { colnames(df) &lt;- names df &lt;- separate_interaction_terms(df) df &lt;- make_response_data(df) return(df) }) }) # add names and scales model_est &lt;- map(model_est, function(l) { imap(l, function(.x, .y) { mutate(.x, scientific_name = .y) }) }) # add names to model estimates names(model_est) &lt;- scales model_est &lt;- imap(model_est, function(.x, .y) { bind_rows(.x) %&gt;% mutate(scale = .y) }) # remove modulators model_est &lt;- bind_rows(model_est) %&gt;% select(-matches(&quot;modulator&quot;)) # join data to species name model_data &lt;- model_data %&gt;% left_join(model_est) # Keep only those predictors whose p-values are significant: model_data &lt;- model_data %&gt;% filter(p_value &lt; 0.05) Export predictor effects. # get predictor effect data data_predictor_effect &lt;- distinct( model_data, scientific_name, se, predictor, coefficient ) # write to file write_csv(data_predictor_effect, file = &quot;data/results/data_predictor_effect.csv&quot; ) Export model data. model_data_to_file &lt;- model_data %&gt;% select( predictor, data, scientific_name, scale ) %&gt;% unnest(cols = &quot;data&quot;) # remove .y model_data_to_file &lt;- model_data_to_file %&gt;% mutate(predictor = str_remove(predictor, &quot;\\\\.y&quot;)) write_csv( model_data_to_file, &quot;data/results/data_occupancy_predictors.csv&quot; ) Read in data after clearing R session. # read from file model_data &lt;- read_csv(&quot;data/results/results-predictors-species-traits.csv&quot;) Fix predictor name. # remove .y from predictors model_data &lt;- model_data %&gt;% mutate_at(.vars = c(&quot;predictor&quot;), .funs = function(x) { stringr::str_remove(x, &quot;.y&quot;) }) 9.5 Get predictor effects # is the coeff positive? how many positive per scale per predictor per axis of split? # now splitting by habitat --- forest or open country data_predictor &lt;- mutate(model_data, direction = coefficient &gt; 0 ) %&gt;% rename(habitat = &quot;Open-country/Forest&quot;) %&gt;% count( predictor, habitat, direction ) %&gt;% mutate(mag = n * (if_else(direction, 1, -1))) # wrangle data to get nice bars data_predictor &lt;- data_predictor %&gt;% select(-n) %&gt;% drop_na(direction) %&gt;% mutate(direction = ifelse(direction, &quot;positive&quot;, &quot;negative&quot;)) %&gt;% pivot_wider(values_from = &quot;mag&quot;, names_from = &quot;direction&quot;) %&gt;% mutate_at( vars(positive, negative), ~ if_else(is.na(.), 0, .) ) data_predictor_long &lt;- data_predictor %&gt;% pivot_longer( cols = c(&quot;negative&quot;, &quot;positive&quot;), names_to = &quot;effect&quot;, values_to = &quot;magnitude&quot; ) # write write_csv(data_predictor_long, path = &quot;data/results/data_predictor_direction_nSpecies.csv&quot; ) Prepare data to determine the direction (positive or negative) of the effect of each predictor. How many species are affected in either direction? # join with predictor names and relative AIC data_predictor_long &lt;- left_join(data_predictor_long, model_imp) Prepare figure of the number of species affected in each direction. Figure code is hidden in versions rendered as HTML and PDF. 9.6 Main Text Figure 4 Figure code is hidden in versions rendered as HTML and PDF. (a) Cumulative AIC weights suggest that climatic predictors have higher relative importance when compared to landscape predictors. (b) The direction of association between species-specific probability of occupancy and climatic and landscape is shown here. While climatic predictors were both positively and negatively associated with the probability of occupancy for a number of species, human-associated land cover types were largely negatively associated with species-specific probability of occupancy. References "],["references.html", "Section 10 References", " Section 10 References "]]
