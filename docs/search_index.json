[
["index.html", "Influence of land cover and climate on the occupancy of avian distributions along a tropical montane gradient Section 1 Introduction 1.1 Attribution 1.2 Data access 1.3 Data processing", " Influence of land cover and climate on the occupancy of avian distributions along a tropical montane gradient Vijay Ramesh, Pratik R Gupte, and Morgan Tingley 2020-05-19 Section 1 Introduction This is the bookdown version of a project in preparation that models occupancy for birds in the Nilgiri hills. Methods and format are derived from Strimas-Mackey et al., the supplement to Johnston et al.Â (2019). 1.1 Attribution Please contact the following in case of interest in the project. Vijay Ramesh (lead author) PhD student, Columbia University Pratik Gupte (repo maintainer) PhD student, University of Groningen Morgan Tingley (PI) 1.2 Data access The data used in this work are available from eBird. 1.3 Data processing The data processing for this project is described in the following sections. Navigate through them using the links in the sidebar. "],
["preparing-ebird-data.html", "Section 2 Preparing eBird data 2.1 Prepare libraries and data sources 2.2 Filter data 2.3 Process filtered data 2.4 Spatial filter 2.5 Handle presence data 2.6 Add decimal time 2.7 Write data", " Section 2 Preparing eBird data 2.1 Prepare libraries and data sources # load libs library(tidyverse) library(readr) library(sf) library(auk) library(readxl) # custom sum function sum.no.na &lt;- function(x){sum(x, na.rm = T)} # set file paths for auk functions f_in_ebd &lt;- file.path(&quot;ebd_Filtered_Jun2019.txt&quot;) f_in_sampling &lt;- file.path(&quot;ebd_sampling_Filtered_Jun2019.txt&quot;) 2.2 Filter data # add species of interest specieslist &lt;- read_excel(&quot;data/V2_ListOfSpecies_ChosenForStudy.csv.xlsx&quot;) speciesOfInterest &lt;- specieslist$scientific_name # run filters using auk packages ebd_filters &lt;- auk_ebd(f_in_ebd, f_in_sampling) %&gt;% auk_species(speciesOfInterest) %&gt;% auk_country(country = &quot;IN&quot;) %&gt;% auk_state(c(&quot;IN-KL&quot;,&quot;IN-TN&quot;, &quot;IN-KA&quot;)) %&gt;% # Restricting geography to TamilNadu, Kerala &amp; Karnataka auk_date(c(&quot;2013-01-01&quot;, &quot;2018-12-31&quot;)) %&gt;% auk_complete() # check filters ebd_filters Below code need not be run if it has been filtered once already and the above path leads to the right dataset. NB: This is a computation heavy process, run with caution. # specify output location and perform filter f_out_ebd &lt;- &quot;data/eBirdDataWG_filtered.txt&quot; f_out_sampling &lt;- &quot;data/eBirdSamplingDataWG_filtered.txt&quot; ebd_filtered &lt;- auk_filter(ebd_filters, file = f_out_ebd, file_sampling = f_out_sampling, overwrite = TRUE) 2.3 Process filtered data # read in the data ebd &lt;- read_ebd(f_out_ebd) # fill zeroes zf &lt;- auk_zerofill(f_out_ebd, f_out_sampling) new_zf &lt;- collapse_zerofill(zf) # Creates a new zero-filled dataframe with a 0 marked for each checklist when the bird was not observed # choose columns of interest columnsOfInterest &lt;- c(&quot;checklist_id&quot;,&quot;scientific_name&quot;,&quot;observation_count&quot;,&quot;locality&quot;,&quot;locality_id&quot;,&quot;locality_type&quot;,&quot;latitude&quot;,&quot;longitude&quot;,&quot;observation_date&quot;,&quot;time_observations_started&quot;,&quot;observer_id&quot;,&quot;sampling_event_identifier&quot;,&quot;protocol_type&quot;,&quot;duration_minutes&quot;,&quot;effort_distance_km&quot;,&quot;effort_area_ha&quot;,&quot;number_observers&quot;,&quot;species_observed&quot;,&quot;reviewed&quot;) # make list of presence and absence data and choose cols of interest data &lt;- list(ebd, new_zf) %&gt;% map(function(x){ x %&gt;% select(one_of(columnsOfInterest)) }) # remove zerofills to save working memory rm(zf, new_zf); gc() # check presence and absence in absences df, remove essentially the presences df data[[2]] &lt;- data[[2]] %&gt;% filter(species_observed == F) 2.4 Spatial filter # load shapefiles of hill ranges library(sf) hills &lt;- st_read(&quot;data/spatial/hillsShapefile/Nil_Ana_Pal.shp&quot;) # write a prelim filter by bounding box box &lt;- st_bbox(hills) # get data spatial coordinates dataLocs &lt;- data %&gt;% map(function(x){ select(x, longitude, latitude) %&gt;% filter(between(longitude, box[&quot;xmin&quot;], box[&quot;xmax&quot;]) &amp; between(latitude, box[&quot;ymin&quot;], box[&quot;ymax&quot;]))}) %&gt;% bind_rows() %&gt;% distinct() %&gt;% st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;)) %&gt;% st_set_crs(4326) %&gt;% st_intersection(hills) # get simplified data and drop geometry dataLocs &lt;- mutate(dataLocs, spatialKeep = T) %&gt;% bind_cols(., as_tibble(st_coordinates(dataLocs))) %&gt;% st_drop_geometry() # bind to data and then filter data &lt;- data %&gt;% map(function(x){ left_join(x, dataLocs, by = c(&quot;longitude&quot; = &quot;X&quot;, &quot;latitude&quot; = &quot;Y&quot;)) %&gt;% filter(spatialKeep == T) %&gt;% select(-Id, -spatialKeep) }) # save a temp data file save(data, file = &quot;data_temp.rdata&quot;) 2.5 Handle presence data # in the first set, replace X, for presences, with 1 data[[1]] &lt;- data[[1]] %&gt;% mutate(observation_count = ifelse(observation_count == &quot;X&quot;, &quot;1&quot;, observation_count)) # remove records where duration is 0 data &lt;- map(data, function(x) filter(x, duration_minutes &gt; 0)) # group data by site and sampling event identifier # then, summarise relevant variables as the sum dataGrouped &lt;- map(data, function(x){ x %&gt;% group_by(sampling_event_identifier) %&gt;% summarise_at(vars(duration_minutes, effort_distance_km, effort_area_ha), list(sum.no.na)) }) # bind rows combining data frames, and filter dataGrouped &lt;- bind_rows(dataGrouped) %&gt;% filter(duration_minutes &lt;= 300, effort_distance_km &lt;= 5, effort_area_ha &lt;= 500) # get data identifiers, such as sampling identifier etc dataConstants &lt;- data %&gt;% bind_rows() %&gt;% select(sampling_event_identifier, time_observations_started, locality, locality_type, locality_id, observer_id, observation_date, scientific_name, observation_count, protocol_type, number_observers, longitude, latitude) # join the summarised data with the identifiers, using sampling_event_identifier as the key dataGrouped &lt;- left_join(dataGrouped, dataConstants, by = &quot;sampling_event_identifier&quot;) # remove checklists or seis with more than 10 obervers count(dataGrouped, number_observers &gt; 10) # count how many have 10+ obs dataGrouped &lt;- filter(dataGrouped, number_observers &lt;= 10) 2.6 Add decimal time # assign present or not, and get time in decimal hours since midnight library(lubridate) time_to_decimal &lt;- function(x) { x &lt;- hms(x, quiet = TRUE) hour(x) + minute(x) / 60 + second(x) / 3600 } # will cause issues if using time obs started as a linear effect and not quadratic dataGrouped = mutate(dataGrouped, pres_abs = observation_count &gt;= 1, decimalTime = time_to_decimal(time_observations_started)) # check class of dataGrouped, make sure not sf assertthat::assert_that(!&quot;sf&quot; %in% class(dataGrouped)) 2.7 Write data # save a temp data file save(dataGrouped, file = &quot;data_prelim_processing.rdata&quot;) "],
["prepare-landscape-data.html", "Section 3 Prepare landscape data 3.1 Prepare libraries 3.2 Prepare initial data 3.3 Resample rasters", " Section 3 Prepare landscape data 3.1 Prepare libraries # load libs library(raster) library(stringi) library(glue) library(gdalUtils) library(purrr) # prep mode function to aggregate funcMode &lt;- function(x, na.rm = T) { ux &lt;- unique(x) ux[which.max(tabulate(match(x, ux)))] } # a basic test assertthat::assert_that(funcMode(c(2,2,2,2,3,3,3,4)) == as.character(2), msg = &quot;problem in the mode function&quot;) # works 3.2 Prepare initial data 3.2.1 Prepare spatial extent # load hills library(sf) hills &lt;- st_read(&quot;data/spatial/hillsShapefile/Nil_Ana_Pal.shp&quot;) hills &lt;- st_transform(hills, 32643) buffer &lt;- st_buffer(hills, 3e4) %&gt;% st_transform(4326) 3.2.2 Prepare terrain rasters # load elevation and crop to hills size, then mask by hills alt &lt;- raster(&quot;data/spatial/Elevation/alt&quot;) alt.hills &lt;- crop(alt, as(buffer, &quot;Spatial&quot;)) rm(alt); gc() # get slope and aspect slopeData &lt;- terrain(x = alt.hills, opt = c(&quot;slope&quot;, &quot;aspect&quot;)) elevData &lt;- raster::stack(alt.hills, slopeData) rm(alt.hills); gc() 3.2.3 Prepare CHELSA rasters # list chelsa files chelsaFiles &lt;- list.files(&quot;data/chelsa/&quot;, full.names = TRUE, pattern = &quot;*.tif&quot;) # gather chelsa rasters chelsaData &lt;- purrr::map(chelsaFiles, function(chr){ a &lt;- raster(chr) crs(a) &lt;- crs(elevData) a &lt;- crop(a, as(buffer, &quot;Spatial&quot;)) return(a) }) # stack chelsa data chelsaData &lt;- raster::stack(chelsaData) 3.2.4 Stack prepared rasters # stack rasters for efficient reprojection later env_data &lt;- stack(elevData, chelsaData) 3.2.5 Prepare landcover # read in landcover raster location landcover &lt;- &quot;data/landUseClassification/Reprojected Image_26thJan2020_UTM_Ghats.tif&quot; # get extent e = bbox(raster(landcover)) # init resolution res_init &lt;- res(raster(landcover)) # res to transform to 1000m res_final &lt;- res_init*100 # use gdalutils gdalwarp for resampling transform # to 1km from 10m gdalwarp(srcfile = landcover, dstfile = &quot;data/landUseClassification/lc_01km.tif&quot;, tr=c(res_final), r=&#39;mode&#39;, te=c(e)) 3.2.6 Show resampled landcover # mask by study area { landcover &lt;- raster(landcover) # landcover &lt;- mask(landcover, mask = as(hills, &quot;Spatial&quot;)) lc_data &lt;- raster(&quot;data/landUseClassification/lc_01km.tif&quot;) # lc_data &lt;- mask(lc_data, mask = as(hills, &quot;Spatial&quot;)) lc_data[lc_data == 0] &lt;- NA } # make raster barplot data data1km = raster::getValues(lc_data); data1km = data1km[data1km &gt; 0]; data1km = table(data1km); data1km = data1km/sum(data1km) { data10m = raster::getValues(landcover); data10m = data10m[data10m &gt; 0] data10m = tibble(value = data10m) data10m = dplyr::count(a, value) %&gt;% dplyr::mutate(n=n/sum(n)) data10m = xtabs(n~value, data10m) } # map rasters { png(filename = &quot;figs/figLandcoverResample.png&quot;, width = 1200, height = 1200, res = 150) par(mfrow=c(2,2)) # rasterplots raster::plot(landcover, col = c(&quot;white&quot;, scico::scico(palette = &quot;batlow&quot;, 7)), main = &quot;10m sentinel data&quot;, xlab = &quot;longitude&quot;, y = &quot;latitude&quot;) plot(hills, add=T, border = &quot;red&quot;, col = &quot;transparent&quot;) raster::plot(rasterAgg1km, col = c(&quot;white&quot;, scico::scico(palette = &quot;batlow&quot;, 7)), main = &quot;1km resampled data&quot;, xlab = &quot;longitude&quot;, y = &quot;latitude&quot;) plot(hills, add=T, border = &quot;red&quot;, col = &quot;transparent&quot;) # barplots barplot(data10m, xlab = c(&quot;landcover class&quot;), ylab = &quot;prop.&quot;, col = scico::scico(palette = &quot;batlow&quot;, 7)) barplot(data1km, xlab = c(&quot;landcover class&quot;), ylab = &quot;prop.&quot;, col = scico::scico(palette = &quot;batlow&quot;, 7), alpha =0.8), add = F) barplot(data10m, xlab = c(&quot;landcover class&quot;), ylab = &quot;prop.&quot;, col = &quot;grey20&quot;, border = NA, density = 30, add = T) dev.off() } knitr::include_graphics(&quot;figs/figLandcoverResample.png&quot;) 3.3 Resample rasters 3.3.1 Read landcover as list Here, we read in the 1km landcover raster and set 0 to NA. lc_data &lt;- raster(&quot;data/landUseClassification/lc_01km.tif&quot;) lc_data[lc_data == 0] &lt;- NA 3.3.2 Reproject environmental data to landcover # resample to the corresponding landcover data env_data_resamp &lt;- projectRaster(from = env_data, to = lc_data, crs = crs(lc_data), res = res(lc_data)) # export as raster stack land_stack &lt;- stack(env_data_resamp, lc_data) # get names land_names &lt;- glue(&#39;data/spatial/landscape_resamp{c(&quot;01&quot;)}km.tif&#39;) # write to file writeRaster(land_stack, filename = as.character(land_names), overwrite=TRUE) "],
["prepare-observer-expertise.html", "Section 4 Prepare observer expertise 4.1 Prepare libraries 4.2 Prepare data 4.3 Explicit spatial subset 4.4 Prepare species of interest 4.5 Prepare checklists for observer score 4.6 Get landcover 4.7 Filter data for stats 4.8 Run observer expertise model 4.9 Write model to file 4.10 Get observer expertise as species in 60 mins 4.11 Write observer expertise to file", " Section 4 Prepare observer expertise 4.1 Prepare libraries # load libs library(data.table) library(readxl) library(magrittr) library(stringr) library(dplyr) library(tidyr) library(auk) # get decimal time function library(lubridate) time_to_decimal &lt;- function(x) { x &lt;- hms(x, quiet = TRUE) hour(x) + minute(x) / 60 + second(x) / 3600 } 4.2 Prepare data Here, we go through the data preparation process again because we might want to assess observer expertise over a larger area than the study site. # Read in shapefile of study area to subset by bounding box library(sf) wg &lt;- st_read(&quot;data/spatial/hillsShapefile/Nil_Ana_Pal.shp&quot;) %&gt;% st_transform(32643) # set file paths for auk functions f_in_ebd &lt;- file.path(&quot;ebd_Filtered_Jun2019.txt&quot;) f_in_sampling &lt;- file.path(&quot;ebd_sampling_Filtered_Jun2019.txt&quot;) # run filters using auk packages ebd_filters &lt;- auk_ebd(f_in_ebd, f_in_sampling) %&gt;% auk_country(country = &quot;IN&quot;) %&gt;% auk_state(c(&quot;IN-KL&quot;,&quot;IN-TN&quot;, &quot;IN-KA&quot;)) %&gt;% # Restricting geography to TamilNadu, Kerala &amp; Karnataka auk_date(c(&quot;2013-01-01&quot;, &quot;2018-12-31&quot;)) %&gt;% auk_complete() # check filters ebd_filters # specify output location and perform filter f_out_ebd &lt;- &quot;data/ebird_for_expertise.txt&quot; f_out_sampling &lt;- &quot;data/ebird_sampling_for_expertise.txt&quot; ebd_filtered &lt;- auk_filter(ebd_filters, file = f_out_ebd, file_sampling = f_out_sampling, overwrite = TRUE) ## Process filtered data # read in the data ebd &lt;- fread(f_out_ebd) names &lt;- names(ebd) %&gt;% stringr::str_to_lower() %&gt;% stringr::str_replace_all(&quot; &quot;, &quot;_&quot;) setnames(ebd, names) # choose columns of interest columnsOfInterest &lt;- c(&quot;checklist_id&quot;,&quot;scientific_name&quot;,&quot;observation_count&quot;,&quot;locality&quot;,&quot;locality_id&quot;,&quot;locality_type&quot;,&quot;latitude&quot;,&quot;longitude&quot;,&quot;observation_date&quot;,&quot;time_observations_started&quot;,&quot;observer_id&quot;,&quot;sampling_event_identifier&quot;,&quot;protocol_type&quot;,&quot;duration_minutes&quot;,&quot;effort_distance_km&quot;,&quot;effort_area_ha&quot;,&quot;number_observers&quot;,&quot;species_observed&quot;,&quot;reviewed&quot;) ebd &lt;- setDF(ebd) %&gt;% as_tibble() %&gt;% select(one_of(columnsOfInterest)) setDT(ebd) 4.3 Explicit spatial subset # get checklist locations ebd_locs &lt;- ebd[,.(longitude, latitude)] ebd_locs &lt;- setDF(ebd_locs) %&gt;% distinct() ebd_locs &lt;- st_as_sf(ebd_locs, coords = c(&quot;longitude&quot;,&quot;latitude&quot;)) %&gt;% `st_crs&lt;-`(4326) %&gt;% bind_cols(as_tibble(st_coordinates(.))) %&gt;% st_transform(32643) %&gt;% mutate(id = 1:nrow(.)) # check whether to include to_keep &lt;- unlist(st_contains(wg, ebd_locs)) # filter locs ebd_locs &lt;- filter(ebd_locs, id %in% to_keep) %&gt;% bind_cols(as_tibble(st_coordinates(.))) %&gt;% st_drop_geometry() ebd &lt;- ebd[longitude %in% ebd_locs$X &amp; latitude %in% ebd_locs$Y,] 4.4 Prepare species of interest # read in species list specieslist = read_excel(path = &quot;data/V2_ListOfSpecies_ChosenForStudy.csv.xlsx&quot;) # set species of interest soi = specieslist$scientific_name ebdSpSum &lt;- ebd[,.(nSp = .N, totSoiSeen = length(intersect(scientific_name, soi))), by = list(sampling_event_identifier)] # write to file and link with checklsit id later fwrite(ebdSpSum, file = &quot;data/dataChecklistSpecies.csv&quot;) 4.5 Prepare checklists for observer score # 1. add new columns of decimal time and julian date ebd[,`:=`(decimalTime = time_to_decimal(time_observations_started), julianDate = yday(as.POSIXct(observation_date)))] ebdEffChk &lt;- setDF(ebd) %&gt;% mutate(year = year(observation_date)) %&gt;% distinct(sampling_event_identifier, observer_id, year, duration_minutes, effort_distance_km, effort_area_ha, longitude, latitude, locality, locality_id, decimalTime, julianDate, number_observers) %&gt;% # drop rows with NAs in cols used in the model tidyr::drop_na(sampling_event_identifier, observer_id, duration_minutes, decimalTime, julianDate) %&gt;% # drop years below 2013 filter(year &gt;= 2013) # 3. join to covariates and remove large groups (&gt; 10) ebdChkSummary &lt;- inner_join(ebdEffChk, ebdSpSum) # remove ebird data rm(ebd); gc() 4.6 Get landcover Using the landcover 1km resolution here. # read in 1km landcover and set 0 to NA library(raster) landcover &lt;- raster::raster(&quot;data/landUseClassification/lc_01km.tif&quot;) landcover[landcover==0] &lt;- NA # get locs in utm coords locs &lt;- distinct(ebdChkSummary, sampling_event_identifier, longitude, latitude, locality, locality_id) locs &lt;- st_as_sf(locs, coords = c(&quot;longitude&quot;, &quot;latitude&quot;)) %&gt;% `st_crs&lt;-`(4326) %&gt;% st_transform(32643) %&gt;% st_coordinates() # get for unique points landcoverVec &lt;- raster::extract(x = landcover, y = locs) # assign to df and overwrite setDT(ebdChkSummary)[,landcover:= landcoverVec] 4.7 Filter data for stats # change names for easy handling setnames(ebdChkSummary, c(&quot;sei&quot;, &quot;observer&quot;,&quot;year&quot;, &quot;duration&quot;, &quot;distance&quot;, &quot;area&quot;, &quot;longitude&quot;, &quot;latitude&quot;, &quot;locality&quot;, &quot;locality_id&quot;, &quot;decimalTime&quot;, &quot;julianDate&quot;, &quot;nObs&quot;, &quot;nSp&quot;, &quot;nSoi&quot;, &quot;landcover&quot;)) # count data points per observer obscount &lt;- count(ebdChkSummary, observer) %&gt;% filter(n &gt;= 10) # make factor variables and remove obs not in obscount # also remove 0 durations ebdChkSummary &lt;- ebdChkSummary %&gt;% mutate(distance = ifelse(is.na(distance), 0, distance), duration = if_else(is.na(duration), 0.0, as.double(duration))) %&gt;% filter(observer %in% obscount$observer, duration &gt; 0, duration &lt;= 300, nSoi &gt;= 0, distance &lt;= 5, !is.na(nSoi)) %&gt;% mutate(landcover = as.factor(landcover), observer = as.factor(observer)) %&gt;% drop_na(landcover) # save to file for later reuse fwrite(ebdChkSummary, file = &quot;data/eBirdChecklistVars.csv&quot;) 4.8 Run observer expertise model Our observer expertise model aims to include the random intercpet effect of observer identity, with a random slope effect of duration. This models the different rate of species accumulation by different observers, as well as their different starting points. # uses either a subset or all data library(lmerTest) # here we specify a glmm with random effects for observer # time is considered a fixed log predictor and a random slope modObsExp &lt;- glmer(nSoi ~ sqrt(duration) + landcover+ sqrt(decimalTime) + I((sqrt(decimalTime))^2) + log(julianDate) + I((log(julianDate)^2)) + (1|observer) + (0+duration|observer), data = ebdChkSummary, family = &quot;poisson&quot;) 4.9 Write model to file # make dir if absent if(!dir.exists(&quot;data/modOutput&quot;)){ dir.create(&quot;data/modOutput&quot;) } # write model output to text file { writeLines(R.utils::captureOutput(list(Sys.time(), summary(modObsExp))), con = &quot;data/modOutput/modOutExpertise.txt&quot;) } 4.10 Get observer expertise as species in 60 mins # make df with means observer &lt;- unique(ebdChkSummary$observer) # predict at 60 mins on the most common landcover dfPredict &lt;- ebdChkSummary %&gt;% summarise_at(vars(duration, decimalTime, julianDate), list(~mean(.))) %&gt;% mutate(duration = 60, landcover = as.factor(6)) %&gt;% tidyr::crossing(observer) # run predict from model on it dfPredict &lt;- mutate(dfPredict, score = predict(modObsExp, newdata = dfPredict, type = &quot;response&quot;, allow.new.levels = TRUE)) %&gt;% mutate(score = scales::rescale(score)) 4.11 Write observer expertise to file fwrite(dfPredict %&gt;% dplyr::select(observer, score), file = &quot;data/dataObsExpScore.csv&quot;) "],
["add-covariates-to-subsampled-data.html", "Section 5 Add covariates to subsampled data 5.1 Prepare libraries and data 5.2 Spatial subsampling 5.3 Temporal subsampling 5.4 Add expertise score 5.5 Add landscape covariates 5.6 Construct buffers around subsampled points 5.7 Getting area means 5.8 Join landscape data to obs data 5.9 Write data to files", " Section 5 Add covariates to subsampled data 5.1 Prepare libraries and data # load libs library(dplyr); library(readr) library(stringr) library(purrr) library(raster) library(glue) library(velox) library(tidyr) library(sf) # load saved data object load(&quot;data_prelim_processing.rdata&quot;) 5.2 Spatial subsampling # grid based spatial thinning gridsize = 1000 # grid size in metres effort_distance_max = 1000 # removing checklists with this distance # make grids across the study site hills &lt;- st_read(&quot;data/spatial/hillsShapefile/Nil_Ana_Pal.shp&quot;) %&gt;% st_transform(32643) grid &lt;- st_make_grid(hills, cellsize = gridsize) # split data by species data_spatial_thin &lt;- split(x = dataGrouped, f = dataGrouped$scientific_name) # spatial thinning on each species retains # site with maximum visits per grid cell data_spatial_thin &lt;- map(data_spatial_thin, function(df){ # count visits per locality df &lt;- group_by(df, locality) %&gt;% mutate(tot_effort = length(sampling_event_identifier)) %&gt;% ungroup() # remove sites with distances above spatial independence df &lt;- df %&gt;% filter(effort_distance_km &lt;= effort_distance_max) %&gt;% st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;)) %&gt;% `st_crs&lt;-`(4326) %&gt;% st_transform(32643) %&gt;% mutate(coordId = 1:nrow(.)) %&gt;% bind_cols(as_tibble(st_coordinates(.))) # whcih cell has which coords grid_contents &lt;- st_contains(grid, df) %&gt;% as_tibble() %&gt;% rename(cell = row.id, coordId = col.id) # what&#39;s the max point in each grid points_max &lt;- left_join(df %&gt;% st_drop_geometry(), grid_contents, by = &quot;coordId&quot;) %&gt;% group_by(cell) %&gt;% filter(tot_effort == max(tot_effort)) return(points_max) }) # remove old data rm(dataGrouped) 5.3 Temporal subsampling Get 10 random (if available) observations of each species at each locality. # subsample data for random 10 observations dataSubsample &lt;- map(data_spatial_thin, function(df){ df &lt;- ungroup(df) df_to_locality &lt;- split(x = df, f = df$locality) df_samples &lt;- map_if(.x = df_to_locality, .p = function(x) {nrow(x) &gt; 10}, .f = function(x) sample_n(x, 10, replace = FALSE)) return(bind_rows(df_samples)) }) # bind all rows for data frame dataSubsample &lt;- bind_rows(dataSubsample) # remove previous data rm(data_spatial_thin) 5.4 Add expertise score # read in obs score and extract numbers expertiseScore &lt;- read_csv(&quot;data/dataObsExpScore.csv&quot;) %&gt;% mutate(numObserver = str_extract(observer, &quot;\\\\d+&quot;)) %&gt;% dplyr::select(-observer) # group seis consist of multiple observers # in this case, seis need to have the highest expertise observer score # as the associated covariate # get unique observers per sei dataSeiScore &lt;- distinct(dataSubsample, sampling_event_identifier, observer_id) %&gt;% # make list column of observers mutate(observers = str_split(observer_id, &quot;,&quot;)) %&gt;% unnest(cols = c(observers)) %&gt;% # add numeric observer id mutate(numObserver = str_extract(observers, &quot;\\\\d+&quot;)) %&gt;% # now get distinct sei and observer id numeric distinct(sampling_event_identifier, numObserver) # now add expertise score to sei dataSeiScore &lt;- left_join(dataSeiScore, expertiseScore, by=&quot;numObserver&quot;) %&gt;% # get max expertise score per sei group_by(sampling_event_identifier) %&gt;% summarise(expertise = max(score)) # add to dataCovar dataSubsample &lt;- left_join(dataSubsample, dataSeiScore, by = &quot;sampling_event_identifier&quot;) # remove data without expertise score dataSubsample &lt;- filter(dataSubsample, !is.na(expertise)) 5.5 Add landscape covariates # list landscape covariate stacks landscape_files &lt;- &quot;data/spatial/landscape_resamp01km.tif&quot; # read in as stacks landscape_data &lt;- stack(landscape_files) # get proper names elev_names &lt;- c(&quot;elev&quot;, &quot;slope&quot;, &quot;aspect&quot;) chelsa_names &lt;- c(&quot;chelsa_bio10_04&quot;, &quot;chelsa_bio10_17&quot;, &quot;chelsa_bio10_18&quot;,&quot;chelsa_prec&quot;, &quot;chelsa_temp&quot;) names(landscape_data) &lt;- as.character(glue(&#39;{c(elev_names, chelsa_names, &quot;landcover&quot;)}&#39;)) 5.6 Construct buffers around subsampled points # assign neighbourhood radius in m sample_radius &lt;- c(2.5, 10, 25) * 1e3 # get distinct points and make buffer ebird_buff &lt;- dataSubsample %&gt;% ungroup() %&gt;% distinct(X, Y) %&gt;% mutate(id = 1:nrow(.)) %&gt;% crossing(sample_radius) %&gt;% arrange(id) %&gt;% group_by(sample_radius) %&gt;% nest() %&gt;% ungroup() # convert to spatial features ebird_buff &lt;- mutate(ebird_buff, data = map2(data, sample_radius, function(df,rd){ df_sf &lt;- st_as_sf(df, coords = c(&quot;X&quot;, &quot;Y&quot;), crs = 32643) %&gt;% # add long lat bind_cols(as_tibble(st_coordinates(.))) %&gt;% # rename(longitude = X, latitude = Y) %&gt;% # # transform to modis projection # st_transform(crs = 32643) %&gt;% # buffer to create neighborhood around each point st_buffer(dist = rd) })) 5.7 Getting area means 5.7.1 Mean environmental covariates All covariates are 2.5km mean values and prefixed âam_â. # get area mean for all preds except landcover, which is the last one env_area_mean &lt;- purrr::map(ebird_buff$data, function(df){ stk &lt;- landscape_data[[-dim(landscape_data)[3]]] # removing landcover here velstk &lt;- velox(stk) dextr &lt;- velstk$extract(sp = df, df = TRUE, fun = function(x)mean(x, na.rm=T)) # assign names for joining names(dextr) &lt;- c(&quot;id&quot;, names(stk)) return(as_tibble(dextr)) }) # join to buffer data ebird_buff &lt;- ebird_buff %&gt;% mutate(data = map2(data, env_area_mean, inner_join, by = &quot;id&quot;)) 5.7.2 Proportional landcover # get the last element of each stack from the list # this is the landcover at that resolution lc_area_prop &lt;- purrr::map(ebird_buff$data, function(df){ lc &lt;- landscape_data[[dim(landscape_data)[3]]] # accessing landcover here lc_velox &lt;- velox(lc) lc_vals &lt;- lc_velox$extract(sp = df, df = TRUE) names(lc_vals) &lt;- c(&quot;id&quot;, &quot;lc&quot;) # get landcover proportions lc_prop &lt;- count(lc_vals, id, lc) %&gt;% group_by(id) %&gt;% mutate(lc = glue(&#39;lc_{str_pad(lc, 2, pad = &quot;0&quot;)}&#39;), prop = n/sum(n)) %&gt;% dplyr::select(-n) %&gt;% tidyr::pivot_wider(names_from = lc, values_from = prop, values_fill = list(prop = 0)) %&gt;% ungroup() return(lc_prop) }) # join to data ebird_buff &lt;- ebird_buff %&gt;% mutate(data = map2(data, lc_area_prop, inner_join, by = &quot;id&quot;)) 5.8 Join landscape data to obs data # duplicate scale data data_at_scale &lt;- ebird_buff # join the full data to landscape samples at each scale data_at_scale$data &lt;- map(data_at_scale$data, function(df){ df &lt;- st_drop_geometry(df) df &lt;- inner_join(dataSubsample, df, by=c(&quot;X&quot;, &quot;Y&quot;)) return(df) }) 5.9 Write data to files # write to file pmap(data_at_scale, function(sample_radius, data){ write_csv(data, path = glue(&#39;data/dataCovars_{str_pad(sample_radius/1e3, 2, pad = &quot;0&quot;)}km.csv&#39;)) message(glue(&#39;export done: data/dataCovars_{str_pad(sample_radius/1e3, 2, pad = &quot;0&quot;)}km.csv&#39;)) }) "],
["occupancy-modelling.html", "Section 6 Occupancy modelling 6.1 Load necessary libraries 6.2 Load dataframe and scale covariates 6.3 Setting up observation covariates for each model as they remain consistent 6.4 Get the AIC values for the top models", " Section 6 Occupancy modelling 6.1 Load necessary libraries # Load libraries library(auk) library(lubridate) library(sf) library(unmarked) library(raster) library(ebirdst) library(MuMIn) library(AICcmodavg) library(fields) library(tidyverse) library(doParallel) library(snow) library(openxlsx) library(data.table) library(dplyr) library(ecodist) # Source necessary functions source(&quot;code/fun_screen_cor.R&quot;) source(&quot;code/fun_model_estimate_collection.r&quot;) 6.2 Load dataframe and scale covariates Here, we load the required dataframe that contains 10 random visits to a site. Please note that this process is repeated for each dataframe where environmental covariates were prepared at a spatial scale of 2.5 and 10 sq.km around each unique locality. We also scaled all covariates (mean around 0 and standard deviation of 1) # Load in the prepared dataframe that contains 10 random visits to each site dat &lt;- fread(&quot;C:\\\\Occupancy Runs\\\\dataCovars_2.5km.csv&quot;,header=T) setDF(dat) head(dat) # Some more pre-processing to get the right data structures # Ensuring that only Traveling and Stationary checklists were considered names(dat) dat &lt;- dat %&gt;% filter(protocol_type==&quot;Traveling&quot; | protocol_type==&quot;Stationary&quot;) # We take all stationary counts and give them a distance of 100 m (so 0.1 km), # as that&#39;s approximately the max normal hearing distance for people doing point counts. dat &lt;- dat %&gt;% mutate(effort_distance_km = replace(effort_distance_km, which(effort_distance_km==0 &amp; protocol_type == &quot;Stationary&quot;), 0.1)) # Converting time observations started to numeric and adding it as a new column # This new column will be minute_observations_started dat &lt;- dat %&gt;% mutate(min_obs_started= strtoi(as.difftime(time_observations_started, format = &quot;%H:%M:%S&quot;, units = &quot;mins&quot;))) # Adding the julian date to the dataframe dat &lt;- dat %&gt;% mutate(julian_date = lubridate::yday(dat$observation_date)) # Removing other unnecessary columns from the dataframe and creating a clean one without the rest names(dat) dat &lt;- dat[,-c(1,4,5,16,18,21,23)] # Rename column names: names(dat) &lt;- c(&quot;duration_minutes&quot;, &quot;effort_distance_km&quot;,&quot;locality&quot;, &quot;locality_type&quot;, &quot;locality_id&quot;, &quot;observer_id&quot;, &quot;observation_date&quot;, &quot;scientific_name&quot;, &quot;observation_count&quot;, &quot;protocol_type&quot;, &quot;number_observers&quot;,&quot;pres_abs&quot;,&quot;tot_effort&quot;,&quot;longitude&quot;, &quot;latitude&quot;,&quot;expertise&quot;, &quot;alt.y&quot;,&quot;slope.y&quot;,&quot;aspect.y&quot;,&quot;bio_4.y&quot;,&quot;bio_17.y&quot;,&quot;bio_18.y&quot;,&quot;prec_interannual.y&quot;,&quot;temp_interannual.y&quot;, &quot;lc_01.y&quot;, &quot;lc_02.y&quot;, &quot;lc_03.y&quot;,&quot;lc_06.y&quot;, &quot;lc_07.y&quot;,&quot;lc_04.y&quot;, &quot;lc_05.y&quot;, &quot;min_obs_started&quot;, &quot;julian_date&quot;) dat.1 &lt;- dat %&gt;% mutate(year = year(observation_date), pres_abs = as.integer(pres_abs)) # occupancy modeling requires an integer response # Scaling detection and occupancy covariates dat.scaled &lt;- dat.1 dat.scaled[,c(1,2,11,16:33)] &lt;- scale(dat.scaled[,c(1,2,11,16:33)]) # Scaling and standardizing detection and site-level covariates fwrite(dat.scaled, file = &quot;C:\\\\Occupancy Runs\\\\scaledCovs_2.5km.csv&quot;) dat.scaled &lt;- fread(&quot;C:\\\\Occupancy Runs\\\\scaledCovs_10km.csv&quot;,header=T) setDF(dat.scaled) head(dat.scaled) # Testing for correlations before running further analyses names(dat.scaled) screen.cor(dat.scaled[,c(1,2,16:33)], threshold = 0.5) # Removing predictors that are highly correlated with one another # At a spatial scale of 2.5km, bio_4.y was removed and at a spatial scale of 10km, # bio_4.y and bio_17.y were removed dat.scaled &lt;- dat.scaled[,c(1:19,22:34)] names(dat.scaled) # Split data by species list_of_species &lt;- split(x=dat.scaled, f=dat.scaled$scientific_name) 6.3 Setting up observation covariates for each model as they remain consistent # Load obs covs obs_covs &lt;- c(&quot;min_obs_started&quot;, &quot;duration_minutes&quot;, &quot;effort_distance_km&quot;, &quot;number_observers&quot;, &quot;protocol_type&quot;, &quot;expertise&quot;, &quot;julian_date&quot;) # Detection terms are fixed across occupancy models det_terms &lt;- c(&quot;p(duration_minutes)&quot;,&quot;p(effort_distance_km)&quot;, &quot;p(expertise)&quot;,&quot;p(julian_date)&quot;,&quot;p(min_obs_started)&quot;, &quot;p(number_observers)&quot;,&quot;p(protocol_type)&quot;) Here, we identify the top models for each species # Getting a list of top models for each species list_of_models_per_species &lt;- map(list_of_species, function(df){ df &lt;- bind_rows(df) # Preparing data for the unmarked model occ &lt;- filter_repeat_visits(df, min_obs = 1, max_obs = 10, annual_closure = FALSE, n_days = 2200, # 6 years is considered a period of closure date_var = &quot;observation_date&quot;, site_vars = c(&quot;locality_id&quot;)) # format for unmarked # The site_covs have to be varied across spatial scales occ_wide &lt;- format_unmarked_occu(occ, site_id = &quot;site&quot;, response = &quot;pres_abs&quot;, site_covs = c(&quot;locality_id&quot;,&quot;lc_01.y&quot;,&quot;lc_02.y&quot;,&quot;lc_03.y&quot;,&quot;lc_04.y&quot;, &quot;lc_05.y&quot;, &quot;lc_06.y&quot;, &quot;lc_07.y&quot;,&quot;bio_18.y&quot;,&quot;alt.y&quot;,&quot;aspect.y&quot;,&quot;slope.y&quot;, &quot;prec_interannual.y&quot;,&quot;temp_interannual.y&quot;),obs_covs = obs_covs) # Convert this dataframe of observations into an unmarked object to start fitting occupancy models occ_um &lt;- formatWide(occ_wide, type = &quot;unmarkedFrameOccu&quot;) # Fit a global model with all detection level covariates global_det &lt;- occu(~ min_obs_started+ julian_date + duration_minutes + effort_distance_km + number_observers + protocol_type + expertise ~ 1, data = occ_um) # Fit a global land cover model with all detection level covariates model_lc &lt;- occu(~min_obs_started+ julian_date + duration_minutes + effort_distance_km + number_observers + protocol_type + expertise~lc_01.y+lc_02.y+lc_03.y+lc_04.y+ lc_05.y+ lc_06.y+ lc_07.y, data = occ_um) # Fit a global model with all climate associated predictors model_clim &lt;- occu(~min_obs_started+ julian_date + duration_minutes + effort_distance_km + number_observers + protocol_type + expertise~ bio_18.y + prec_interannual.y + temp_interannual.y, data = occ_um) # Fit a global model with all elevation associated predictors model_elev &lt;- occu(~min_obs_started+ julian_date + duration_minutes + effort_distance_km + number_observers + protocol_type + expertise~alt.y + slope.y +aspect.y , data = occ_um) # Landcover:Elevation - Fit a global model with grasslands, forests, plantations and tea model_lc1_elev &lt;- occu(~min_obs_started+ julian_date + duration_minutes + effort_distance_km + number_observers + protocol_type + expertise~lc_03.y*alt.y+lc_02.y*alt.y+lc_04.y*alt.y+ lc_06.y*alt.y, data = occ_um) # Landcover:Elevation - Fit a global model with agriculture, settlements and waterbodies model_lc2_elev &lt;- occu(~min_obs_started+ julian_date + duration_minutes + effort_distance_km + number_observers + protocol_type + expertise~lc_01.y*alt.y+lc_05.y*alt.y+ lc_07.y*alt.y, data = occ_um) # Fit a global model with climate associated predictors (bio_4,bio_17,bio_18,prec_interannual) model_clim_elev &lt;- occu(~min_obs_started+ julian_date + duration_minutes + effort_distance_km + number_observers + protocol_type + expertise~bio_18.y*alt.y + prec_interannual.y*alt.y, data = occ_um) # Fit a global model with all landcover and climate associated predictors model_lc_clim &lt;- occu(~min_obs_started+ julian_date + duration_minutes + effort_distance_km + number_observers + protocol_type + expertise~lc_01.y + lc_02.y +lc_03.y + lc_04.y + lc_05.y + lc_06.y + lc_07.y+ prec_interannual.y + temp_interannual.y, data = occ_um) # Set up the cluster clusterType &lt;- if(length(find.package(&quot;snow&quot;, quiet = TRUE))) &quot;SOCK&quot; else &quot;PSOCK&quot; clust &lt;- try(makeCluster(getOption(&quot;cl.cores&quot;, 4), type = clusterType)) clusterEvalQ(clust, library(unmarked)) clusterExport(clust, &quot;occ_um&quot;) # Dredging all_det &lt;- pdredge(global_det, clust) all_lc &lt;- pdredge(model_lc, clust, fixed=det_terms) all_clim &lt;- pdredge(model_clim, clust, fixed = det_terms) all_elev &lt;- pdredge(model_elev, clust, fixed = det_terms) all_lc1_elev &lt;- pdredge(model_lc1_elev, clust, fixed = det_terms) all_lc2_elev &lt;- pdredge(model_lc2_elev, clust, fixed = det_terms) all_clim_elev &lt;- pdredge(model_clim_elev, clust, fixed = det_terms) all_lc_clim &lt;- pdredge(model_lc_clim, clust, fixed = det_terms) # Get the top models, which we&#39;ll define as those with deltaAICc &lt; 2 top_det &lt;- get.models(all_det, subset = delta &lt; 2, cluster = clust) top_lc &lt;- get.models(all_lc, subset = delta &lt; 2, cluster = clust) top_clim &lt;- get.models(all_clim, subset = delta &lt; 2, cluster = clust) top_elev &lt;- get.models(all_elev, subset = delta &lt; 2, cluster = clust) top_lc1_elev &lt;- get.models(all_lc1_elev, subset = delta &lt; 2, cluster = clust) top_lc2_elev &lt;- get.models(all_lc2_elev, subset = delta &lt; 2, cluster = clust) top_clim_elev &lt;- get.models(all_clim_elev, subset = delta &lt; 2, cluster = clust) top_lc_clim &lt;- get.models(all_lc_clim, subset = delta &lt; 2, cluster = clust) return_data &lt;- list(top_det, top_lc, top_clim, top_elev, top_lc1_elev, top_lc2_elev, top_clim_elev,top_lc_clim) names(return_data) &lt;- c(&quot;Top Detection Models&quot;,&quot;Top LandCover Models&quot;, &quot;Top Climate Models&quot;,&quot;Top Elevation Models&quot;, &quot;Top Landcover1*Elevation Models&quot;,&quot;Top Landcover2*Elevation Models&quot;, &quot;Top Climate*Elevation Models&quot;,&quot;Top LandCover+Climate Models&quot;) return(return_data) stopCluster(clust) }) 6.4 Get the AIC values for the top models # Unable to get the modify_depth to work ?modify_depth trial &lt;- list_of_models_per_species[1] list_of_AIC_per_species &lt;- map(list_of_models_per_species, function(x){ a &lt;- modify_depth(x,3, min) return(a) }) "],
["results-best-supported-hypotheses.html", "Section 7 Results: Best supported hypotheses 7.1 Prepare libraries 7.2 Prepare data 7.3 Main text Figure 3 7.4 Plot supplementary Figure 3 7.5 Show Figure 3 Supplement", " Section 7 Results: Best supported hypotheses 7.1 Prepare libraries # load libraries library(dplyr) library(readr) library(forcats) library(tidyr) # plotting library(ggplot2) library(ggthemes) library(scico) 7.2 Prepare data # load data data &lt;- list.files(path = &quot;data/results&quot;, pattern = &quot;ElevationSummary&quot;, full.names = TRUE) data &lt;- lapply(data, readxl::read_excel) # attach scale sp_scale &lt;- c(&quot;10.0km&quot;, &quot;2.5km&quot;) data &lt;- purrr::map2(data, sp_scale, function(df, sp_scale){ df$scale = sp_scale return(df) }) # prepare data data &lt;- data %&gt;% bind_rows() %&gt;% group_by(scale) %&gt;% arrange(Hypothesis, median) %&gt;% group_by(scale) %&gt;% mutate(plot_order = 1:length(median)) %&gt;% ungroup() %&gt;% mutate(scale = as_factor(scale)) # plot positions of hypothesis code data_hyp &lt;- count(data, scale, Hypothesis) %&gt;% group_by(scale) %&gt;% mutate(plot_pos = cumsum(n)) %&gt;% ungroup() %&gt;% mutate(Hypothesis = as_factor(Hypothesis), plot_letter = letters[as.numeric(Hypothesis)]) 7.3 Main text Figure 3 # make figure 3 using option 1 fig_hypothesis_elev &lt;- ggplot(data, aes(x = plot_order, y = median, col = Hypothesis))+ geom_hline(yintercept = c(700,1400), lty = 2, size = 0.2)+ geom_errorbar(width = 0.4, aes(ymin = q1, ymax = q3,))+ geom_point(aes(fill = Hypothesis), shape = 21, col = &quot;grey20&quot;)+ geom_text(aes(label = Species), size = 2.5, fontface = &quot;italic&quot;, nudge_x = 0.4, col = &quot;black&quot;)+ geom_text(data = data_hyp, aes(x = plot_pos, y = 2250, label = plot_letter), col = &quot;grey20&quot;, fontface = &quot;bold&quot;)+ geom_vline(data = data_hyp, aes(xintercept = plot_pos + 0.5), lty = 3, lwd = 0.4, col = &quot;grey&quot;)+ scale_color_hue(l = 50, c = 50)+ scale_fill_hue(l = 70)+ theme_few()+ theme(legend.position = &quot;none&quot;, legend.key = element_rect(colour = &quot;white&quot;, size = 0.3), axis.text.y = element_blank(), axis.ticks.y = element_blank())+ labs(y = &quot;elevation (m)&quot;, x = &quot;Species&quot;, colour = &quot;Best supported\\nhypothesis&quot;)+ coord_flip()+ facet_wrap(~scale, labeller = label_both) # save figure # save figure ggsave(fig_hypothesis_elev, filename = &quot;figs/fig_hyp_elev.png&quot;, height = 10, width = 8) knitr::include_graphics(&quot;figs/fig_hyp_elev.png&quot;) 7.4 Plot supplementary Figure 3 This figure is intended to show the change (or not) in best supported hypothesis between spatial scales. 7.4.1 Prepare data # load data again data &lt;- list.files(path = &quot;data/results&quot;, pattern = &quot;ElevationSummary&quot;, full.names = TRUE) data &lt;- lapply(data, readxl::read_excel) # attach scale sp_scale &lt;- c(&quot;10.0km&quot;, &quot;2.5km&quot;) data &lt;- purrr::map2(data, sp_scale, function(df, sp_scale){ df$scale = sp_scale return(df) }) # unlist and prepare to plot data &lt;- data %&gt;% bind_rows() %&gt;% arrange(Hypothesis) %&gt;% # select(Species, median, Hypothesis, scale) mutate(Hypothesis = as_factor(Hypothesis)) %&gt;% pivot_wider(names_from = &quot;scale&quot;, values_from = &quot;Hypothesis&quot;, names_prefix = &quot;hypothesis_&quot;) # set order data &lt;- arrange(data, median) %&gt;% ungroup() %&gt;% mutate(plot_order = 1:nrow(data)) # make figure 3 for supplementary material fig_hypothesis_elev_supp &lt;- ggplot(data, aes(x = plot_order, y = median, ymin = q1, ymax = q3))+ geom_hline(yintercept = c(700,1400), lty = 2, size = 0.2)+ geom_errorbar(size = 0.3, width = 0.4)+ geom_point(fill = &quot;grey&quot;, shape = 21)+ geom_text(aes(label = Species), size = 3, fontface = &quot;italic&quot;, nudge_x = 0.5)+ geom_label(aes(y = 2500, col = hypothesis_10.0km, label = as.numeric(hypothesis_10.0km)), label.padding = unit(0.2, &quot;lines&quot;), fontface = &quot;bold&quot;, fill = &quot;grey95&quot;, size = 3)+ geom_label(aes(y = 2250, col = hypothesis_2.5km, label = as.numeric(hypothesis_2.5km)), label.padding = unit(0.2, &quot;lines&quot;), fontface = &quot;bold&quot;, fill = &quot;grey95&quot;, size = 3)+ annotate(geom = &quot;text&quot;, x = c(45, 43, 43), y = c(2375, 2250, 2500), label = c(&quot;best supported\\nhypothesis&quot;,&quot;2.5km&quot;, &quot;10km&quot;))+ scale_colour_scico_d(end = 0.6)+ scale_y_continuous(breaks = c(seq(0, 2000, 500)))+ # scale_fill_hue(l = 100, c = 50)+ coord_flip()+ theme_few()+ theme(legend.position = &quot;none&quot;, legend.key = element_rect(colour = &quot;white&quot;, size = 0.3), axis.text.y = element_blank(), axis.ticks.y = element_blank())+ labs(y = &quot;elevation (m)&quot;, x = &quot;Species&quot;, colour = &quot;Best supported\\nhypothesis&quot;) # save figure ggsave(fig_hypothesis_elev_supp, filename = &quot;figs/fig_hyp_elev_supp.png&quot;, height = 10, width = 6) 7.5 Show Figure 3 Supplement knitr::include_graphics(&quot;figs/fig_hyp_elev_supp.png&quot;) "],
["results-occupancy-predictors.html", "Section 8 Results: Occupancy predictors 8.1 Prepare libraries 8.2 Get data from hypothesis sheet 8.3 Read model estimates 8.4 Prepare interaction plots 8.5 Show example interaction 8.6 Occupancy predictorsâ aggregated effect", " Section 8 Results: Occupancy predictors 8.1 Prepare libraries # to load data library(readxl) # to handle data library(dplyr) library(readr) library(forcats) library(tidyr) library(purrr) library(stringr) # to wrangle models source(&quot;code/fun_model_estimate_collection.r&quot;) source(&quot;code/fun_make_resp_data.r&quot;) # plotting library(ggplot2) library(patchwork) source(&#39;code/fun_plot_interaction.r&#39;) 8.2 Get data from hypothesis sheet # read in the excel sheet containing information on the best supported hypothesis sheet_names &lt;- readxl::excel_sheets(&quot;data/results/all_hypoComparisons_allScales.xlsx&quot;) which_sheet &lt;- which(str_detect(sheet_names, &quot;Best&quot;)) hypothesis_data &lt;- readxl::read_excel(&quot;data/results/all_hypoComparisons_allScales.xlsx&quot;, sheet = sheet_names[which_sheet]) # Subsetting the data needed to call in each species&#39; model coefficient information hypothesis_data &lt;- select(hypothesis_data, Scientific_name, Common_name, contains(&quot;Best supported hypothesis&quot;)) # pivot longer hypothesis_data &lt;- pivot_longer(hypothesis_data, cols = contains(&quot;Best&quot;), names_to = &quot;scale&quot;, values_to = &quot;hypothesis&quot;) # fix scale to numeric hypothesis_data &lt;- mutate(hypothesis_data, scale = if_else(str_detect(scale, &quot;10&quot;), &quot;10km&quot;, &quot;2.5km&quot;)) # list the supported hypotheses # first separate the hypotheses into two columns hypothesis_data &lt;- separate(hypothesis_data, col = hypothesis, sep = &quot;; &quot;, into = c(&quot;hypothesis_01&quot;, &quot;hypothesis_02&quot;), fill = &quot;right&quot;) %&gt;% # then get the data into long format pivot_longer(cols = c(&quot;hypothesis_01&quot;,&quot;hypothesis_02&quot;), values_to = &quot;hypothesis&quot;) %&gt;% # remove NA where there is only one hypothesis drop_na() %&gt;% # remove the name column select(-name) # correct the name landCover to lc hypothesis_data &lt;- mutate(hypothesis_data, hypothesis = replace(hypothesis, hypothesis %in% c(&quot;landCover&quot;, &quot;climate&quot;, &quot;elevation&quot;), c(&quot;lc&quot;,&quot;clim&quot;,&quot;elev&quot;))) 8.3 Read model estimates # which file to read model estimates from hypothesis_data &lt;- mutate(hypothesis_data, file_read = glue::glue(&#39;data/results/results_model_est_{scale}/{hypothesis}_modelEst.xlsx&#39;)) # read in data as list column model_data &lt;- mutate(hypothesis_data, model_est = map2(file_read, Scientific_name, function(fr, sn){ readxl::read_excel(fr, sheet = sn) })) # rename model data components and separate predictors names &lt;- c(&quot;predictor&quot;, &quot;coefficient&quot;, &quot;se&quot;, &quot;ci_lower&quot;, &quot;ci_higher&quot;, &quot;z_value&quot;, &quot;p_value&quot;) # get data for plotting: separate the interaction terms and make the response df model_data &lt;- mutate(model_data, model_est = map(model_est, function(df){ colnames(df) &lt;- names df &lt;- separate_interaction_terms(df) df &lt;- make_response_data(df) return(df) })) # keep significant predictors model_data &lt;- model_data[map_int(model_data$model_est, nrow) &gt; 0,] # remove filename model_data &lt;- select(model_data, -file_read) %&gt;% unnest(model_est) %&gt;% unnest(data) Elevation always varies between 0 to 2625m Landcover predictors are proportions - so 0 to 1 But climate varies: prec_interannual: 0 to 300 temp_interannual: 0 to 50 bio_17: 0 to 80 bio_18: 0 to 300 8.4 Prepare interaction plots Select useful columns and split by scale. Hypothesis can be shape? Facet is predictor. Patchwork is scale. # select useful columns plot_data &lt;- select(model_data, Scientific_name, Common_name, scale, hypothesis, predictor, modulator, m_group, seq_x, mean, ci) # nest at the scale level and ungroup plot_data &lt;- group_by(plot_data, scale) %&gt;% nest() %&gt;% ungroup() # test plot temp_plot &lt;- function(df){ ggplot(df, aes(seq_x, mean, col = m_group, shape = hypothesis, group = interaction(Scientific_name, hypothesis, m_group)))+ geom_line(show.legend = FALSE, size = 0.3)+ geom_ribbon(aes(ymin = mean-ci, ymax = mean+ci, fill = m_group), alpha= 0.3, col = NA)+ scale_colour_manual(values = c(&quot;grey&quot;, &quot;dodgerblue&quot;, &quot;indianred&quot;))+ scale_fill_manual(values = c(&quot;grey&quot;, &quot;dodgerblue&quot;, &quot;indianred&quot;))+ theme_test(base_size = 8, base_family = &quot;Arial&quot;)+ theme(legend.position = &quot;top&quot;)+ guides(colour = guide_legend(nrow = 1))+ facet_wrap(~predictor, scales = &quot;free_x&quot;, nrow = 2, strip.position = &quot;bottom&quot;)+ labs(x = NULL, y = &quot;p(occupancy)&quot;) } # make plots plot_data$plot &lt;- map(plot_data$data, temp_plot) # make patchwork temp_plots &lt;- wrap_plots(plot_data$plot) temp_plots fig_list &lt;- pmap(plot_data, function(Common_name, Scientific_name, scale, hypothesis, figures){ final_plot &lt;- wrap_plots(figures, ncol = 7)+ patchwork::plot_annotation( title = glue::glue(&#39;{Common_name} ({Scientific_name})&#39;), subtitle = glue::glue(&#39;scale: {scale}, best supported hypothesis: {hypothesis}&#39;), tag_levels = &quot;a&quot;, tag_prefix = &quot;(&quot;, tag_suffix = &quot;)&quot;, theme = theme(plot.title = element_text(size = 10, face = &quot;bold&quot;))) return(final_plot) }) # make pdf of patchwork cairo_pdf(file = &quot;figs/fig_occupancy.pdf&quot;, width = 10, height = 3, onefile = TRUE) { print(fig_list) } dev.off() 8.5 Show example interaction library(magick) ## Linking to ImageMagick 6.9.10.23 ## Enabled features: fontconfig, freetype, fftw, lcms, pango, webp, x11 ## Disabled features: cairo, ghostscript, rsvg fig_interaction = image_read_pdf(&quot;figs/fig_occupancy.pdf&quot;, pages = 86) fig_interaction 8.6 Occupancy predictorsâ aggregated effect Plot the number of species affected and the direction of the effect, for each predictor. # select the data rev_bar_data &lt;- distinct(model_data, Scientific_name, scale, predictor, coefficient) # is the coeff positive? how many positive per scale per predictor? rev_bar_data &lt;- mutate(rev_bar_data, direction = coefficient &gt; 0) %&gt;% count(scale, predictor, direction) %&gt;% mutate(mag = n * (if_else(direction, 1, -1))) # visualise the data fig_predictor_effect &lt;- ggplot(rev_bar_data)+ geom_col(aes(x = factor(predictor), y = mag, fill = direction))+ scico::scale_fill_scico_d(palette = &quot;lisbon&quot;, begin = 0.2, end = 0.8, direction = -1)+ scale_x_discrete(guide = guide_axis(n.dodge = 2))+ theme_test()+ theme(legend.position = &quot;none&quot;)+ facet_wrap(~scale, labeller = label_both)+ labs(x = &quot;predictor&quot;, y = &quot;# species&quot;) # save plot ggsave(fig_predictor_effect, filename = &quot;figs/fig_predictor_effect.png&quot;, dpi = 300) knitr::include_graphics(&quot;figs/fig_predictor_effect.png&quot;) "],
["supplementary-material.html", "Section 9 Supplementary material 9.1 Distance to roads 9.2 Species observation distributions 9.3 Climate in relation to elevation 9.4 Landcover in relation to elevation 9.5 Climate in relation to landcover 9.6 Obsever expertise in time and space 9.7 Spatial autocorrelation in climatic predictors 9.8 Climatic raster resampling 9.9 Matching effort with spatial independence 9.10 Spatial thinning: A comparison of approaches", " Section 9 Supplementary material 9.1 Distance to roads 9.1.1 Prepare libraries # load libraries library(reticulate) library(sf) library(dplyr) library(scales) library(readr) library(purrr) library(ggplot2) library(ggthemes) library(ggspatial) library(scico) # round any function round_any &lt;- function(x, accuracy = 20000){round(x/accuracy)*accuracy} # ci function ci &lt;- function(x){qnorm(0.975)*sd(x, na.rm = TRUE)/sqrt(length(x))} # set python path use_python(&quot;/usr/bin/python3&quot;) Importing python libraries. # import classic python libs import itertools from operator import itemgetter import numpy as np import matplotlib.pyplot as plt import math # libs for dataframes import pandas as pd # import libs for geodata from shapely.ops import nearest_points import geopandas as gpd import rasterio # import ckdtree from scipy.spatial import cKDTree from shapely.geometry import Point, MultiPoint, LineString, MultiLineString 9.1.2 Prepare data for processing # read in roads shapefile roads = gpd.read_file(&quot;data/spatial/roads_studysite_2019/roads_studysite_2019.shp&quot;) roads.head() # read in checklist covariates for conversion to gpd # get unique coordinates, assign them to the df # convert df to geo-df chkCovars = pd.read_csv(&quot;data/eBirdChecklistVars.csv&quot;) unique_locs = chkCovars.drop_duplicates(subset=[&#39;longitude&#39;,&#39;latitude&#39;])[[&#39;longitude&#39;, &#39;latitude&#39;]] unique_locs[&#39;coordId&#39;] = np.arange(1, unique_locs.shape[0]+1) chkCovars = chkCovars.merge(unique_locs, on=[&#39;longitude&#39;, &#39;latitude&#39;]) unique_locs = gpd.GeoDataFrame( unique_locs, geometry=gpd.points_from_xy(unique_locs.longitude, unique_locs.latitude)) unique_locs.crs = {&#39;init&#39; :&#39;epsg:4326&#39;} # reproject spatials to 43n epsg 32643 roads = roads.to_crs({&#39;init&#39;: &#39;epsg:32643&#39;}) unique_locs = unique_locs.to_crs({&#39;init&#39;: &#39;epsg:32643&#39;}) # function to simplify multilinestrings def simplify_roads(complex_roads): simpleRoads = [] for i in range(len(complex_roads.geometry)): feature = complex_roads.geometry.iloc[i] if feature.geom_type == &quot;LineString&quot;: simpleRoads.append(feature) elif feature.geom_type == &quot;MultiLineString&quot;: for road_level2 in feature: simpleRoads.append(road_level2) return simpleRoads # function to use ckdtrees for nearest point finding def ckdnearest(gdfA, gdfB): A = np.concatenate( [np.array(geom.coords) for geom in gdfA.geometry.to_list()]) simplified_features = simplify_roads(gdfB) B = [np.array(geom.coords) for geom in simplified_features] B = np.concatenate(B) ckd_tree = cKDTree(B) dist, idx = ckd_tree.query(A, k=1) return dist # function to use ckdtrees for nearest point finding def ckdnearest_point(gdfA, gdfB): A = np.concatenate( [np.array(geom.coords) for geom in gdfA.geometry.to_list()]) #simplified_features = simplify_roads(gdfB) B = np.concatenate( [np.array(geom.coords) for geom in gdfB.geometry.to_list()]) #B = np.concatenate(B) ckd_tree = cKDTree(B) dist, idx = ckd_tree.query(A, k=[2]) return dist # get distance to nearest road unique_locs[&#39;dist_road&#39;] = ckdnearest(unique_locs, roads) # get distance to nearest other site unique_locs[&#39;nnb&#39;] = ckdnearest_point(unique_locs, unique_locs) # write to file unique_locs = pd.DataFrame(unique_locs.drop(columns=&#39;geometry&#39;)) unique_locs[&#39;dist_road&#39;] = unique_locs[&#39;dist_road&#39;] unique_locs[&#39;nnb&#39;] = unique_locs[&#39;nnb&#39;] unique_locs.to_csv(path_or_buf=&quot;data/locs_dist_to_road.csv&quot;, index=False) # merge unique locs with chkCovars chkCovars = chkCovars.merge(unique_locs, on=[&#39;latitude&#39;, &#39;longitude&#39;, &#39;coordId&#39;]) 9.1.3 Species specific nearest sites # load data and send to python load(&quot;data_prelim_processing.rdata&quot;) py$data &lt;- dataGrouped # split data by species datalist = [pd.DataFrame(y) for x, y in data.groupby(&#39;scientific_name&#39;, as_index=False)] # function to get unique vals anc convert to gpd def convData(somedata): somedata = somedata.drop_duplicates(subset=[&#39;longitude&#39;,&#39;latitude&#39;])[[&#39;longitude&#39;, &#39;latitude&#39;, &#39;scientific_name&#39;]] unique_locs = gpd.GeoDataFrame(somedata, geometry=gpd.points_from_xy(somedata.longitude, somedata.latitude)) unique_locs.crs = {&#39;init&#39; :&#39;epsg:4326&#39;} unique_locs = unique_locs.to_crs({&#39;init&#39;: &#39;epsg:32643&#39;}) dists = ckdnearest_point(unique_locs, unique_locs) unique_locs = pd.DataFrame(unique_locs.drop(columns=&#39;geometry&#39;)) unique_locs[&#39;nnb&#39;] = dists return unique_locs # apply function to datalist datalist = list(map(convData, datalist)) 9.1.4 Explicit spatial filter # extract data from python chkCovars &lt;- py$chkCovars chkCovars &lt;- st_as_sf(chkCovars, coords = c(&quot;longitude&quot;, &quot;latitude&quot;)) %&gt;% `st_crs&lt;-`(4326) %&gt;% st_transform(32643) # read wg wg &lt;- st_read(&quot;data/spatial/hillsShapefile/Nil_Ana_Pal.shp&quot;) %&gt;% st_transform(32643) # spatial subset chkCovars &lt;- chkCovars %&gt;% mutate(id = 1:nrow(.)) %&gt;% filter(id %in% unlist(st_contains(wg, chkCovars))) 9.1.5 Species specific filter # extract values from python sp_spec_data &lt;- py$datalist sp_spec_data &lt;- map(sp_spec_data, function(df){ df &lt;- as_tibble(df) %&gt;% st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;)) %&gt;% `st_crs&lt;-`(4326) %&gt;% st_transform(32643) %&gt;% mutate(id = 1:nrow(.)) %&gt;% filter(id %in% unlist(st_contains(wg, .))) %&gt;% st_drop_geometry() }) sp_spec_data &lt;- bind_rows(sp_spec_data) 9.1.6 Plot histogram: distance to roads # make histogram hist_roads &lt;- ggplot(chkCovars)+ geom_histogram(aes(dist_road / 1e3), bins = 20, size=0.2, fill=&quot;steelblue&quot;)+ labs(x = &quot;distance to roads (km)&quot;, y = &quot;# checklists&quot;)+ scale_x_log10(label=label_number(accuracy = 0.1), breaks = c(0.1, 1, 10))+ scale_y_continuous(label=label_number(scale=0.001, accuracy = 1, suffix = &quot;K&quot;))+ theme_few()+ theme(plot.background = element_rect(fill=NA, colour = 1), panel.background = element_blank(), panel.border = element_blank(), axis.line = element_blank()) 9.1.7 Table: Distance to roads # write the mean and ci95 to file chkCovars %&gt;% st_drop_geometry() %&gt;% select(dist_road, nnb) %&gt;% tidyr::pivot_longer(cols = c(&quot;dist_road&quot;, &quot;nnb&quot;), names_to = &quot;variable&quot;) %&gt;% group_by(variable) %&gt;% summarise_at(vars(value), list(~mean(.), ~sd(.), ~min(.), ~max(.))) %&gt;% write_csv(&quot;data/results/distance_roads_sites.csv&quot;) # read in and show readr::read_csv(&quot;data/results/distance_roads_sites.csv&quot;) %&gt;% knitr::kable() variable mean sd min max dist_road 389.7699 859.1526 0.2791282 7637.243 nnb 296.8934 552.9132 0.1367772 12849.668 9.1.8 Plot histogram: distance to nearest site # get unique locations locs &lt;- py$unique_locs # make histogram of nearest neighbours hist_sites &lt;- ggplot(locs)+ geom_histogram(aes(nnb / 1e3), bins = 100, size=0.2, fill=&quot;steelblue&quot;)+ labs(x = &quot;dist. nearest site (km)&quot;, y = &quot;# sites&quot;)+ # scale_x_log10(label=label_number(accuracy = 0.1), # breaks = c(0.1, 1, 10))+ coord_cartesian(xlim=c(0,10))+ scale_y_continuous(label=label_number(scale=0.001, accuracy = 1, suffix = &quot;K&quot;))+ theme_few()+ theme(plot.background = element_rect(fill=NA, colour = 1), panel.background = element_blank(), panel.border = element_blank(), axis.line = element_blank()) 9.1.9 Plot species specific histograms: distance to nearest site # plot histograms by species hist_sites_sp &lt;- ggplot(sp_spec_data)+ geom_histogram(aes(nnb / 1e3), bins = 100, size=0.2, fill=&quot;steelblue&quot;)+ labs(x = &quot;dist. nearest site (km)&quot;, y = &quot;# sites&quot;)+ # scale_x_log10(label=label_number(accuracy = 0.1), # breaks = c(0.1, 1, 10))+ facet_wrap(~scientific_name)+ scale_x_log10()+ #coord_cartesian(xlim=c(0,10))+ scale_y_continuous(label=label_number(scale=0.001, accuracy = 1, suffix = &quot;K&quot;))+ theme_few()+ theme(plot.background = element_rect(fill=NA, colour = 1), panel.background = element_blank(), panel.border = element_blank(), axis.line = element_blank()) ggsave(hist_sites_sp, filename = &quot;figs/fig_nnb_species.png&quot;) 9.1.10 Table: Species specific nearest site # write the mean and ci95 to file sp_spec_data %&gt;% group_by(scientific_name) %&gt;% summarise_at(vars(nnb), list(~mean(.), ~sd(.), ~ci(.), ~min(.), ~max(.))) %&gt;% write_csv(&quot;data/results/dist_nnb_species_specific.csv&quot;) # show table of distance to nearest site for each species readr::read_csv(&quot;data/results/dist_nnb_species_specific.csv&quot;) %&gt;% knitr::kable() scientific_name mean sd ci min max Alcippe poioicephala 416.7076 808.0633 25.00732 0.1367772 16460.7 Carpodacus erythrinus 416.3566 807.9932 25.00515 0.1367772 16460.7 Centropus sinensis 416.8469 808.2412 25.01906 0.1367772 16460.7 Chalcophaps indica 416.6251 807.9633 25.00111 0.1367772 16460.7 Chloropsis aurifrons 417.2270 808.7698 25.02918 0.1367772 16460.7 Chrysocolaptes guttacristatus 416.6501 808.0689 25.00749 0.1367772 16460.7 Cinnyris asiaticus 416.7731 808.1539 25.01324 0.1367772 16460.7 Copsychus fulicatus 416.8469 808.2412 25.01906 0.1367772 16460.7 Copsychus saularis 416.6281 808.0744 25.00766 0.1367772 16460.7 Culicicapa ceylonensis 416.8469 808.2412 25.01906 0.1367772 16460.7 Cyornis tickelliae 416.7184 808.1772 25.01396 0.1367772 16460.7 Dicaeum erythrorhynchos 416.4383 807.8246 24.99370 0.1367772 16460.7 Eumyias albicaudatus 416.5625 808.0762 25.00772 0.1367772 16460.7 Hierococcyx varius 416.5075 807.9157 24.99964 0.1367772 16460.7 Hypsipetes ganeesa 416.9883 808.1900 25.01436 0.1367772 16460.7 Iole indica 417.5128 809.7107 25.05830 0.1367772 16460.7 Irena puella 416.7806 808.0570 25.00712 0.1367772 16460.7 Lanius schach 416.9222 808.0495 25.00689 0.1367772 16460.7 Leptocoma minima 416.7510 808.1632 25.01353 0.1367772 16460.7 Leptocoma zeylonica 416.8469 808.2412 25.01906 0.1367772 16460.7 Motacilla maderaspatensis 416.7644 808.1430 25.01290 0.1367772 16460.7 Myophonus horsfieldii 416.8469 808.2412 25.01906 0.1367772 16460.7 Orthotomus sutorius 416.7268 808.1720 25.01380 0.1367772 16460.7 Parus cinereus 416.7780 808.1521 25.01319 0.1367772 16460.7 Passer domesticus 416.8469 808.2412 25.01906 0.1367772 16460.7 Pellorneum ruficeps 416.8469 808.2412 25.01906 0.1367772 16460.7 Pericrocotus cinnamomeus 416.7935 808.1474 25.01304 0.1367772 16460.7 Pericrocotus flammeus 416.7258 808.1102 25.01189 0.1367772 16460.7 Picus xanthopygaeus 416.8469 808.2412 25.01906 0.1367772 16460.7 Pomatorhinus horsfieldii 416.8663 808.1413 25.01285 0.1367772 16460.7 Psilopogon viridis 416.8469 808.2412 25.01906 0.1367772 16460.7 Psittacula columboides 416.8469 808.2412 25.01906 0.1367772 16460.7 Psittacula cyanocephala 416.8469 808.2412 25.01906 0.1367772 16460.7 Pycnonotus cafer 416.9760 807.9851 25.00490 0.1367772 16460.7 Pycnonotus jocosus 416.7780 808.1521 25.01319 0.1367772 16460.7 Saxicola caprata 416.8469 808.2412 25.01906 0.1367772 16460.7 Sitta frontalis 417.1803 808.0823 25.00479 0.1367772 16460.7 Streptopelia chinensis 416.7387 808.1661 25.01362 0.1367772 16460.7 Turdoides striata 418.0558 810.0605 25.06913 0.1367772 16460.7 Turdus simillimus 416.8469 808.2412 25.01906 0.1367772 16460.7 Upupa epops 416.8469 808.2412 25.01906 0.1367772 16460.7 Zosterops palpebrosus 416.7707 808.1456 25.01298 0.1367772 16460.7 Histograms showing the species-specific distances to nearest neighbouring site. knitr::include_graphics(&quot;figs/fig_nnb_species.png&quot;) 9.1.11 Plot map: points on roads roads &lt;- st_read(&quot;data/spatial/roads_studysite_2019/roads_studysite_2019.shp&quot;) %&gt;% st_transform(32643) points &lt;- chkCovars %&gt;% bind_cols(as_tibble(st_coordinates(.))) %&gt;% st_drop_geometry() %&gt;% mutate(X = round_any(X, 2500), Y = round_any(Y, 2500)) points &lt;- count(points, X,Y) # add land library(rnaturalearth) land &lt;- ne_countries(scale = 50, type = &quot;countries&quot;, continent = &quot;asia&quot;, country = &quot;india&quot;, returnclass = c(&quot;sf&quot;)) %&gt;% st_transform(32643) bbox &lt;- st_bbox(wg) # plot on maps ggplot()+ geom_sf(data = land, fill = &quot;grey90&quot;, col = NA)+ geom_sf(data = wg, fill= NA, col = 1)+ annotation_custom(grob = hist_roads %&gt;% ggplotGrob(), xmin = bbox[&quot;xmax&quot;] - (bbox[&quot;xmax&quot;] - bbox[&quot;xmin&quot;])/2.5, xmax = bbox[&quot;xmax&quot;], ymin = bbox[&quot;ymax&quot;] - (bbox[&quot;ymax&quot;] - bbox[&quot;ymin&quot;])/3, ymax = bbox[&quot;ymax&quot;])+ geom_tile(data=points, aes(X,Y,fill=n), col = &quot;grey90&quot;)+ geom_sf(data=roads, size=0.2, col=&quot;steelblue&quot;)+ # scale_colour_manual(values = &quot;steelblue&quot;, labels = &quot;roads&quot;)+ scale_fill_scico(trans = &quot;log10&quot;, palette = &quot;lajolla&quot;, values=c(0, 1))+ annotation_north_arrow(location = &quot;br&quot;, which_north = &quot;true&quot;, pad_x = unit(0.1, &quot;in&quot;), pad_y = unit(0.5, &quot;in&quot;), style = north_arrow_fancy_orienteering) + annotation_scale(location = &quot;br&quot;, width_hint = 0.4, text_cex = 1) + theme_few()+ theme(legend.position = c(0.9,0.55), legend.background = element_blank(), legend.key = element_rect(fill=&quot;grey90&quot;), axis.title = element_blank(), panel.background = element_rect(fill=&quot;lightblue&quot;))+ coord_sf(expand = FALSE, xlim = bbox[c(&quot;xmin&quot;, &quot;xmax&quot;)], ylim = bbox[c(&quot;ymin&quot;, &quot;ymax&quot;)])+ labs(fill = &quot;checklists&quot;, colour=NULL) # save figure ggsave(filename = &quot;figs/fig_distRoads.png&quot;, device = png()) dev.off() # transform points to utm locs &lt;- locs %&gt;% st_as_sf(coords=c(&quot;longitude&quot;, &quot;latitude&quot;)) %&gt;% `st_crs&lt;-`(4326) %&gt;% st_transform(32643) # add nnb to locations ggplot()+ geom_sf(data = land, fill = &quot;grey90&quot;, col = NA)+ geom_sf(data = wg, fill= NA, col = 1)+ annotation_custom(grob = hist_sites %&gt;% ggplotGrob(), xmin = bbox[&quot;xmax&quot;] - (bbox[&quot;xmax&quot;] - bbox[&quot;xmin&quot;])/2.5, xmax = bbox[&quot;xmax&quot;], ymin = bbox[&quot;ymax&quot;] - (bbox[&quot;ymax&quot;] - bbox[&quot;ymin&quot;])/3, ymax = bbox[&quot;ymax&quot;])+ geom_sf(data=roads, size=0.2, col=&quot;steelblue&quot;)+ geom_sf(data=locs, aes(col=nnb/1000))+ scale_colour_scico(palette = &quot;oslo&quot;, values=c(0, 1), direction = -1, limits = c(0, 5), na.value = &quot;indianred&quot;)+ annotation_north_arrow(location = &quot;br&quot;, which_north = &quot;true&quot;, pad_x = unit(0.1, &quot;in&quot;), pad_y = unit(0.5, &quot;in&quot;), style = north_arrow_fancy_orienteering) + annotation_scale(location = &quot;br&quot;, width_hint = 0.4, text_cex = 1) + theme_few()+ theme(legend.position = c(0.9,0.55), legend.background = element_blank(), legend.key = element_rect(fill=&quot;grey90&quot;), axis.title = element_blank(), panel.background = element_rect(fill=&quot;lightblue&quot;))+ coord_sf(expand = FALSE, xlim = bbox[c(&quot;xmin&quot;, &quot;xmax&quot;)], ylim = bbox[c(&quot;ymin&quot;, &quot;ymax&quot;)])+ labs(fill = &quot;checklists&quot;, colour=NULL) (#fig:plot_figure1)Checklist locations across the Nilgiris, Anamalais and the Palani hills. Inset histogram shows checklistsâ distance to the nearest road, with the X-axis on a log-scale. 9.2 Species observation distributions 9.2.1 Prepare libraries # load libraries library(data.table) library(readxl) library(magrittr) library(stringr) library(dplyr) library(tidyr) library(ggplot2) library(ggthemes) library(scico) # round any function round_any &lt;- function(x, accuracy = 20000){round(x/accuracy)*accuracy} 9.2.2 Read species of interest # add species of interest specieslist &lt;- read_excel(&quot;data/V2_ListOfSpecies_ChosenForStudy.csv.xlsx&quot;) speciesOfInterest &lt;- specieslist$scientific_name 9.2.3 Load raw data for locations # read in shapefile of the study area to subset by bounding box library(sf) wg &lt;- st_read(&quot;data/spatial/hillsShapefile/Nil_Ana_Pal.shp&quot;); box &lt;- st_bbox(wg) # read in data and subset ebd &lt;- fread(&quot;ebd_Filtered_Jun2019.txt&quot;) ebd &lt;- ebd[between(LONGITUDE, box[&quot;xmin&quot;], box[&quot;xmax&quot;]) &amp; between(LATITUDE, box[&quot;ymin&quot;], box[&quot;ymax&quot;]),] ebd &lt;- ebd[year(`OBSERVATION DATE`) &gt;= 2013,] # make new column names newNames &lt;- str_replace_all(colnames(ebd), &quot; &quot;, &quot;_&quot;) %&gt;% str_to_lower() setnames(ebd, newNames) # keep useful columns columnsOfInterest &lt;- c(&quot;scientific_name&quot;,&quot;observation_count&quot;,&quot;locality&quot;, &quot;locality_id&quot;,&quot;locality_type&quot;,&quot;latitude&quot;,&quot;longitude&quot;,&quot;observation_date&quot;, &quot;sampling_event_identifier&quot;) ebd &lt;- ebd[,..columnsOfInterest] # strict spatial filter and assign grid locs &lt;- ebd[,.(longitude, latitude)] # transform to UTM and get 20km boxes coords &lt;- setDF(locs) %&gt;% st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;)) %&gt;% `st_crs&lt;-`(4326) %&gt;% bind_cols(as.data.table(st_coordinates(.))) %&gt;% st_transform(32643) %&gt;% mutate(id = 1:nrow(.)) # convert wg to UTM for filter wg &lt;- st_transform(wg, 32643) coords &lt;- coords %&gt;% filter(id %in% unlist(st_contains(wg, coords))) %&gt;% rename(longitude = X, latitude = Y) %&gt;% bind_cols(as.data.table(st_coordinates(.))) %&gt;% st_drop_geometry() %&gt;% as.data.table() # remove unneeded objs rm(locs); gc() coords &lt;- coords[,.N,by=.(longitude, latitude, X, Y)] ebd &lt;- merge(ebd, coords, all = FALSE, by = c(&quot;longitude&quot;, &quot;latitude&quot;)) ebd &lt;- ebd[(longitude %in% coords$longitude) &amp; (latitude %in% coords$latitude),] 9.2.4 Get proportional obs counts in 20km cells # round to 20km cell in UTM coords ebd[,`:=`(X = round_any(X), Y = round_any(Y))] # count checklists in cell ebd_summary &lt;- ebd[,nchk := length(unique(sampling_event_identifier)), by=.(X,Y)] # count checklists reporting each species in cell and get proportion ebd_summary &lt;- ebd_summary[,.(nrep = length(unique(sampling_event_identifier))), by = .(X,Y,nchk,scientific_name)] ebd_summary[,p_rep := nrep/nchk ] # filter for soi ebd_summary &lt;- ebd_summary[scientific_name %in% speciesOfInterest,] # complete the dataframe for no reports ebd_summary &lt;- setDF(ebd_summary) %&gt;% complete(nesting(X,Y), scientific_name, fill = list(p_rep = 0)) 9.2.5 Plot maps # add land library(rnaturalearth) land &lt;- ne_countries(scale = 50, type = &quot;countries&quot;, continent = &quot;asia&quot;, country = &quot;india&quot;, returnclass = c(&quot;sf&quot;)) # crop land land &lt;- st_transform(land, 32643) # make plot wg &lt;- st_transform(wg, 32643) bbox &lt;- st_bbox(wg) plotDistributions &lt;- ggplot()+ geom_sf(data = land, fill = &quot;grey90&quot;, col = NA)+ geom_tile(data = ebd_summary, aes(X, Y, fill = p_rep), lwd = 0.5, col = &quot;grey90&quot;)+ geom_sf(data = wg, fill = NA, col = &quot;black&quot;, lwd = 0.3)+ scale_fill_scico(palette = &quot;lajolla&quot;, direction = 1, label = scales::percent)+ facet_wrap(~scientific_name, ncol = 7)+ coord_sf(xlim = bbox[c(&quot;xmin&quot;, &quot;xmax&quot;)], ylim = bbox[c(&quot;ymin&quot;, &quot;ymax&quot;)])+ theme_few()+ theme(legend.position = &quot;right&quot;, axis.title = element_blank(), axis.text.y = element_text(angle = 90), panel.background = element_rect(fill = &quot;lightblue&quot;))+ labs(fill = &quot;prop.\\nreporting\\nchecklists&quot;) # export data ggsave(plotDistributions, filename = &quot;figs/fig_species_distributions.png&quot;, height = 12, width = 17, device = png(), dpi = 300); dev.off() # show exported image knitr::include_graphics(&quot;figs/fig_species_distributions.png&quot;) (#fig:export_fig_obs_dist)Proportion of checklists reporting a species in each grid cell (20km side) between 2013 and 2018. Checklists were filtered to be within the boundaries of the Nilgiris, Anamalais and the Palani hills (black outline), but rounding to 20km cells may place cells outside the boundary. Deeper shades of red indicate a higher proportion of checklists reporting a species. 9.3 Climate in relation to elevation 9.3.1 Prepare libraries # load libs library(raster) library(glue) library(purrr) library(dplyr) library(tidyr) library(scales) library(ggplot2) library(ggthemes) # get ci func ci &lt;- function(x){qnorm(0.975)*sd(x, na.rm = T)/sqrt(length(x))} # read landscape prepare for plotting landscape &lt;- stack(&quot;data/spatial/landscape_resamp01km.tif&quot;) # get proper names elev_names &lt;- c(&quot;elev&quot;, &quot;slope&quot;, &quot;aspect&quot;) chelsa_names &lt;- c(&quot;chelsa_bio10_04&quot;, &quot;chelsa_bio10_17&quot;, &quot;chelsa_bio10_18&quot;, &quot;chelsa_prec&quot;, &quot;chelsa_temp&quot;) names(landscape) &lt;- as.character(glue(&#39;{c(elev_names, chelsa_names, &quot;landcover&quot;)}&#39;)) # make duplicate stack land_data &lt;- landscape[[c(&quot;elev&quot;, chelsa_names)]] # convert to list land_data &lt;- as.list(land_data) # map get values over the stack land_data &lt;- purrr::map(land_data, getValues) names(land_data) &lt;- c(&quot;elev&quot;, chelsa_names) # conver to dataframe and round to 100m land_data &lt;- bind_cols(land_data) land_data &lt;- drop_na(land_data) %&gt;% mutate(elev_round = plyr::round_any(elev, 200)) %&gt;% select(-elev) %&gt;% pivot_longer(cols = contains(&quot;chelsa&quot;), names_to = &quot;clim_var&quot;) %&gt;% group_by(elev_round, clim_var) %&gt;% summarise_all(.funs = list(~mean(.), ~ci(.))) 9.3.2 Plot climatic variables over elevation # plot in facets fig_climate_elev &lt;- ggplot(land_data)+ geom_line(aes(x = elev_round, y = mean), size = 0.2, col = &quot;grey&quot;)+ geom_pointrange(aes(x = elev_round, y = mean, ymin=mean-ci, ymax=mean+ci), size = 0.3)+ scale_x_continuous(labels = scales::comma)+ scale_y_continuous(labels = scales::comma)+ facet_wrap(~clim_var, scales = &quot;free_y&quot;)+ theme_few()+ labs(x = &quot;elevation (m)&quot;, y = &quot;CHELSA variable value&quot;) # save as png ggsave(fig_climate_elev, filename = &quot;figs/fig_climate_elev.png&quot;, height = 4, width = 6, device = png(), dpi = 300); dev.off() # show exported image knitr::include_graphics(&quot;figs/fig_climate_elev.png&quot;) (#fig:export_fig_clim_elev)CHELSA climatic variables as a function of elevation, in increments of 200m. Points represent means, while vertical lines show 95% confidence intervals. 9.4 Landcover in relation to elevation 9.4.1 Get data from landscape rasters # get data from landscape rasters lc_elev &lt;- tibble(elev = getValues(landscape[[&quot;elev&quot;]]), landcover = getValues(landscape[[&quot;landcover&quot;]])) # process data for proportions lc_elev &lt;- lc_elev %&gt;% filter(!is.na(landcover), landcover != 0) %&gt;% mutate(elev = plyr::round_any(elev, 100)) %&gt;% count(elev, landcover) %&gt;% group_by(elev) %&gt;% mutate(prop = n/sum(n)) 9.4.2 Plot proportional landcover in elevation # plot figure as tilemap fig_lc_elev &lt;- ggplot(lc_elev)+ geom_tile(aes(x=elev, y=factor(landcover), fill=prop), col=&quot;grey99&quot;, size = 0.6)+ scale_fill_scico(palette = &quot;bilbao&quot;, begin = 0.0, end = 1.0)+ scale_x_continuous(breaks = seq(0, 2500, 500), labels = comma)+ scale_alpha_continuous(range = c(0.3, 1))+ labs(x = &quot;elevation (m)&quot;, y = &quot;landcover&quot;)+ theme_few() # export figure ggsave(fig_lc_elev, filename = &quot;figs/fig_lc_elev.png&quot;, height = 3, width = 6, device = png(), dpi = 300); dev.off() # show exported image knitr::include_graphics(&quot;figs/fig_lc_elev.png&quot;) (#fig:show_fig_lc_elev)Proportional landcover (low = white, high = dark red), as a function of elevation in the study site. Data represent elevation in increments of 100m. 9.5 Climate in relation to landcover 9.5.1 Prepare libraries # load libs library(raster) library(glue) library(purrr) library(dplyr) library(tidyr) # plotting options library(ggplot2) library(ggthemes) library(scico) # get ci func ci &lt;- function(x){qnorm(0.975)*sd(x, na.rm = T)/sqrt(length(x))} # read landscape prepare for plotting landscape &lt;- stack(&quot;data/spatial/landscape_resamp01km.tif&quot;) # get proper names elev_names &lt;- c(&quot;elev&quot;, &quot;slope&quot;, &quot;aspect&quot;) chelsa_names &lt;- c(&quot;chelsa_bio10_04&quot;, &quot;chelsa_bio10_17&quot;, &quot;chelsa_bio10_18&quot;, &quot;chelsa_prec&quot;, &quot;chelsa_temp&quot;) names(landscape) &lt;- as.character(glue(&#39;{c(elev_names, chelsa_names, &quot;landcover&quot;)}&#39;)) # make duplicate stack land_data &lt;- landscape[[c(&quot;landcover&quot;, chelsa_names)]] # convert to list land_data &lt;- as.list(land_data) # map get values over the stack land_data &lt;- purrr::map(land_data, raster::getValues) names(land_data) &lt;- c(&quot;landcover&quot;, chelsa_names) # conver to dataframe and round to 100m land_data &lt;- bind_cols(land_data) land_data &lt;- drop_na(land_data) %&gt;% filter(landcover != 0) %&gt;% pivot_longer(cols = contains(&quot;chelsa&quot;), names_to = &quot;clim_var&quot;) #%&gt;% # group_by(landcover, clim_var) %&gt;% # summarise_all(.funs = list(~mean(.), ~ci(.))) 9.5.2 Plot climatic variables over landcover # plot in facets fig_climate_lc &lt;- ggplot(land_data)+ geom_jitter(aes(x = landcover-0.25, y=value, col = factor(landcover)), width = 0.2, size = 0.1, alpha = 0.1, shape = 4)+ geom_boxplot(aes(x = landcover+0.25, y = value, group = landcover), width = 0.2, outlier.size = 0.2, alpha = 0.3, fill = NA)+ scale_colour_scico_d(begin=0.2, end=0.8)+ scale_y_continuous(labels = scales::comma)+ scale_x_continuous(breaks = c(1:7))+ facet_wrap(~clim_var, scales = &quot;free_y&quot;)+ theme_few()+ theme(legend.position = &quot;none&quot;)+ labs(x = &quot;landcover class&quot;, y = &quot;CHELSA variable value&quot;) # save as png ggsave(fig_climate_lc, filename = &quot;figs/fig_climate_landcover.png&quot;, height = 5, width = 8, device = png(), dpi = 300); dev.off() # show exported image knitr::include_graphics(&quot;figs/fig_climate_landcover.png&quot;) (#fig:export_fig_clim_lc)CHELSA climatic variables as a function of landcover class. Grey points in the background represent raw data. 9.6 Obsever expertise in time and space 9.6.1 Prepare libraries # load libs library(raster) library(glue) library(purrr) library(dplyr) library(tidyr) library(readr) library(scales) # plotting libs library(ggplot2) library(ggthemes) library(scico) # get ci func ci &lt;- function(x){qnorm(0.975)*sd(x, na.rm = T)/sqrt(length(x))} # read in scores and checklist data and link scores &lt;- read_csv(&quot;data/dataObsExpScore.csv&quot;) data &lt;- read_csv(&quot;data/eBirdChecklistVars.csv&quot;) data &lt;- left_join(data, scores, by = c(&quot;observer&quot; = &quot;observer&quot;)) data &lt;- select(data, score, nSp, nSoi, landcover, year) %&gt;% filter(!is.na(score)) 9.6.2 Species seen in relation to osberver expertise # summarise data by rounded score and year data_summary01 &lt;- data %&gt;% mutate(score = plyr::round_any(score, 0.2)) %&gt;% select(score, year, nSp, nSoi) %&gt;% pivot_longer(cols = c(&quot;nSp&quot;, &quot;nSoi&quot;), names_to = &quot;variable&quot;, values_to = &quot;value&quot;) %&gt;% group_by(score, year, variable) %&gt;% summarise_at(vars(value), list(~mean(.), ~ci(.))) # make plot and export fig_nsp_score &lt;- ggplot(data_summary01)+ geom_jitter(data = data, aes(x = score, y = nSp), col = &quot;grey&quot;, alpha = 0.2, size = 0.1)+ geom_pointrange(aes(x = score, y = mean, ymin=mean-ci, ymax=mean+ci, col = as.factor(variable)), position = position_dodge(width = 0.05))+ facet_wrap(~year)+ scale_y_log10()+ # coord_cartesian(ylim=c(0,50))+ scale_colour_scico_d(palette = &quot;cork&quot;, begin = 0.2, end = 0.8)+ labs(x = &quot;expertise score&quot;, y = &quot;species reported&quot;)+ theme_few()+ theme(legend.position = &quot;none&quot;) # export figure ggsave(filename = &quot;figs/fig_nsp_score.png&quot;, width = 8, height = 6, device = png(), dpi = 300); dev.off() # show exported image knitr::include_graphics(&quot;figs/fig_nsp_score.png&quot;) (#fig:show_fig_nsp_score)Total number of species (blue) and species of interest to this study (green) reported in checklists from the study area over the years 2013 â 2018, as a function of the expertise score of the reporting observer. Points represent means, with bars showing the 95% confidence intervals; data shown are for expertise scores rounded to multiples of 0.2, and the y-axis is on a log scale. Raw data are shown in the background (grey points). 9.6.3 Expertise in relation to landcover # plot histograms of expertise scores in different landcover classes data &lt;- filter(data, !is.na(landcover)) # make plot fig_exp_lc &lt;- ggplot(data)+ geom_histogram(aes(x = score), fill = &quot;steelblue&quot;, bins = 20)+ facet_wrap(~landcover, scales = &quot;free_y&quot;, labeller = label_both, nrow = 2)+ scale_y_continuous(labels = comma)+ theme_few()+ theme(legend.position = &quot;none&quot;)+ labs(x = &quot;expertise score&quot;, y = &quot;count&quot;) # export figure ggsave(filename = &quot;figs/fig_exp_lc.png&quot;, width = 8, height = 4, device = png(), dpi = 300); dev.off() # show exported image knitr::include_graphics(&quot;figs/fig_exp_lc.png&quot;) (#fig:show_fig_exp_lc)Distribution of expertise scores in the seven landcover classes present in the study site. 9.7 Spatial autocorrelation in climatic predictors 9.7.1 Load libs and prep data # load libs library(raster) library(gstat) library(stars) library(purrr) library(tibble) library(dplyr) library(tidyr) library(glue) library(scales) library(gdalUtils) # plot libs library(ggplot2) library(ggthemes) library(scico) library(gridExtra) library(cowplot) library(ggspatial) #&#39;make custom functiont to convert matrix to df raster_to_df &lt;- function(inp) { # assert is a raster obj assertthat::assert_that(&quot;RasterLayer&quot; %in% class(inp), msg = &quot;input is not a raster&quot;) coords &lt;- coordinates(inp) vals &lt;- getValues(inp) data &lt;- tibble(x = coords[,1], y = coords[,2], value = vals) return(data) } # list landscape covariate stacks landscape_files &lt;- &quot;data/spatial/landscape_resamp01km.tif&quot; landscape_data &lt;- stack(landscape_files) # get proper names { elev_names &lt;- c(&quot;elev&quot;, &quot;slope&quot;, &quot;aspect&quot;) chelsa_names &lt;- c(&quot;chelsa_bio10_04&quot;, &quot;chelsa_bio10_17&quot;, &quot;chelsa_bio10_18&quot;,&quot;chelsa_prec&quot;, &quot;chelsa_temp&quot;) names(landscape_data) &lt;- as.character(glue(&#39;{c(elev_names, chelsa_names, &quot;landcover&quot;)}&#39;)) } # get chelsa rasters chelsa &lt;- landscape_data[[chelsa_names]] chelsa &lt;- purrr::map(as.list(chelsa), raster_to_df) 9.7.2 Calculate variograms # prep variograms vgrams &lt;- purrr::map(chelsa, function(z){ z &lt;- drop_na(z) vgram &lt;- gstat::variogram(value~1, loc=~x+y, data = z) return(vgram) }) # save temp save(vgrams, file = &quot;data/chelsa/chelsaVariograms.rdata&quot;) # get variogram data vgrams &lt;- purrr::map(vgrams, function(df){ df %&gt;% select(dist, gamma) }) vgrams &lt;- tibble(variable = chelsa_names, data = vgrams) wg &lt;- st_read(&quot;data/spatial/hillsShapefile/Nil_Ana_Pal.shp&quot;) %&gt;% st_transform(32643) bbox &lt;- st_bbox(wg) # add lamd library(rnaturalearth) land &lt;- ne_countries(scale = 50, type = &quot;countries&quot;, continent = &quot;asia&quot;, country = &quot;india&quot;, returnclass = c(&quot;sf&quot;)) # crop land land &lt;- st_transform(land, 32643) 9.7.3 Plot CHELSA data and variograms # make ggplot of variograms yaxis &lt;- c(&quot;semivariance&quot;, rep(&quot;&quot;, 4)) xaxis &lt;- c(&quot;&quot;, &quot;&quot;, &quot;distance (km)&quot;, &quot;&quot;, &quot;&quot;) fig_vgrams &lt;- purrr::pmap(list(vgrams$data, yaxis, xaxis), function(df, ya, xa){ ggplot(df)+ geom_line(aes(x = dist/1000, y = gamma), size = 0.2, col = &quot;grey&quot;)+ geom_point(aes(x = dist/1000, y = gamma), col = &quot;black&quot;)+ scale_x_continuous(labels = comma, breaks = c(seq(0,100,25)))+ scale_y_log10(labels = comma)+ labs(x = xa, y = ya)+ theme_few()+ theme(axis.text.y = element_text(angle = 90, hjust = 0.5, size = 8), strip.text = element_blank()) }) fig_vgrams &lt;- purrr::map(fig_vgrams, as_grob) # make ggplot of chelsa data chelsa &lt;- as.list(landscape_data[[chelsa_names]]) %&gt;% purrr::map(st_as_stars) # colour palettes pal &lt;- c(&quot;bilbao&quot;, &quot;davos&quot;, &quot;davos&quot;, &quot;nuuk&quot;, &quot;bilbao&quot;) title &lt;- c(&quot;a Temp. seasonality&quot;, &quot;b Ppt. driest qtr.&quot;, &quot;c Ppt. warmest qtr.&quot;, &quot;d Variation ppt.&quot;, &quot;e Variation temp.&quot;) direction &lt;- c(1,-1,-1,-1,1) lims &lt;- list(range(chelsa[[1]]$chelsa_bio10_04, na.rm = T), c(0, 500), c(0, 500), c(0,500),#range(chelsa[[4]]$chelsa_prec, na.rm = T), range(chelsa[[5]]$chelsa_temp, na.rm = T)) fig_list_chelsa &lt;- purrr::pmap(list(chelsa, pal, title, direction, lims), function(df, pal, t, d, l){ ggplot()+ geom_stars(data = df)+ geom_sf(data = land, fill = NA, colour = &quot;black&quot;)+ geom_sf(data = wg, fill = NA, colour = &quot;black&quot;, size = 0.3)+ scale_fill_scico(palette = pal, direction = d, label = comma, na.value = NA, limits = l)+ coord_sf(xlim = bbox[c(&quot;xmin&quot;, &quot;xmax&quot;)], ylim = bbox[c(&quot;ymin&quot;, &quot;ymax&quot;)])+ annotation_scale(location = &quot;tr&quot;, width_hint = 0.4, text_cex = 1) + theme_few()+ theme(legend.position = &quot;top&quot;, title = element_text(face = &quot;bold&quot;, size = 8), legend.key.height = unit(0.2, &quot;cm&quot;), legend.key.width = unit(1, &quot;cm&quot;), legend.text = element_text(size = 8), axis.title = element_blank(), axis.text.y = element_text(angle = 90, hjust = 0.5), # panel.background = element_rect(fill = &quot;lightblue&quot;), legend.title = element_blank())+ labs(x=NULL, y=NULL, title = t) }) fig_list_chelsa &lt;- purrr::map(fig_list_chelsa, as_grob) fig_list_chelsa &lt;- append(fig_list_chelsa, fig_vgrams) lmatrix &lt;- matrix(c(c(1,2,3,4,5), c(1,2,3,4,5), c(6,7,8,9,10)), nrow = 3, byrow = T) plot_grid &lt;- grid.arrange(grobs = fig_list_chelsa, layout_matrix = lmatrix) ggsave(plot = plot_grid, filename = &quot;figs/fig_chelsa_variograms.png&quot;, dpi = 300, width = 12, height = 6) # show exported image knitr::include_graphics(&quot;figs/fig_chelsa_variograms.png&quot;) (#fig:show_fig_chelsa)CHELSA rasters with study area outline, and associated semivariograms. Semivariograms are on a log-transformed y-axis. 9.8 Climatic raster resampling 9.8.1 Prepare landcover # read in landcover raster location landcover &lt;- &quot;data/landUseClassification/Reprojected Image_26thJan2020_UTM_Ghats.tif&quot; # get extent e = bbox(raster(landcover)) # init resolution res_init &lt;- res(raster(landcover)) # res to transform to 1000m res_final &lt;- map(c(100, 250, 500, 1e3, 2.5e3), function(x){x*res_init}) # use gdalutils gdalwarp for resampling transform # to 1km from 10m for (i in 1:length(res_final)) { this_res &lt;- res_final[[i]] this_res_char &lt;- stringr::str_pad(this_res[1], 5, pad = &quot;0&quot;) gdalwarp(srcfile = landcover, dstfile = as.character(glue(&#39;data/landUseClassification/lc_{this_res_char}m.tif&#39;)), tr=c(this_res), r=&#39;mode&#39;, te=c(e)) } # read in resampled landcover raster files as a list lc_files &lt;- list.files(&quot;data/landUseClassification/&quot;, pattern = &quot;lc&quot;, full.names = TRUE) lc_data &lt;- map(lc_files, raster) 9.8.2 Prepare spatial extent # load hills library(sf) hills &lt;- st_read(&quot;data/spatial/hillsShapefile/Nil_Ana_Pal.shp&quot;) hills &lt;- st_transform(hills, 32643) buffer &lt;- st_buffer(hills, 3e4) %&gt;% st_transform(4326) bbox &lt;- st_bbox(hills) 9.8.3 Prepare CHELSA rasters # list chelsa files chelsaFiles &lt;- list.files(&quot;data/chelsa/&quot;, full.names = TRUE, pattern = &quot;*.tif&quot;) # gather chelsa rasters chelsaData &lt;- purrr::map(chelsaFiles, function(chr){ a &lt;- raster(chr) crs(a) &lt;- crs(buffer) a &lt;- crop(a, as(buffer, &quot;Spatial&quot;)) return(a) }) # stack chelsa data chelsaData &lt;- raster::stack(chelsaData) names(chelsaData) &lt;- c(&quot;chelsa_bio10_04&quot;, &quot;chelsa_bio10_17&quot;, &quot;chelsa_bio10_18&quot;,&quot;chelsa_prec&quot;, &quot;chelsa_temp&quot;) 9.8.4 Resample prepared rasters # make resampled data resamp_data &lt;- map(lc_data, function(this_scale){ rr &lt;- projectRaster(from = chelsaData, to = this_scale, crs = crs(this_scale), res = res(this_scale)) }) # make a stars list resamp_data &lt;- map2(resamp_data, lc_data, function(z1,z2){ z2[z2 == 0] &lt;- NA z2 &lt;- append(z2, as.list(z1)) %&gt;% map(st_as_stars) }) %&gt;% flatten() # colour palettes pal &lt;- c(&quot;batlow&quot;, &quot;bilbao&quot;, &quot;davos&quot;, &quot;davos&quot;, &quot;nuuk&quot;, &quot;bilbao&quot;) title &lt;- c(&quot;a landcover&quot;, &quot;b Temp. seasonality&quot;, &quot;c Ppt. driest qtr.&quot;, &quot;d Ppt. warmest qtr.&quot;, &quot;e Variation ppt.&quot;, &quot;f Variation temp.&quot;) title &lt;- c(title, rep(&quot;&quot;, 24)) direction &lt;- c(1,1,-1,-1,-1,1) scales &lt;- c(c(&quot;1.0km&quot;, rep(&quot;&quot;, 5)), c(&quot;2.5km&quot;, rep(&quot;&quot;, 5)), c(&quot;5.0km&quot;, rep(&quot;&quot;, 5)), c(&quot;10km&quot;, rep(&quot;&quot;, 5)), c(&quot;25km&quot;, rep(&quot;&quot;, 5))) # make figures across the list fig_list_chelsa_resamp &lt;- purrr::pmap(list(resamp_data, scales, rep(pal, 5), title, rep(direction, 5)), function(df, scale, pal, t, d){ ggplot()+ geom_stars(data = df)+ geom_sf(data = hills, fill = NA, colour = &quot;black&quot;, size = 0.3)+ scale_fill_scico(palette = pal, direction = d, label = comma, na.value = NA)+ coord_sf(xlim = bbox[c(&quot;xmin&quot;, &quot;xmax&quot;)], ylim = bbox[c(&quot;ymin&quot;, &quot;ymax&quot;)])+ theme_void()+ theme(#legend.position = &quot;top&quot;, panel.border = element_rect(), title = element_text(face = &quot;bold&quot;, size = 8), # legend.key.height = unit(0.1, &quot;cm&quot;), # legend.key.width = unit(0.6, &quot;cm&quot;), # legend.text = element_text(size = 8), axis.title = element_text(), axis.title.y = element_text(angle = 90), # axis.text.y = element_text(angle = 90, hjust = 0.5), # panel.background = element_rect(fill = &quot;lightblue&quot;), legend.title = element_blank())+ labs(x=NULL, y=scale, title = t) }) fig_list_chelsa_resamp &lt;- purrr::map(fig_list_chelsa_resamp, as_grob) fig_chelsa_resamp &lt;- grid.arrange(grobs = fig_list_chelsa_resamp, ncol=6) ggsave(plot = fig_chelsa_resamp, filename = &quot;figs/fig_chelsa_resamp.png&quot;, dpi = 100, width = 24, height = 12, device = png(), units = &quot;in&quot;) # use magick to convert library(magick) pl &lt;- image_read_pdf(&quot;figs/fig_chelsa_resamp.pdf&quot;) image_write(pl, path = &quot;figs/fig_chelsa_resamp.png&quot;, format = &quot;png&quot;) # show exported image knitr::include_graphics(&quot;figs/fig_chelsa_resamp.png&quot;) (#fig:show_fig_chelsa_resamp)CHELSA rasters with study area outline, at different scales. Semivariograms are on a log-transformed y-axis. 9.9 Matching effort with spatial independence How many sites would be lost if effort distance was restricted based on spatial independence? 9.9.1 Load librarires # load data packagaes library(data.table) library(dplyr) # load plotting packages library(ggplot2) library(scico) library(ggthemes) library(scales) 9.9.2 Load data # load checklist covariates data &lt;- fread(&quot;data/eBirdChecklistVars.csv&quot;) effort_distance_summary &lt;- data[, effort_distance_class := cut(distance, breaks = c(-1, 0.001, 0.1, 0.25, 0.5, 1, 2.5, 5, Inf), ordered_result = T) ][,.N, by = effort_distance_class ][order(effort_distance_class)] effort_distance_summary[,prop_effort:=cumsum(effort_distance_summary$N)/nrow(data)] 9.9.3 Plot distance exclusion effect # plot effort distance class cumulative sum fig_dist_exclusion &lt;- ggplot(effort_distance_summary)+ geom_point(aes(effort_distance_class, prop_effort), size = 3)+ geom_path(aes(effort_distance_class, prop_effort, group = NA))+ # scale_y_continuous(label=label_number(scale=0.001, accuracy = 1, suffix = &quot;K&quot;))+ scale_x_discrete(labels = c(&quot;stationary&quot;, &quot;100m&quot;, &quot;250m&quot;, &quot;500m&quot;, &quot;1 km&quot;, &quot;2.5 km&quot;, &quot;5 km&quot;))+ theme_few()+ theme(panel.grid = element_line(size = 0.2, color = &quot;grey&quot;))+ labs(x = &quot;effort distance cutoff&quot;, y = &quot;proportion of checklists&quot;) ggsave(plot = fig_dist_exclusion, &quot;figs/fig_cutoff_effort.png&quot;, height = 6, width = 8, dpi = 300) dev.off() knitr::include_graphics(&quot;figs/fig_cutoff_effort.png&quot;) 9.10 Spatial thinning: A comparison of approaches 9.10.1 Prepare libraries # load libraries library(tidyverse) library(glue) library(readr) library(sf) # plotting library(ggthemes) library(scico) library(scales) # ci func ci &lt;- function(x){qnorm(0.975)*sd(x, na.rm = T)/sqrt(length(x))} # load python libs here library(reticulate) # set python path use_python(&quot;/usr/bin/python3&quot;) 9.10.2 Traditional grid-based thinning # load the shapefile of the study area wg &lt;- st_read(&quot;data/spatial/hillsShapefile/Nil_Ana_Pal.shp&quot;) %&gt;% st_transform(32643) # get scales # load checklist data and select one per rounded 500m coordinates { data &lt;- read_csv(&quot;data/eBirdChecklistVars.csv&quot;) %&gt;% count(longitude, latitude, name = &quot;tot_effort&quot;) # how many unique points n_all_points &lt;- nrow(data) d_all_effort &lt;- sum(data$tot_effort) # round to different scales scale &lt;- c(100, 250, 500, 1000) # group data by scale data &lt;- crossing(scale, data) %&gt;% group_by(scale) %&gt;% nest() %&gt;% ungroup() } # select one point per grid cell data &lt;- mutate(data, data = map2(scale, data, function(sc, df){ # transform the data df &lt;- df %&gt;% st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;)) %&gt;% `st_crs&lt;-`(4326) %&gt;% st_transform(32643) %&gt;% bind_cols(as_tibble(st_coordinates(.))) %&gt;% mutate(coordId = 1:nrow(.), X_round = plyr::round_any(X, sc), Y_round = plyr::round_any(Y, sc)) # make a grid grid &lt;- st_make_grid(wg, cellsize = sc) # which cell contains which points grid_contents &lt;- st_contains(grid, df) %&gt;% as_tibble() %&gt;% rename(cell = row.id, coordId = col.id) rm(grid) # what&#39;s the max point in each grid points_max &lt;- left_join(df %&gt;% st_drop_geometry(), grid_contents, by = &quot;coordId&quot;) %&gt;% group_by(cell) %&gt;% filter(tot_effort == max(tot_effort)) # get summary for max max_sites &lt;- points_max %&gt;% ungroup() %&gt;% summarise(prop_points= length(coordId)/n_all_points, prop_effort = sum(tot_effort)/d_all_effort) %&gt;% pivot_longer(cols = everything(), names_to = &quot;variable&quot;) # select a random point in each grid points_rand &lt;- left_join(df %&gt;% st_drop_geometry(), grid_contents, by = &quot;coordId&quot;) %&gt;% group_by(cell) %&gt;% sample_n(size = 1) # get summary for rand rand_sites &lt;- points_rand %&gt;% ungroup() %&gt;% summarise(prop_points = length(coordId)/n_all_points, prop_effort = sum(tot_effort)/d_all_effort) %&gt;% pivot_longer(cols = everything(), names_to = &quot;variable&quot;) df &lt;- tibble(grid_rand = list(rand_sites), grid_max = list(max_sites), points_rand = list(points_rand), points_max = list(points_max)) })) # unnest data data &lt;- unnest(data, cols = data) # save summary as another object data_thin_trad &lt;- data %&gt;% select(-contains(&quot;points&quot;)) %&gt;% pivot_longer(cols = -contains(&quot;scale&quot;), names_to = &quot;method&quot;, values_to = &quot;somedata&quot;) %&gt;% unnest(cols = somedata) # save points for later comparison points_thin_trad &lt;- data %&gt;% select(contains(&quot;points&quot;), scale) rm(data) 9.10.3 Network-based thinning Load python libraries. # import classic python libs import numpy as np import matplotlib.pyplot as plt # libs for dataframes import pandas as pd # network lib import networkx as nx # import libs for geodata import geopandas as gpd # import ckdtree from scipy.spatial import cKDTree 9.10.3.1 Finding modularity in proximity networks # read in checklist covariates for conversion to gpd # get unique coordinates, assign them to the df # convert df to geo-df chkCovars = pd.read_csv(&quot;data/eBirdChecklistVars.csv&quot;) ul = chkCovars[[&#39;longitude&#39;, &#39;latitude&#39;]].drop_duplicates(subset=[&#39;longitude&#39;, &#39;latitude&#39;]) ul[&#39;coordId&#39;] = np.arange(0, ul.shape[0]) # get effort at each coordinate effort = chkCovars.groupby([&#39;longitude&#39;, &#39;latitude&#39;]).size().to_frame(&#39;tot_effort&#39;) effort = effort.reset_index() # merge effort on ul ul = pd.merge(ul, effort, on=[&#39;longitude&#39;, &#39;latitude&#39;]) # make gpd and drop col from ul ulgpd = gpd.GeoDataFrame(ul, geometry=gpd.points_from_xy(ul.longitude, ul.latitude)) ulgpd.crs = {&#39;init&#39; :&#39;epsg:4326&#39;} # reproject spatials to 43n epsg 32643 ulgpd = ulgpd.to_crs({&#39;init&#39;: &#39;epsg:32643&#39;}) ul = pd.DataFrame(ul.drop(columns=&quot;geometry&quot;)) # function to use ckdtrees for nearest point finding def ckd_pairs(gdfA, dist_indep): A = np.concatenate([np.array(geom.coords) for geom in gdfA.geometry.to_list()]) ckd_tree = cKDTree(A) dist = ckd_tree.query_pairs(r=dist_indep, output_type=&#39;ndarray&#39;) return dist # define scales in metres scales = [100, 250, 500, 1000] # function to process ckd_pairs def make_modules(scale): site_pairs = ckd_pairs(gdfA=ulgpd, dist_indep=scale) site_pairs = pd.DataFrame(data=site_pairs, columns=[&#39;p1&#39;, &#39;p2&#39;]) site_pairs[&#39;scale&#39;] = scale # get site ids site_id = np.concatenate((site_pairs.p1.unique(), site_pairs.p2.unique())) site_id = np.unique(site_id) # make network network = nx.from_pandas_edgelist(site_pairs, &#39;p1&#39;, &#39;p2&#39;) # get modules modules = list(nx.algorithms.community.greedy_modularity_communities(network)) # get modules as df m = [] for i in np.arange(len(modules)): module_number = [i] * len(modules[i]) module_coords = list(modules[i]) m = m + list(zip(module_number, module_coords)) # add location and summed sampling duration unique_locs = ul[ul.coordId.isin(site_id)] module_data = pd.DataFrame(m, columns=[&#39;module&#39;, &#39;coordId&#39;]) module_data = pd.merge(module_data, unique_locs, on=&#39;coordId&#39;) # add scale module_data[&#39;scale&#39;] = scale return [site_pairs, module_data] # run make modules on ulgpd at scales data = list(map(make_modules, scales)) # extract data for output tot_pair_data = [] tot_module_data = [] for i in np.arange(len(data)): tot_pair_data.append(data[i][0]) tot_module_data.append(data[i][1]) tot_pair_data = pd.concat(tot_pair_data, ignore_index=True) tot_module_data = pd.concat(tot_module_data, ignore_index=True) # make dict of positions and array of coordinates # site_id = np.concatenate((site_pairs.p1.unique(), site_pairs.p2.unique())) # site_id = np.unique(site_id) # locations_df = ul[ul.coordId.isin(site_id)][[&#39;longitude&#39;, &#39;latitude&#39;]].to_numpy() # pos_dict = dict(zip(site_id, locations_df)) # output data tot_module_data.to_csv(path_or_buf=&quot;data/site_modules.csv&quot;, index=False) tot_pair_data.to_csv(path_or_buf=&quot;data/site_pairs.csv&quot;, index=False) # ends here 9.10.3.2 Process proximity networks in R # read in pair and module data pairs &lt;- read_csv(&quot;data/site_pairs.csv&quot;) mods &lt;- read_csv(&quot;data/site_modules.csv&quot;) # count pairs at each scale count(pairs, scale) pairs %&gt;% group_by(scale) %&gt;% summarise(non_indep_pairs = length(unique(c(p1, p2)))/n_all_points) count(mods, scale) # nest by scale and add module data data &lt;- nest(pairs, data = c(p1, p2)) modules &lt;- group_by(mods, scale) %&gt;% nest() %&gt;% ungroup() # add module data data &lt;- mutate(data, modules = modules$data, data = map2(data, modules, function(df, m){ df &lt;- left_join(df, m, by = c(&quot;p1&quot; = &quot;coordId&quot;)) df &lt;- left_join(df, m, by = c(&quot;p2&quot; = &quot;coordId&quot;)) df &lt;- filter(df, module.x == module.y) return(df) })) %&gt;% select(-modules) # split by module data$data &lt;- map(data$data, function(df){ df &lt;- group_by(df, module.x, module.y) %&gt;% nest() %&gt;% ungroup() return(df) }) 9.10.3.3 A function that removes sites # a function to remove sites remove_which_sites &lt;- function(pair_data){ { a = pair_data %&gt;% select(p1, p2) nodes_a_init = unique(c(a$p1, a$p2)) i_n_d = filter(mods, coordId %in% nodes_a_init) %&gt;% select(node = coordId, tot_effort) %&gt;% mutate(s_f_r = NA) nodes_keep = c() nodes_removed = c() } while(nrow(a) &gt; 0){ # how many nodes in a nodes_a = unique(c(a$p1, a$p2)) # get node or site efforts and arrange in ascending order b = i_n_d %&gt;% filter(node %in% nodes_a) for (i in 1:nrow(b)){ # which node to remove node_out = b$node[i] # how much tot_effort lost d_n_o = b$tot_effort[i] # how many rows remain in a if node_out is removed? a_n_o = filter(a, p1 != node_out, p2 != node_out) indep_nodes = setdiff(nodes_a, unique(c(a_n_o$p1, a_n_o$p2, node_out))) # how much sampling effort made spatially independent indep_sampling = filter(b, node %in% indep_nodes) %&gt;% summarise(tot_effort = sum(tot_effort)) %&gt;% .$tot_effort # message(glue::glue(&#39;{node_out} removal frees {indep_sampling} m&#39;)) # sampling freed by sampling lost b$s_f_r[i] = indep_sampling/d_n_o } # arrange node data by decreasing sfr and increasing tot_effort # highest tot_effort nodes are processed last b = arrange(b, -s_f_r, tot_effort) nodes_removed = c(nodes_removed, b$node[1]) # remove pairs of nodes containing the highest sfr node in b a = filter(a, p1 != b$node[1], p2 != b$node[1]) nodes_keep = c(nodes_keep, setdiff(nodes_a, unique(c(a$p1, a$p2, nodes_removed)))) } message(glue::glue(&#39;keeping {length(nodes_keep)} of {length(nodes_a_init)}&#39;)) # node_status &lt;- tibble(nodes = c(nodes_keep, nodes_removed), # status = c(rep(TRUE, length(nodes_keep)), # rep(FALSE, length(nodes_removed)))) return(as.integer(nodes_removed)) } 9.10.3.4 Removing non-independent sites # remove 5km and 2.5km scale data &lt;- data %&gt;% filter(scale &lt;=1000) # run select sites on the various modules sites_removed &lt;- map(data$data, function(df){ remove_sites &lt;- unlist(purrr::map(df$data, remove_which_sites)) }) # save as rdata save(sites_removed, file = &quot;data/data_network_sites_removed.rdata&quot;) # get python sites ul = py$ul load(&quot;data/data_network_sites_removed.rdata&quot;) # subset sites data &lt;- mutate(data, data = map(sites_removed, function(site_id){ as_tibble(filter(ul, !coordId %in% site_id)) })) # which points are kept points_thin_net &lt;- mutate(data, data = map(data,function(df){ df &lt;- df %&gt;% select(&quot;longitude&quot;, &quot;latitude&quot;) %&gt;% st_as_sf(coords = c(&quot;longitude&quot;, &quot;latitude&quot;)) %&gt;% `st_crs&lt;-`(4326) %&gt;% st_transform(32643) %&gt;% bind_cols(as_tibble(st_coordinates(.))) %&gt;% st_drop_geometry() })) # get metrics for method data_thin_net &lt;- unnest(data, cols = &quot;data&quot;) %&gt;% group_by(scale) %&gt;% summarise(prop_points = length(coordId)/n_all_points, prop_effort = sum(tot_effort)/d_all_effort) %&gt;% mutate(method = &quot;network&quot;) %&gt;% pivot_longer(cols = -one_of(c(&quot;method&quot;, &quot;scale&quot;)), names_to = &quot;variable&quot;) 9.10.4 Measuring method fallibility How many points, at different spatial scales, remain after the application of each method? 9.10.4.1 Prepare data for Python # get points by each method points_list &lt;- append(points_thin_net$data, values = append(points_thin_trad$points_rand, points_thin_trad$points_max)) # get scales as list scales_list &lt;- list(100,250,500,1000, rep(c(100,250, 500,1000), 2)) %&gt;% flatten() # send to python py$points_list = points_list py$scales_list = scales_list 9.10.4.2 Count props under threshold in Python # transform each element to gpd # a function to convert to gpd def make_gpd(df): df = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.X, df.Y)) df.crs = {&#39;init&#39; :&#39;epsg:32643&#39;} return df # function for mean nnd # function to use ckdtrees for nearest point finding def ckd_test(gdfA, gdfB, dist_indep): A = np.concatenate([np.array(geom.coords) for geom in gdfA.geometry.to_list()]) #simplified_features = simplify_roads(gdfB) B = np.concatenate([np.array(geom.coords) for geom in gdfB.geometry.to_list()]) #B = np.concatenate(B) ckd_tree = cKDTree(B) dist, idx = ckd_tree.query(A, k=[2]) dist_diff = list(map(lambda x: x - dist_indep, dist)) mean_dist_diff = np.asarray(dist_diff).mean() return mean_dist_diff # apply to all data points_list = list(map(make_gpd, points_list)) # get nnb all data mean_dist_diff = list(map(ckd_test, points_list, points_list, scales_list)) # count points above threshold # n_non_indep = [] # for i in np.arange(len(points_list)): # ni_pairs = ckd_test(gdfA=points_list[i],gdfB=points_list[i], dist_indep=scales_list[i]) # ni_pairs = pd.DataFrame(data=ni_pairs, columns=[&#39;p1&#39;, &#39;p2&#39;]) # site_id = np.concatenate((ni_pairs.p1.unique(), ni_pairs.p2.unique())) # ni_sites = len(np.unique(site_id))/points_list[i].shape[0] # n_non_indep.append(ni_sites) 9.10.5 Plot metrics for different methods # combine the thinning metrics data data_plot &lt;- bind_rows(data_thin_net, data_thin_trad) # get data for mean distance data_thin_compare &lt;- tibble(scale = unlist(scales_list), method = c(rep(&quot;network&quot;, 4), rep(&quot;grid_rand&quot;, 4), rep(&quot;grid_max&quot;, 4)), `mean NND - buffer (m)` = unlist(py$mean_dist_diff)) %&gt;% pivot_longer(cols = &quot;mean NND - buffer (m)&quot;, names_to = &quot;variable&quot;) # bind rows with other data data_plot &lt;- bind_rows(data_plot, data_thin_compare) # plot results fig_spatial_thinning &lt;- ggplot(data_plot)+ geom_vline(xintercept = scale, lty = 3, colour=&quot;grey&quot;, lwd=0.4)+ geom_line(aes(x = scale, y= value, col =method))+ geom_point(aes(x = scale, y= value, col =method, shape = method))+ facet_wrap(~variable, scales = &quot;free&quot;)+ scale_shape_manual(values = c(1, 2, 0))+ scale_x_continuous(breaks = scale)+ scale_y_continuous()+ scale_colour_scico_d(palette = &quot;batlow&quot;, begin=0.2, end=0.8)+ theme_few()+ theme(legend.position = &quot;top&quot;)+ labs(x= &quot;buffer distance (m)&quot;) # save ggsave(fig_spatial_thinning, filename = &quot;figs/fig_spatial_thinning_02.png&quot;, width = 10, height = 4, dpi=300); dev.off() knitr::include_graphics(&quot;figs/fig_spatial_thinning_02.png&quot;) "]
]
