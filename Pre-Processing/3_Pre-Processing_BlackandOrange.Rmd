---
output: html_notebook
---
### Script to pre-process eBird data for Occupancy Modeling
Note: Current Data was filtered only for TamilNadu, Karnataka and Kerala to restrict myself to Nilgiris, Anamalais and Palani Hills
Follows instructions from : Matthew Strickay (rOpenSci)
Link: https://ropensci.org/blog/2018/08/07/auk/

#### Installing auk from CRAN or use the Github versiontools fom Cornell Lab of Ornithology
```{r}
devtools::install_github("CornellLabOfOrnithology/auk")

```

#### Five tasks that need to be performed:
#### Cleaning, Filtering, Importing, Pre-processing, Zero-filling
```{r}
library(auk)
library(raster)
library(tidyverse)
library(sf)
```

#### Define the paths to ebd and sampling event files
```{r}
f_in_ebd <- file.path("E:\\Occupancy Modeling\\Data\\ebd_IN_relMay-2018.txt")
f_in_sampling <- file.path("E:\\Occupancy Modeling\\Data\\ebd_sampling_relMay-2018.txt")
```

#### Adding necessary filters 
#### NOTE: This code has been run previously and data can be loaded directly
```{r}
ebd_filters <- auk_ebd(f_in_ebd, f_in_sampling) %>% 
  auk_species(c("Anthus nilghiriensis","Montecincla cachinnans","Montecincla fairbanki",
              "Sholicola albiventris","Sholicola major","Culicicapa ceylonensis",
            "Pomatorhinus horsfieldii","Ficedula nigrorufa","Pycnonotus jocosus",
            "Iole indica","Hemipus picatus","Saxicola caprata","Eumyias albicaudatus",
            "Rhopocichla atriceps")) %>% 
  auk_country(country = "IN") %>%
  auk_state(c("IN-KL","IN-TN", "IN-KA")) %>% # Restricting geography to TamilNadu, Kerala & Karnataka
  auk_date(c("2000-01-01", "2018-09-17")) %>% 
  auk_complete()
ebd_filters # Now the filters have been set
```

#### Perform the actual filtering
```{r}
f_out_ebd <- "E:\\Occupancy Modeling\\Data\\ebd_Filtered_May2018.txt"
f_out_sampling <- "E:\\Occupancy Modeling\\Data\\ebd_sampling_Filtered_May2018.txt"

# Below code need not be run if it has been filtered once already and the above path leads to
# the right dataset 
# ebd_filtered <- auk_filter(ebd_filters, file = f_out_ebd, 
#                           file_sampling = f_out_sampling)

```

#### Now that the dataframe has been reduced, let's take a glimpse at the data
```{r}
ebd <- read_ebd(f_out_ebd)
glimpse(ebd)
```

#### Zero-filling
```{r}
zf <- auk_zerofill(f_out_ebd, f_out_sampling)
new_zf <- collapse_zerofill(zf) # Creates a new zero-filled dataframe with a 0 marked for each checklist when the bird was not observed
glimpse(zf)
```

#### Let us play around with data for the Black and Orange Flycatcher (Ficedula nigrorufa)
```{r}
# Choose one species at a time and curate the presence records
ebd_subset <- ebd[,c(1,7,8,22,23,24,25,26,27,28,29,30,31,34,35,36,37,42)]
zf_subset <- new_zf[,c(1,32,33,13,14,15,16,17,18,19,20,21,22,25,26,27,28,34)]

barfly.pr <- ebd_subset[ebd_subset$scientific_name=="Ficedula nigrorufa",]
barfly.pr[is.na(barfly.pr)] <- 0 # Setting all NAs to 0

barfly.abs <- zf_subset[zf_subset$scientific_name=="Ficedula nigrorufa",]
barfly.abs[is.na(barfly.abs)] <- 0 # Setting all NAs to 0

```


#### Load shapefiles that are extended contours of Nilgiris, Anaimalais and Palanis and filter the presences and absences, after converting them to spatial objects
```{r}
## Loading the shapefiles
library(rgdal)
library(mapview)
WG <- st_read("D:\\Occupancy Modeling\\Data\\Shapefiles\\Nil_Ana_Pal.shp")
WG <- st_as_sf(WG)

## Simple visualizations of an sf object
plot(WG, key.pos = 1, axes = TRUE, key.width = lcm(1.3), key.length = 1.0)
plot(WG, graticule = TRUE, key.pos = NULL, axes = TRUE)

## Converting the dataframes to spatial objects
library(sf)
barfly.pr.sp <- st_as_sf(barfly.pr, coords = c("longitude","latitude")) %>% st_set_crs("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs")
barfly.abs.sp <- st_as_sf(barfly.abs, coords = c("longitude","latitude")) %>% st_set_crs("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs") 

## Filtering the number of presences and absences within the given extent
library(raster)
m1 <- mapview(WG) # Checking if presence points fall outside the provided shapefile
m2 <- mapview(barfly.pr.sp)
m1 + m2

barfly.pr.sp <- st_intersection(WG,barfly.pr.sp)
m3 <- mapview(barfly.pr.sp) 
m1+m3 # Indeed the points fall within the extent

barfly.abs.sp <- st_intersection(WG, barfly.abs.sp)
m4 <- mapview(barfly.abs.sp)
m1+m4 # Indeed the points fall within the extent

```


#### Pre-processing the presences and absences of the Black and Orange Flycatcher
```{r}

# Step 1: One notices that there are many presence points within the absence data that needs to be removed and added to the presence dataframe
library(dplyr)
barfly.pr.temp <- barfly.abs.sp %>% filter(species_observed=="TRUE")

# However the above points are the presence points themselves and can be removed
rm(barfly.pr.temp)
barfly.abs.sp <- barfly.abs.sp %>% filter(species_observed != "TRUE")

# Step 2: A combination of locality_id, observation data, observer id and statrt time will be used to group the data
# This was suggested by Morgan Tingley on 29th Jan (meeting with VR)
# One can readily use the sampling event identifier that is offered through the eBird dataset

# Replacing the X with 1, with the logic being that if an individual was heard, we assume the presence of atleast 1
barfly.pr.sp$observation_count <- gsub('X',1,barfly.pr.sp$observation_count)

# Removing all data for which duration_minutes==0, and keeping only those rows where effort was > 0
barfly.pr.sp <- barfly.pr.sp[barfly.pr.sp$duration_minutes>0,]
barfly.abs.sp <- barfly.abs.sp[barfly.abs.sp$duration_minutes>0,]

# Create a new dataframe that sums up the effort related columns for each unique sampling event identifier. In other words, I am creating one replicate per day, and totalling the effort spent that day
barfly.pr.sp <- barfly.pr.sp %>% group_by(sampling_event_identifier) %>% 
  summarise(total_duration = sum(duration_minutes), total_dist = sum(effort_distance_km),total_area = sum(effort_area_ha),time_obs_start = paste(time_observations_started,collapse=","),localities = unique(locality), locality_type=unique(locality_type), locality_id=unique(locality_id), observer_id=unique(observer_id), observation_date= 
unique(observation_date), species = unique(scientific_name), observation = paste(observation_count, collapse=","), prot_type = paste(protocol_type, collapse=","), num_observers = paste(number_observers, collapse=","))

barfly.abs.sp <- barfly.abs.sp %>% group_by(sampling_event_identifier) %>% 
summarise(total_duration = sum(duration_minutes), total_dist = sum(effort_distance_km),total_area = sum(effort_area_ha),time_obs_start = paste(time_observations_started,collapse=","),localities = unique(locality), locality_type=unique(locality_type),locality_id=unique(locality_id), observer_id=unique(observer_id), observation_date= 
unique(observation_date),species = unique(scientific_name), observation = unique(observation_count), prot_type = paste(protocol_type, collapse=","), num_observers = paste(number_observers, collapse=","))

# Step 3: Combine the Presences and Absences to create a dataframe for EDA
# This can be done since the  column names for both dataframes are the same
dat <- rbind(barfly.pr.sp, barfly.abs.sp)

# Step 4: Removing Observations from checklists that cite duration in minutes < 240 min and distance < 5 km or area < 500 ha; and converting data structure to factor etc. where necessary

dat <- dat %>% filter(total_duration <= 240 & total_dist <= 5 & total_area <= 500)

```

#### Creating a dataset for every year and across all years 
```{r}
# Visualize the number of presences and absences

# 1. Spatial occurrence for visualization purposes (Code need not be executed)
# m1 <- mapview(WG)
# mapview(dat, zcol ="observation")
# mapview(dat, zcol="total_duration")
# mapview(dat, cex="total_duration")

## Exploratory analyses before year-wise datasets are prepared
## Taking the mean number of observations (abundance) for ease of calculation and rounding this number
dat$observation <- sapply(strsplit(dat$observation, ","), function(x) round(mean(as.numeric(x))))

## Creating a new column of Presences and Absences, where all values greater than 1 are marked 1 and less than 1 as 0
dat$pres_abs <- pmin(dat$observation,1)

# Loading covariates for occupancy modeling

## 1. Elevation
alt <- raster("D:\\Occupancy Modeling\\Data\\Elevation\\alt")
cr <- crop(alt, extent(WG))
alt.WG <- mask(cr, WG)

# Project the raster to UTM
# UTMCrs <- CRS("+proj=utm +zone=43 +datum=WGS84 +units=m +no_defs ")
# alt.WG <- projectRaster(alt.WG, crs = UTMCrs, res=1000)

## 2. Bioclimatic variables from CHELSA
Bio <- list.files("D:\\MasterThesis\\Absence Approach\\Data\\CHELSA_Data\\", pattern=".tif$", full.names=T)

Clim <- stack()
for (i in 1:length(Bio)){
  a <- raster(Bio[[i]])
  crs(a) <- CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs")
  # pro <- projectRaster(a, alt.WG) # Projection can be done later
  pro <- crop(a, extent(alt.WG))
  pro <- mask(pro, alt.WG)
  Clim <- stack(Clim, pro)
}

## 3. Loading EVI data for all years, by taking the average 
EVI.all <- raster::stack("D:\\Occupancy Modeling\\Data\\EVI\\MOD13Q1_EVI_AllYears.tif")
names(EVI.all) <- c("Jan.all","Feb.all","Mar.all","Apr.all","May.all","Jun.all",
                    "Jul.all","Aug.all","Sep.all","Oct.all","Nov.all","Dec.all")
EVI.all <-  EVI.all*0.0001 # Scaling to ensure values remain between -1 and 1
# EVI.all <- projectRaster(EVI.all, alt.WG) # method='bilinear' is the default, can be done later

## 4. Loading EVI data for years 2013 to 2018 (as had been decided earlier)
## This will be used later on when running occupancy models for five years only (2013 to 2018)
## For Pratik to Pre-process (for later analyses)

EVI.2013 <- raster::stack("D:\\Occupancy Modeling\\Data\\EVI\\MOD13Q1_EVI_2013.tif")
EVI.2014 <- raster::stack("D:\\Occupancy Modeling\\Data\\EVI\\MOD13Q1_EVI_2014.tif")
EVI.2015 <- raster::stack("D:\\Occupancy Modeling\\Data\\EVI\\MOD13Q1_EVI_2015.tif")
EVI.2016 <- raster::stack("D:\\Occupancy Modeling\\Data\\EVI\\MOD13Q1_EVI_2016.tif")
EVI.2017 <- raster::stack("D:\\Occupancy Modeling\\Data\\EVI\\MOD13Q1_EVI_2017.tif")
EVI.2018 <- raster::stack("D:\\Occupancy Modeling\\Data\\EVI\\MOD13Q1_EVI_2018.tif")

EVI.yearly <- raster::stack((EVI.2013 + EVI.2014 + EVI.2015 + EVI.2016 +
                               EVI.2017 + EVI.2018) / 6)
names(EVI.yearly) <- c("Jan","Feb","Mar","Apr","May","Jun",
                    "Jul","Aug","Sep","Oct","Nov","Dec")
EVI.yearly <- EVI.yearly*0.0001 # Scaling to ensure values remain between -1 and 1
# EVI.yearly <- projectRaster(EVI.yearly, alt.WG) # method='bilinear' is the default, can be done later
# Note: the above EVI is the average EVI for each month across five years (2013 to 2018)

## 6. Resample and reclassify rasters to ensure that they are all at the same scale and resolution

alt.WG
Clim # same resolution as alt.WG
EVI.all.resam <-  resample(EVI.all, alt.WG,method ="bilinear") # same resolution as alt.WG
EVI.yearly.resam <- resample(EVI.yearly, alt.WG, method="bilinear") # same resolution as alt.WG

## Deriving slope and aspect from elevation
slope <- terrain(alt.WG ,opt = 'slope',unit='degrees', neighbors = 8) # same resolution as alt.WG
aspect <- terrain(alt.WG ,opt='aspect', unit='degrees', neighbors = 8)

## 7. Creating a single dataframe across all years (includes data from 2000 to Present)

t <- dat[,c(1,2,3,5,6,7,8,9,10,11,12,13,14,16)] # Subset only those columns that are necessary for later
s <- t %>% group_by(group = sampling_event_identifier) %>% spread(group,pres_abs)

# Extract elevation data 
elev <- raster::extract(alt.WG,s)
clim <- raster::extract(Clim,s)
evi <- raster::extract(EVI.all.resam,s)
sl <- raster::extract(slope,s)
as <- raster::extract(aspect,s)

a <- cbind(elev,clim,evi,sl,as,s)
names(a) <- c("Elevation","Bio1","Bio10","Bio11","Bio12","Bio13","Bio14","Bio15","Bio16",
              "Bio17","Bio18","Bio19","Bio2","Bio3","Bio4","Bio5","Bio6","Bio7","Bio8","Bio9",
              "Precip_Interannual","Temp_Interannual","Jan_EVI","Feb_EVI","Mar_EVI","Apr_EVI",
              "May_EVI","Jun_EVI","Jul_EVI","Aug_EVI","Sep_EVI","Oct_EVI","Nov_EVI","Dec_EVI",
              "Slope","Aspect",colnames(s))
              
# New dataset
bar.all <- a
rm(t,s,a,elev,clim,evi,sl,as)

```


#### Carrying out Exploratory Data analyses for the covariate data extracted
```{r}
# Load the necessary libraries and scripts to assess correlations
library(ecodist)

# Load Dean Urban's script to remove check for correlations where r > 0.7
source("D:\\Occupancy Modeling\\Scripts\\Pre-Processing Scripts\\screen_cor.R")

# Subset only covariates to look for correlations
env <- bar.all[1:36]
colnames(env)

# Drop the geometry else the correlation function cannot be run
st_drop_geometry <- function(x) {
  if(inherits(x,"sf")) {
      x <- st_set_geometry(x, NULL)
      class(x) <- 'data.frame'
  }
  return(x)
}

env <- st_drop_geometry(env)

# Testing for correlations
screen.cor(env)

# Removing Bio13, Bio12, Bio15, Bio16 as they are highly correlated with Interannual_Precip
env2 <- env[,c(-5,-6,-8,-9)]
screen.cor(env2)

# Removing Bio1, Bio10, Bio11, Bio19, Bio3,Bio5,Bio6,Bio7,Bio8,Bio9 as they are highly correlated with Elevation
colnames(env2)
env3 <- env2[,c(-2,-3,-4,-8,-10,-12,-13,-14,-15,-16)]
screen.cor(env3)

# Removing Bio14,Bio2, Feb_EVI, May_EVI, Sep_EVI, Nov_EVI
colnames(env3)
env4 <- env3[,c(-2,-5,-10,-13,-17,-19)]
screen.cor(env4)

# Removing Mar_EVI, Jun_EVI, Aug_EVI, Dec_EVI, Apr_EVI
colnames(env4)
env5 <- env4[,c(-8,-10,-12,-14,-9)]
screen.cor(env5)

# Look at correlations among the leftover environmental variables
library(psych)
pairs.panels(env5)
rm(env,env2,env3,env4)

# Add env5 back to the list of observations

coords <- st_coordinates(st_centroid(bar.all))
coords <- as.data.frame(coords)

bar.drop <- st_drop_geometry(bar.all)
bar.all <- cbind(env5, coords, bar.drop[37:ncol(bar.drop)])
bar.all <- st_as_sf(bar.all, coords = c("X","Y"))

```


#### Preparing a dataset to model probability of detection
```{r}
# Preparing respective covariates which include:
# Time - Duration in Minutes, Distance -in kilometres, Number of observers, Julian Date, Time Observation Started

# Time
t <- dat[,c(1,2)]
s <- t %>% group_by(group = sampling_event_identifier) %>% spread(group,total_duration)
bar.time <- s
rm(t,s)

# Distance
t <- dat[,c(1,3)]
s <- t %>% group_by(group = sampling_event_identifier) %>% spread(group,total_dist)
bar.dist <- s
rm(t,s)

# Number of Observers
# Take the mean of multiple numbers of observers
dat$num_observers <- sapply(dat$num_observers, function(x) mean(scan(text = x, what = numeric(), sep = ","), na.rm=T))

t <- dat[,c(1,14)]
s <- t %>% group_by(group = sampling_event_identifier) %>% spread(group,num_observers)
bar.num_obs <- s
rm(t,s)

# Julian Date
# To extract the julian date, we use the package lubridate
library(lubridate)
jul.date <- as.Date(dat$observation_date)
dat$jul.date <- yday(jul.date)

t <- dat[,c(1,17)]
s <- t %>% group_by(group = sampling_event_identifier) %>% spread(group,jul.date)
bar.jul.date <- s
rm(t,s)

# Start times
t <- dat[,c(1,5)]
s <- t %>% group_by(group = sampling_event_identifier) %>% spread(group,time_obs_start)
bar.starttime <- s
rm(t,s)


```


#### subsampling the dataset to retain only 10 visits / sampling events to a given site
```{r}

## Sort data for every unique locality_id and in descending order of effort
## And keep only top 10 rows based on the highest amount of total_duration
## Does this make sense Pratik?

trial <- bar.all %>% group_by(locality_id) %>% arrange(desc(total_duration)) %>% top_n(10, total_duration)

## Looks like the above is not keeping only 10 rows, but more than 10 combinations of locality_id and total_duration

## Next step is to match remaining sampling event identifiers with covariates obtained above for probability of detection


```



#### Creating vectors that look at mean effort in terms of time and distance
#### let's work on this after we talk about it. I am not sure if this is necessary since
```{r}
# Calculate time spent and distance covered at a site across all years 
# @Pratik: This needs to be re-run (getting errors)
# Error: Error in eval_tidy(enquo(var), var_env) : object 'group' not found (Line 382)

# Time
a <- st_drop_geometry(bar.time) # Convert the sf object to a dataframe
a <- a %>% gather(date,N, -sampling_event_identifier) %>%   group_by(sampling_event_identifier) %>% summarise(tot_visits = sum(N, na.rm=T)) %>% spread(date,tot_visits)
z <- as.data.frame(sapply(a[,-1], function(x) as.numeric(x)))

tot_time <- rowSums(z, na.rm = T)
Nvisits <- as.numeric(table(bar.time$locality_id)) # Total number of visits for each unique site

bar.avg.time <- tot_time / Nvisits 
rm(a,z)

# Distance
a <- st_drop_geometry(bar.dist) # Convert the sf object to a dataframe
a <- a %>% gather(date, N, -locality_id, -observer_id, -observation_date) %>%   group_by(locality_id, observation_date) %>% summarise(tot_visits = sum(N, na.rm=T)) %>% spread(observation_date, tot_visits)
z <- as.data.frame(sapply(a[,-1], function(x) as.numeric(x)))

tot_dist <- rowSums(z, na.rm = T)
Nvisits <- as.numeric(table(bar.dist$locality_id)) # Total number of visits for each unique site

bar.avg.dist <- tot_dist / Nvisits 
rm(a,z)
```



